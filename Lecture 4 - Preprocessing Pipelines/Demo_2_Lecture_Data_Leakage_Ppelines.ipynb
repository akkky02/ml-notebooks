{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["IfnZ3LsUHxNl"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# <font color = 'pickle'>**Introduction**</font>\n","<font color = 'indianred'>In this lecture, we will learn what data leakage is, what problems can occur due to it, and how to resolve it."],"metadata":{"id":"_LHZe88YcY7N"}},{"cell_type":"markdown","metadata":{"id":"bb4kb2iHWA8J"},"source":["# <font color = 'pickle'>**Import Libraries**"]},{"cell_type":"markdown","source":["First, we will all the required libraries that we will use across this lecture.\n","\n","It is always a good practice to import all the required libraries initially."],"metadata":{"id":"kBUT6800cVTV"}},{"cell_type":"code","metadata":{"id":"IdqdJx7WNaQe","executionInfo":{"status":"ok","timestamp":1723005800275,"user_tz":300,"elapsed":3423,"user":{"displayName":"Akshat Patil","userId":"09657732965291711317"}}},"source":["# numerical processing\n","import numpy as np\n","\n","# control teh wdith of text displayed\n","import textwrap as tw\n","\n","# get/create dataset\n","from sklearn.datasets import load_diabetes\n","from sklearn.datasets import fetch_openml\n","from sklearn.datasets import make_classification\n","\n","# sklearn for pre-processing\n","from sklearn.model_selection import GridSearchCV, KFold, cross_val_score, train_test_split\n","\n","from sklearn.preprocessing import PolynomialFeatures, StandardScaler, MinMaxScaler, OneHotEncoder\n","from sklearn.impute import SimpleImputer\n","from sklearn.feature_selection import SelectKBest\n","from sklearn.pipeline import make_pipeline, Pipeline\n","from sklearn.compose import make_column_transformer, ColumnTransformer\n","from sklearn.metrics import accuracy_score\n","\n","# sklearn models\n","from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n","from sklearn.linear_model import Ridge, Lasso\n","from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier"],"execution_count":1,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jhFSQR0Xp_bq"},"source":["# <font color = 'pickle'>**Common Pitfalls**"]},{"cell_type":"markdown","metadata":{"id":"5-GFFYE-qH0e"},"source":["## <font color = 'pickle'>**Inconsistent Preprocessing**\n"]},{"cell_type":"markdown","source":["* Before understanding the data leakage problem, first, we will understand what problem can occur if the preprocessing is done inconsistently.\n","* For example, we apply some preprocessing in training data but forget to use the same on test data, so due to this, how our model gets impacted and how we can resolve it."],"metadata":{"id":"c8hgsQBEkeZy"}},{"cell_type":"markdown","source":["Sklearn's make_classification() can be used to create dummy classification data.\n","\n","In ML, we extensively use this to create dummy data, and then do training with that data to better understand the model.\n","\n","It's few of the parameters are as follows:\n","\n","- n_samples: The total number of samples i.e. data points.\n","- n_features: The total number of features to be generated.\n","- n_informative: The total number of informative features.\n","- n_redundant: The total number of redundant features.\n","- random_state: To set random state, so that re-running the code will create exact same data.\n","\n","You can find more about this from their [official documentation](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_classification.html)."],"metadata":{"id":"0ljhL7vuC4Wj"}},{"cell_type":"code","metadata":{"id":"VnkEqOZCFTiC","executionInfo":{"status":"ok","timestamp":1723005808111,"user_tz":300,"elapsed":335,"user":{"displayName":"Akshat Patil","userId":"09657732965291711317"}}},"source":["# Creating 1000 features with 2 class labels using make_classification() that we have imported earlier.\n","X12, y = make_classification(n_samples=1000, n_features=2, n_informative=2, n_redundant=0, random_state=7)"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"k6Ur1LwGw8px","executionInfo":{"status":"ok","timestamp":1723005808338,"user_tz":300,"elapsed":2,"user":{"displayName":"Akshat Patil","userId":"09657732965291711317"}}},"source":["# Creating standard normally distributed 1000 features.\n","np.random.seed(0)\n","X3 = 1000 * np.random.standard_normal((1000, 1))"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"B1_mPmj6FY3B","executionInfo":{"status":"ok","timestamp":1723005809292,"user_tz":300,"elapsed":155,"user":{"displayName":"Akshat Patil","userId":"09657732965291711317"}}},"source":["# Concatenating the feature\n","X = np.concatenate((X12,X3), axis =1)"],"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":["So, we have created our randomly generated dataset."],"metadata":{"id":"Gti7GrpRlGqD"}},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QLeHPeCYJSUn","executionInfo":{"status":"ok","timestamp":1723005810522,"user_tz":300,"elapsed":186,"user":{"displayName":"Akshat Patil","userId":"09657732965291711317"}},"outputId":"5135b5e8-5712-41eb-ccdb-2bb9ae602fc8"},"source":["print(X[:,0].mean(), X[:,1].mean(), X[:,2].mean())"],"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["0.04148292912185122 -0.000783387580940758 -45.25670749019538\n"]}]},{"cell_type":"markdown","source":["Now, let's split our data into train and test set."],"metadata":{"id":"FDoGnAGxlSlo"}},{"cell_type":"code","metadata":{"id":"U4P-KQTQ_CiD","executionInfo":{"status":"ok","timestamp":1723005821067,"user_tz":300,"elapsed":150,"user":{"displayName":"Akshat Patil","userId":"09657732965291711317"}}},"source":["# split the data into train/test split\n","X_train, X_test, y_train, y_test = train_test_split( X, y, random_state=200, train_size = 0.3)"],"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":["Now, let's standardize our training data and train a simple KNN model.\n","\n","**Note:** StandardScaler() is used to standardize the data in such a way that it has a mean of 0 and a standard deviation of 1."],"metadata":{"id":"UodWE6JcoAzm"}},{"cell_type":"code","metadata":{"id":"b1LSSxwLBSKK","executionInfo":{"status":"ok","timestamp":1723005853486,"user_tz":300,"elapsed":164,"user":{"displayName":"Akshat Patil","userId":"09657732965291711317"}}},"source":["# Initializing StandardScaler that we have imported earlier.\n","preprocessor = StandardScaler()"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"CCLwM0bh_G5u","executionInfo":{"status":"ok","timestamp":1723005853854,"user_tz":300,"elapsed":208,"user":{"displayName":"Akshat Patil","userId":"09657732965291711317"}}},"source":["# select top 10 features\n","X_train = preprocessor.fit_transform(X_train)"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":74},"id":"hr8qutnz_MXV","executionInfo":{"status":"ok","timestamp":1723005854550,"user_tz":300,"elapsed":147,"user":{"displayName":"Akshat Patil","userId":"09657732965291711317"}},"outputId":"aff8d455-2acf-413b-ba39-545163b2bea9"},"source":["# Taining KNN classification model\n","knn = KNeighborsClassifier()\n","knn.fit(X_train, y_train)"],"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/plain":["KNeighborsClassifier()"],"text/html":["<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>KNeighborsClassifier()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">KNeighborsClassifier</label><div class=\"sk-toggleable__content\"><pre>KNeighborsClassifier()</pre></div></div></div></div></div>"]},"metadata":{},"execution_count":9}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oznSXxWlByDZ","executionInfo":{"status":"ok","timestamp":1723005855621,"user_tz":300,"elapsed":142,"user":{"displayName":"Akshat Patil","userId":"09657732965291711317"}},"outputId":"cc42e744-b74b-455a-c3b0-66bd866bc5c3"},"source":["# score on train data set\n","knn.score(X_train, y_train)"],"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.9433333333333334"]},"metadata":{},"execution_count":10}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ww9NARg8_QeC","executionInfo":{"status":"ok","timestamp":1723005856520,"user_tz":300,"elapsed":145,"user":{"displayName":"Akshat Patil","userId":"09657732965291711317"}},"outputId":"f736a330-2bd3-4780-dd6c-2e917afa72a2"},"source":["# Score the model on test dataset\n","knn.score(X_test, y_test)"],"execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.49"]},"metadata":{},"execution_count":11}]},{"cell_type":"markdown","metadata":{"id":"mGj8CtW3F24D"},"source":["<font color = 'indianred'>**Question**\n","- <font color = 'indianred'>**Why model perfrmed poorly on Test Data?**\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"ybUPz_UxGE3r"},"source":["<font color = 'indianred'>**Solution:**\n"]},{"cell_type":"code","metadata":{"id":"vYwtqO2BLjja"},"source":["X_test  = # code here\n","knn.score(# code here)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3UQetnS7qTE4"},"source":["## <font color = 'pickle'>**Data Leakage**\n","\n","*In statistics and machine learning, leakage (also known as data leakage or target leakage) is the use of information in the model training process which would not be expected to be available at prediction time, causing the predictive scores (metrics) to overestimate the model's utility when run in a production environment*\n","\n","Source: [Wikipedia](https://en.wikipedia.org/wiki/Leakage_(machine_learning)\n","\n","\n","* Leakage means that information is revealed to the model, giving it an unrealistic advantage to make better predictions.\n","* This could happen when test data is leaked into the training set or when data from the future is leaked to the past. Any time a model is given information that it shouldn’t have access to when making predictions in real-time in production, there is leakage.\n","\n","— Page 93, Feature Engineering for Machine Learning, 2018."]},{"cell_type":"markdown","metadata":{"id":"sVvI6lM-GXQh"},"source":["### <font color = 'pickle'>**Data Leakage - Preprocessing before train/test split**\n","\n","This is not a direct type of data leakage. The model is not trained on the test dataset. However, some information from the test data set is captured during the preprocessing step and made available to model during training."]},{"cell_type":"code","metadata":{"id":"1qXLNaisNENC"},"source":["# Creating 2000 normally distributed data with 2 class labels and 5000 features.\n","np.random.seed(123)\n","X = np.random.standard_normal((2000, 5000))\n","y = np.random.choice(2, 2000)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YrQtG9A3Q2FG","executionInfo":{"status":"ok","timestamp":1663516445374,"user_tz":300,"elapsed":208,"user":{"displayName":"Harpreet SIngh","userId":"15106381096049879330"}},"outputId":"ace2c1ec-86ef-40a1-abc6-856e9ed8c246"},"source":["y[0:5]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([1, 1, 1, 0, 1])"]},"metadata":{},"execution_count":14}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sncQQkZXQ97M","executionInfo":{"status":"ok","timestamp":1663516446933,"user_tz":300,"elapsed":98,"user":{"displayName":"Harpreet SIngh","userId":"15106381096049879330"}},"outputId":"8b28b366-a50b-4756-d5a9-68a77b42173d"},"source":["X[0:5]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[-1.0856306 ,  0.99734545,  0.2829785 , ..., -1.85971515,\n","         0.91382219, -1.35383977],\n","       [ 0.3187635 ,  1.51110387, -1.13662678, ..., -0.47226641,\n","         0.58196437,  0.97061286],\n","       [-1.24096967, -0.31294679, -0.84894679, ..., -1.82934642,\n","         0.9741791 , -0.6933265 ],\n","       [ 0.90756418,  1.68521718, -1.1163093 , ..., -1.40283982,\n","         1.04454086,  0.36928112],\n","       [ 1.03159348,  1.33194488,  0.09584389, ...,  0.65930018,\n","        -0.29068836,  0.98800033]])"]},"metadata":{},"execution_count":15}]},{"cell_type":"markdown","source":["Now, let's use SelectKBest() to get the top 20 features among 5000 features."],"metadata":{"id":"m1wMbIgIs0FJ"}},{"cell_type":"code","metadata":{"id":"Cbw_CZC5R0IN"},"source":["# select top 10 features\n","X_selected = SelectKBest(k=20).fit_transform(X, y)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0t737Fn9SDVx"},"source":["# split the data into train/test split\n","X_train, X_test, y_train, y_test = train_test_split( X_selected, y, random_state=200, train_size = 0.3)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"R7ZxCvTuSS9_","executionInfo":{"status":"ok","timestamp":1663516499663,"user_tz":300,"elapsed":97,"user":{"displayName":"Harpreet SIngh","userId":"15106381096049879330"}},"outputId":"f51849a6-877a-41ae-cc45-255188230e99"},"source":["# fit KNNClassifier on train data\n","knn = KNeighborsClassifier()\n","knn.fit(X_train, y_train)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["KNeighborsClassifier()"]},"metadata":{},"execution_count":18}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"quY6Qo-UShEA","executionInfo":{"status":"ok","timestamp":1663516504951,"user_tz":300,"elapsed":142,"user":{"displayName":"Harpreet SIngh","userId":"15106381096049879330"}},"outputId":"960f6bef-1def-473b-c920-d391e38c10d2"},"source":["knn.score(X_train, y_train)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.695"]},"metadata":{},"execution_count":19}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nH3_KTJcSo6W","executionInfo":{"status":"ok","timestamp":1663516507710,"user_tz":300,"elapsed":138,"user":{"displayName":"Harpreet SIngh","userId":"15106381096049879330"}},"outputId":"7851cd5d-b899-44ec-a4d5-d81a1e9aa572"},"source":["knn.score(X_test, y_test)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.5685714285714286"]},"metadata":{},"execution_count":20}]},{"cell_type":"markdown","metadata":{"id":"kteufTtOWRVt"},"source":["<font color = 'indianred'>**Questions:**\n","- What should be the expected score on test data?\n","- What is wrong in the above model?\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"NiuhYz3zGwhS"},"source":["#### <font color = 'indianred'>**Soution**\n"]},{"cell_type":"code","metadata":{"id":"2XF6YYygUvol"},"source":["# split the data into train/test split\n","X_train, X_test, y_train, y_test = train_test_split( X, y, random_state=200, train_size = 0.3)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kEz4phssVOyF"},"source":["preprocessor = SelectKBest(k=20)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pe80n1VbUySh"},"source":["X_train_selected = preprocessor.fit_transform(X_train, y_train)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ef_rqltmU4tA"},"source":["# note we do not need y_test in this step\n","# We are just selecting subset of X_test determined based on the training data\n","X_test_selected = preprocessor.transform(X_test)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6q7l2CSUVK-t","executionInfo":{"status":"ok","timestamp":1663516707787,"user_tz":300,"elapsed":106,"user":{"displayName":"Harpreet SIngh","userId":"15106381096049879330"}},"outputId":"4b6f285d-a4dc-4745-f288-86688fdd1ef3"},"source":["# fit KNNClassifier on train data\n","knn = KNeighborsClassifier()\n","knn.fit(X_train_selected, y_train)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["KNeighborsClassifier()"]},"metadata":{},"execution_count":25}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gTOnj4anVi4k","executionInfo":{"status":"ok","timestamp":1663516710459,"user_tz":300,"elapsed":121,"user":{"displayName":"Harpreet SIngh","userId":"15106381096049879330"}},"outputId":"cd1227d4-b4d1-4b37-c8fa-b0f31209da89"},"source":["knn.score(X_train_selected, y_train)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.7233333333333334"]},"metadata":{},"execution_count":26}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VF_CLZ2vU9T6","executionInfo":{"status":"ok","timestamp":1663516712469,"user_tz":300,"elapsed":264,"user":{"displayName":"Harpreet SIngh","userId":"15106381096049879330"}},"outputId":"97497561-4026-42df-b922-ab5adbed9d72"},"source":["knn.score(X_test_selected, y_test)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.49142857142857144"]},"metadata":{},"execution_count":27}]},{"cell_type":"markdown","metadata":{"id":"CfhdUB9oNzqW"},"source":["- Once we apply preprocesing after train/test split and use only transform on test dataset, we can see that model perfroms as expected i.e. it gives the same perfromance as random prediction (50% accuracy)."]},{"cell_type":"markdown","metadata":{"id":"UgW1ifB9G9eL"},"source":["### <font color = 'pickle'>**Data Leakage in Cross Validation**</font>\n","\n","Let us revisit KFold Cross Validation.\n","- The main purpose of the croos- validation is to use multiple train/valid folds and take the average score across valid folds in muliple splits to acees how the model will generalize to unseen data.\n","\n","<img src =\"https://drive.google.com/uc?export=view&id=1LQ_9W5Xeqnj4LNuM5mPmZV3M-nYiy8Hv\" width =400 >"]},{"cell_type":"markdown","metadata":{"id":"AkAMXmvObWKv"},"source":["**We will have similar data leakage issue as in the previous section, if we apply data transformation before cross validation**"]},{"cell_type":"code","metadata":{"id":"3OMvOXKAZ570"},"source":["# Geneate Data\n","np.random.seed(123)\n","X = np.random.standard_normal((2000, 5000))\n","y = np.random.choice(2, 2000)\n","# split the data into train/test split\n","X_train, X_test, y_train, y_test = train_test_split( X, y, random_state=200, train_size = 0.3)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"A0fryc4gaDnl","executionInfo":{"status":"ok","timestamp":1663516806051,"user_tz":300,"elapsed":248,"user":{"displayName":"Harpreet SIngh","userId":"15106381096049879330"}},"outputId":"1a3a7335-568b-4cac-9200-11f13b78ee4c"},"source":["preprocessor = SelectKBest(k=20)\n","X_train_selected = preprocessor.fit_transform(X_train, y_train)\n","X_test_selected = preprocessor.transform(X_test)\n","\n","# Cross Validation\n","knn = KNeighborsClassifier()\n","kfolds = KFold(n_splits = 5, random_state=0, shuffle = True)\n","scores = cross_val_score(knn, X_train_selected, y_train, cv=kfolds)\n","\n","scores.mean()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.5883333333333334"]},"metadata":{},"execution_count":30}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qVRRDROLJ7s2","executionInfo":{"status":"ok","timestamp":1663516810109,"user_tz":300,"elapsed":142,"user":{"displayName":"Harpreet SIngh","userId":"15106381096049879330"}},"outputId":"748289f3-2008-4e3a-bcfd-73bb2351e6bf"},"source":["knn.fit(X_train_selected, y_train)\n","knn.score(X_test_selected, y_test)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.49142857142857144"]},"metadata":{},"execution_count":31}]},{"cell_type":"markdown","metadata":{"id":"Z6EN0jKYKLnn"},"source":["- <font color = 'indianred'>**What do we observe here?**</font>\n"]},{"cell_type":"markdown","metadata":{"id":"vDkqFsRgK3Mw"},"source":["**Let  us see the inner working of above code**"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZOibVEwphNqp","executionInfo":{"status":"ok","timestamp":1663516900601,"user_tz":300,"elapsed":261,"user":{"displayName":"Harpreet SIngh","userId":"15106381096049879330"}},"outputId":"2927ca88-7384-49d6-a1c0-f2be530cd95d"},"source":["preprocessor = SelectKBest(k=20)\n","X_train_selected = preprocessor.fit_transform(X_train, y_train)\n","X_test_selected = preprocessor.transform(X_test)\n","\n","kfolds = KFold(n_splits = 5, random_state=0, shuffle = True)\n","scores =[]\n","for train, valid in kfolds.split(X_train_selected, y_train):\n","  knn = KNeighborsClassifier().fit(X_train_selected[train], y_train[train])\n","\n","  # the model is evaluated on X_train_selected[valid]\n","  score = knn.score(X_train_selected[valid], y_train[valid])\n","  scores.append(score)\n","\n","np.mean(scores)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.5883333333333334"]},"metadata":{},"execution_count":32}]},{"cell_type":"markdown","metadata":{"id":"XziqDgSONPO_"},"source":["<font color = 'indianred'>\n","\n","- **In cross validation model was evaluated on X_train_selected[valid].**\n","- **This does not reflect the perfromnace on unseen data, The model has seen the X_train_selected[valid] during the preprocessing step.**\n","-**The features were selected based on the complete X_train_selected.**\n","</font>"]},{"cell_type":"markdown","metadata":{"id":"Ut5FbWulHnps"},"source":["#### <font color = 'pickle'>**Solution : Need to include pre-processing inside  CV**"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nqQZ1V34McRp","executionInfo":{"status":"ok","timestamp":1663517032742,"user_tz":300,"elapsed":458,"user":{"displayName":"Harpreet SIngh","userId":"15106381096049879330"}},"outputId":"d5a8ef53-33b0-43a8-d54f-1e7fa385b924"},"source":["# let us do pre-processing inside the CV loop now.\n","\n","# code here"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mean Cross Validation Score\n"]},{"output_type":"execute_result","data":{"text/plain":["0.49000000000000005"]},"metadata":{},"execution_count":33}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4WYpq28LPgz7","executionInfo":{"status":"ok","timestamp":1663517041719,"user_tz":300,"elapsed":253,"user":{"displayName":"Harpreet SIngh","userId":"15106381096049879330"}},"outputId":"68d559f6-7a50-4568-ae79-1673ac7f5926"},"source":["print('Test Score')\n","knn.score(X_test_selected, y_test)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Test Score\n"]},{"output_type":"execute_result","data":{"text/plain":["0.49142857142857144"]},"metadata":{},"execution_count":34}]},{"cell_type":"markdown","metadata":{"id":"0I2zSWYFPlHU"},"source":["- In the above example, pre-processing was moved inside the cross-validation\n","- The pre-processing was done based on X_train[train]\n","- The best features were selected based on X_train[train]\n","- The model was fitted on X_train[train]\n","\n","- The same two steps are applied on X_train[valid]\n","- However, the features are selected based on X_train[train]\n","- We only used the transform method on X_train[valid]\n","- The model has never seen X_train[valid]\n","- Thus, the cross-validation score reflected the model's ability to generalize on unseen data\n","- Many pre-processing steps like imputing missing values with mean use statistics from training data. These pre-processing steps should be done inside the cross-validation loop."]},{"cell_type":"markdown","metadata":{"id":"obQ7Kpo2VnIp"},"source":["### <font color = 'pickle'>**Incorrect Hyperparameter Tuning**\n","<font color = 'indianred'>**Data leakage in Cross Validation step of Hyperparameter Tuning using GridSearch**"]},{"cell_type":"code","metadata":{"id":"8_DVBbaEY_UT"},"source":["np.random.seed(123)\n","X = np.random.standard_normal((2000, 5000))\n","y = np.random.choice(2, 2000)\n","# split the data into train/test split\n","X_train, X_test, y_train, y_test = train_test_split( X, y, random_state=200, train_size = 0.3)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"svoGTRsGV4RC","executionInfo":{"status":"ok","timestamp":1663517669357,"user_tz":300,"elapsed":1520,"user":{"displayName":"Harpreet SIngh","userId":"15106381096049879330"}},"outputId":"cd825e7f-b29a-4707-992a-3966be72bfc7"},"source":["preprocessor = SelectKBest(k=20)\n","X_train_selected = preprocessor.fit_transform(X_train, y_train)\n","X_test_selected = preprocessor.transform(X_test)\n","\n","kfolds = KFold(n_splits = 5, random_state=0, shuffle = True)\n","\n","# giving the param_grid values\n","param_grid = {'n_neighbors':  np.arange(1, 16, 2)}\n","\n","# Using GridSearchCV for kNN classification and returning the train_score as True\n","grid = GridSearchCV(KNeighborsClassifier(), param_grid=param_grid, cv=kfolds,\n","                   return_train_score=True)\n","\n","# Now fit the  GridSearchCV on the X_train, y_train by using fit() method\n","grid.fit(X_train_selected, y_train)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["GridSearchCV(cv=KFold(n_splits=5, random_state=0, shuffle=True),\n","             estimator=KNeighborsClassifier(),\n","             param_grid={'n_neighbors': array([ 1,  3,  5,  7,  9, 11, 13, 15])},\n","             return_train_score=True)"]},"metadata":{},"execution_count":36}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MtgqfwxBcIeB","executionInfo":{"status":"ok","timestamp":1663517675261,"user_tz":300,"elapsed":241,"user":{"displayName":"Harpreet SIngh","userId":"15106381096049879330"}},"outputId":"d61aab64-bf77-45e3-9f0a-d52efa88e2d9"},"source":["# The grid can be used to generate the mean of cross validation by using best_score_\n","# grid.best_params_ generates the best parameter i.e n_neighbor\n","# grid.score(data) gives the score on the data after fitting the model on # complete training data using the best hyper parameters\n","\n","print(f\"best mean cross-validation score: {grid.best_score_}\")\n","print(f\"best parameters: {grid.best_params_}\")\n","\n","# We can check the accuracy score of training dataset and test dataset.\n","print(f\"train-set score: {grid.score(X_train_selected, y_train):.3f}\")\n","print(f\"test-set score: {grid.score(X_test_selected, y_test):.3f}\")"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["best mean cross-validation score: 0.6216666666666667\n","best parameters: {'n_neighbors': 15}\n","train-set score: 0.695\n","test-set score: 0.489\n"]}]},{"cell_type":"markdown","metadata":{"id":"ufgdNi5YUp8K"},"source":["<font color = 'indianred'>**- Here again the cross validation score is over-optimistic**\n","\n","<font color = 'indianred'>**- Let us look at the inner working of this code**"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8SqVXaRXZn00","executionInfo":{"status":"ok","timestamp":1663517722703,"user_tz":300,"elapsed":550,"user":{"displayName":"Harpreet SIngh","userId":"15106381096049879330"}},"outputId":"b1c7688d-68e2-46b8-f755-2030b01300ab"},"source":["# split the data into train/test split\n","X_train, X_test, y_train, y_test = train_test_split( X, y, random_state=200, train_size = 0.3)\n","\n","preprocessor = SelectKBest(k=20)\n","X_train_selected = preprocessor.fit_transform(X_train, y_train)\n","X_test_selected = preprocessor.transform(X_test)\n","\n","# create empty list to store cross validation scores\n","cross_val_scores = []\n","kfolds = KFold(n_splits = 5, random_state=0, shuffle = True)\n","\n","# Taking k values ranging from 1 to 15 with a step of 2\n","neighbors = np.arange(1, 16, 2)\n","for i in neighbors:\n","    knn = KNeighborsClassifier(n_neighbors=i)\n","\n","    scores = cross_val_score(knn, X_train_selected, y_train, cv=kfolds)\n","\n","    # scores will give us five values corrsponding to five validation splits\n","    # We will take mean of these five values and append the mean value to cross_val_score list\n","    cross_val_scores.append(np.mean(scores))\n","\n","# consider the accuracy i.e highest score by using max() function\n","print(f\"best cross-validation score: {np.max(cross_val_scores):.3}\")\n","\n","# Consider the neighbor from the split which gives  maximum cross validation score\n","best_n_neighbors = neighbors[np.argmax(cross_val_scores)]\n","print(f\"best_value_of_k: {best_n_neighbors}\")\n","\n","# Retrain the model with the best_value_of_k\n","\n","knn = KNeighborsClassifier(n_neighbors = best_n_neighbors)\n","knn.fit(X_train_selected, y_train)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["best cross-validation score: 0.622\n","best_value_of_k: 15\n"]},{"output_type":"execute_result","data":{"text/plain":["KNeighborsClassifier(n_neighbors=15)"]},"metadata":{},"execution_count":38}]},{"cell_type":"markdown","metadata":{"id":"AwSwubyij3H9"},"source":["**Summary: GridSeachCV (Grid Serach Cross Validation to find best parameters)**\n","\n","<img src =\"https://drive.google.com/uc?export=view&id=1iK80BvXepRL1xHwJWqQa14BiNhJMVH9S\" width =600 >"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qQT0zyQcdLmU","executionInfo":{"status":"ok","timestamp":1663517761063,"user_tz":300,"elapsed":258,"user":{"displayName":"Harpreet SIngh","userId":"15106381096049879330"}},"outputId":"5af742e4-45cc-47f7-ebc4-14919bb8fdc1"},"source":["print(f\"best mean cross-validation score: {np.max(cross_val_scores):.3}\")\n","print(f\"best parameters: {best_n_neighbors}\")\n","\n","# We can check the accuracy score of training dataset and test dataset.\n","print(f\"train-set score: {knn.score(X_train_selected, y_train):.3f}\")\n","print(f\"test-set score: {knn.score(X_test_selected, y_test):.3f}\")"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["best mean cross-validation score: 0.622\n","best parameters: 15\n","train-set score: 0.695\n","test-set score: 0.489\n"]}]},{"cell_type":"markdown","metadata":{"id":"w7kIhQoNhopB"},"source":["<font color = 'dodgerblue'>- **The best hyperparameter is selected based on mean of cross_val_score.**\n","\n","<font color = 'dodgerblue'>- **However, as seen earlier, the cross_val_score is incorrect**\n","\n","<font color = 'indianred'>- **Key Take Away - Chain together pre-processing steps and classification/regression steps. The combined preprocessing steps and classifcation/regression steps should be considered as final model**"]},{"cell_type":"markdown","metadata":{"id":"IfnZ3LsUHxNl"},"source":["## <font color = 'pickle'>**What is Pipeline**\n","\n","<font color = 'indianred'>**Pipeline**</font> is a simple way of combining pre-processing and modeling steps so you can use the combination as if it were a single step. This will help us to counter both problems - **Inconsistent Preprocessing and Data Leakage**\n","\n","<font color = 'indianred'>**Advantages of a Pipeline:**</font>\n","\n","- <font color = 'indianred'>**Cleaner Code and reduce data leakage**:</font> We may have to apply multiple pre-processing steps. For example, mean imputation followed by variable transformation. Manually keeping track of training and validation folds at each stage can get messy and increase the chance of data leakage. Using pipelines will reduce the likelihood of data leakage significantly.\n","- <font color = 'indianred'>**Avoid inconsistent pre-processing**:</font> You are less likely to forget to apply a pre-processing step to a test or newer dataset. Hence pipelines can help us to avoid inconsistent pre-processing.\n","- <font color = 'indianred'>**Easier to Productionize**:</font> Since everything is done in one step, it becomes easier to deploy the model in **production pipelines**.\n","- <font color = 'indianred'>**More Options for Model Validation**:</font> We can optimize choices in pre-processing and classification/regression steps together."]},{"cell_type":"markdown","metadata":{"id":"PTlEMO7KhH9v"},"source":["## <font color = 'pickle'>**Pipelines - Use Cases**"]},{"cell_type":"markdown","source":["Till now, we have learned some common pitfalls like inconsistent pre-processing, data leakage issues in various scenarios like in cross-validation, during hyperparameter tuning, etc.\n","\n","So, now we will learn how to create pipelines that resolve these issues.\n","\n","\n","**Note:** *In the below explanations, we may use machine learning algorithms like Ridge, Lasso, LogisticeRegresssion, etc. In this lecture, we will not learn about these models; these models are just to show how to create pipelines with various ML models. All the required and necessary machine learning model implementation will be explained in future lectures.*"],"metadata":{"id":"4S97sHPCzhdW"}},{"cell_type":"markdown","metadata":{"id":"3dzuYGOQJdCD"},"source":["### <font color = 'pickle'>**Pipeline for inconsistent pre-processing**"]},{"cell_type":"code","metadata":{"id":"eRItHasmh7EN"},"source":["# Generate dataset\n","X12, y = make_classification(n_samples=1000, n_features=2, n_informative=2, n_redundant=0, random_state=7)\n","X3 = 1000 * np.random.standard_normal((1000, 1))\n","X = np.concatenate((X12,X3), axis =1)\n","X_train, X_test, y_train, y_test = train_test_split( X, y, random_state=200, train_size = 0.3)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vO69d2Q-h7EO"},"source":["# specify the preprocessing step and model\n","preprocessor = StandardScaler()\n","knn = KNeighborsClassifier()\n","# chain preprocessing step and model into one step using pipeline\n","\n","model = # code here"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jE6c5s7RjVWL"},"source":["- Pipeline chains together multiple transformations and final estimator in one step.\n","- The transformations and estimators are applied sequentially."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cmaHyvZ5h7EO","executionInfo":{"status":"ok","timestamp":1663518110477,"user_tz":300,"elapsed":161,"user":{"displayName":"Harpreet SIngh","userId":"15106381096049879330"}},"outputId":"b2763ac1-6439-42f0-c1d3-981a13e1cb17"},"source":["# fit the model on train dataset\n","model.fit(X_train, y_train)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Pipeline(steps=[('standardscaler', StandardScaler()),\n","                ('kneighborsclassifier', KNeighborsClassifier())])"]},"metadata":{},"execution_count":42}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5U2aba7Uj70U","executionInfo":{"status":"ok","timestamp":1663518112700,"user_tz":300,"elapsed":111,"user":{"displayName":"Harpreet SIngh","userId":"15106381096049879330"}},"outputId":"b150abb4-a519-4a8c-db5d-897d9fad2fac"},"source":["print(f'Model score on training dataset: {model.score(X_train, y_train)}')\n","print(f'Model score on test dataset: {model.score(X_test, y_test)}')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Model score on training dataset: 0.9433333333333334\n","Model score on test dataset: 0.9471428571428572\n"]}]},{"cell_type":"markdown","metadata":{"id":"VmTmpEthnT9Q"},"source":["**Since we have chained everything into one step, it is difficult to omit to apply pre-processing step on test data.**"]},{"cell_type":"markdown","metadata":{"id":"PJLaxllaf-CS"},"source":["### <font color = 'pickle'>**Pipeline for  Data leakage**"]},{"cell_type":"code","metadata":{"id":"hkVps_yDltC4"},"source":["np.random.seed(123)\n","X = np.random.standard_normal((2000, 5000))\n","y = np.random.choice(2, 2000)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kbcaLQwIl2Oj"},"source":["# make a pipeline to combine SelectKBest and KNeighborsClassifier() in to one step\n","model = # code here"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9HNrWkJ6mCcY"},"source":["# split the data into train/test split\n","X_train, X_test, y_train, y_test = train_test_split( X, y, random_state=200, train_size = 0.3)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1VxnXFCfmVQI","executionInfo":{"status":"ok","timestamp":1663518154867,"user_tz":300,"elapsed":347,"user":{"displayName":"Harpreet SIngh","userId":"15106381096049879330"}},"outputId":"e09bc3be-caff-4871-b524-5b51aaead879"},"source":["model.fit(X_train, y_train)\n","print(f'Model score on training dataset: {model.score(X_train, y_train)}')\n","print(f'Model score on test dataset: {model.score(X_test, y_test)}')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Model score on training dataset: 0.7233333333333334\n","Model score on test dataset: 0.49142857142857144\n"]}]},{"cell_type":"markdown","metadata":{"id":"IkQc8yUxnsVz"},"source":["<font color = 'indianred'>**Since we fit the combined model (preprocesing + classifier) on training data, the features are selected only based on the training data. When we call the score, both the steps are applied to the test dataset.**\n","\n"]},{"cell_type":"markdown","metadata":{"id":"6QUr7smcgLtY"},"source":["### <font color = 'pickle'>**Pipeline - Data leakage in cross validation**\n","**Task** - Redo the cross validation in section \"Data Leakage in Cross Validation\" using Pipelines. Exaplin how using pipelines will resolve the data leakage."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OzQFtE9ynCNB","executionInfo":{"status":"ok","timestamp":1663518231904,"user_tz":300,"elapsed":359,"user":{"displayName":"Harpreet SIngh","userId":"15106381096049879330"}},"outputId":"820ed03e-5860-4e20-ea13-acce581dc122"},"source":["kfolds = KFold(n_splits = 5, random_state=0, shuffle = True)\n","scores = cross_val_score(model, X_train, y_train, cv=kfolds)\n","scores.mean()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.49000000000000005"]},"metadata":{},"execution_count":48}]},{"cell_type":"markdown","metadata":{"id":"r4JEoVEKnN0q"},"source":["- In each cross-validation split, the model (pre-processor + classifier) is fitted on the training fold.\n","\n","- Therefore, during training, the model is never exposed to data from the validation fold.\n","- The scores are then calculated by applying the trained model (pre-processor + classifier) on the validation fold.\n","\n","- Thus, the cross-validation score gives a good approximation of how the model will perform on the unseen data."]},{"cell_type":"markdown","metadata":{"id":"UaaS36obKTbG"},"source":["### <font color = 'pickle'>**Pipeline and GridserachCV- Correct Hyperparameter Tuning**\n","\n","**Task** - Redo the hyperparameter tuning in section \"Incorrect Hyperparameter Tuning\" using Pipelines. Explain how using pipelines will resolve the data leakage."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"onkUxiGi2RA7","executionInfo":{"status":"ok","timestamp":1663518300620,"user_tz":300,"elapsed":3527,"user":{"displayName":"Harpreet SIngh","userId":"15106381096049879330"}},"outputId":"2caf21ce-6e7f-44b9-d342-f298bd7a2e9c"},"source":["model = make_pipeline(SelectKBest(k=20), KNeighborsClassifier())\n","# giving the param_grid values\n","param_grid = {'kneighborsclassifier__n_neighbors':  np.arange(1, 16, 2)}\n","\n","# Using GridSearchCV for kNN classification and returning the train_score as True\n","grid = GridSearchCV(model, param_grid=param_grid, cv=kfolds,\n","                   return_train_score=True)\n","\n","# Now fit the  GridSearchCV on the X_train, y_train by using fit() method\n","grid.fit(X_train, y_train)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["GridSearchCV(cv=KFold(n_splits=5, random_state=0, shuffle=True),\n","             estimator=Pipeline(steps=[('selectkbest', SelectKBest(k=20)),\n","                                       ('kneighborsclassifier',\n","                                        KNeighborsClassifier())]),\n","             param_grid={'kneighborsclassifier__n_neighbors': array([ 1,  3,  5,  7,  9, 11, 13, 15])},\n","             return_train_score=True)"]},"metadata":{},"execution_count":49}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"njJK1KqA1cCE","executionInfo":{"status":"ok","timestamp":1663518305183,"user_tz":300,"elapsed":240,"user":{"displayName":"Harpreet SIngh","userId":"15106381096049879330"}},"outputId":"9c33f28f-8230-4f07-80bd-9d983fdffba3"},"source":["print(f\"best mean cross-validation score: {grid.best_score_}\")\n","print(f\"best parameters: {grid.best_params_}\")\n","\n","# We can check the accuracy score of training dataset and test dataset.\n","print(f\"train-set score: {grid.score(X_train, y_train):.3f}\")\n","print(f\"test-set score: {grid.score(X_test, y_test):.3f}\")"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["best mean cross-validation score: 0.5066666666666666\n","best parameters: {'kneighborsclassifier__n_neighbors': 3}\n","train-set score: 0.782\n","test-set score: 0.496\n"]}]},{"cell_type":"markdown","metadata":{"id":"jTV764XA3DpM"},"source":["- Since we combined preprocessing and classifier into one step, there is no data leakage in the cross-validation step that is used to find the hyperparameters.\n","- We no longer see over-optimistic best cross-validation score."]},{"cell_type":"markdown","metadata":{"id":"TyfjA3ixKbmQ"},"source":["### <font color = 'pickle'>**Optimizing PreProcessing and Classifier together**"]},{"cell_type":"markdown","metadata":{"id":"pqS8lsZr8qTg"},"source":["**Task**\n","- Create a pipeline where we add polynomial features (PolynomialFeatures(), followed by scaling (StandardScaler() and finally KNeighborsRegressor().\n","- Optimize Polynomial features (degree of 1, 2, and 3) and KNeighborsRegressor (n_neighbors = 1 to 10) jointly in a single pipeline. Here you will evaluate different combinations of pre-processing steps and KNeighborsRegressor."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sdKj5nD28b6E","executionInfo":{"status":"ok","timestamp":1663518444998,"user_tz":300,"elapsed":143,"user":{"displayName":"Harpreet SIngh","userId":"15106381096049879330"}},"outputId":"8a94af9f-2a01-40e6-ecc0-07dafd23f8a1"},"source":["diabetes = load_diabetes()\n","print(tw.fill(diabetes.DESCR, 100))\n","X_train, X_test, y_train, y_test = train_test_split(\n","    diabetes.data, diabetes.target, random_state=1)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":[".. _diabetes_dataset:  Diabetes dataset ----------------  Ten baseline variables, age, sex, body\n","mass index, average blood pressure, and six blood serum measurements were obtained for each of n =\n","442 diabetes patients, as well as the response of interest, a quantitative measure of disease\n","progression one year after baseline.  **Data Set Characteristics:**    :Number of Instances: 442\n",":Number of Attributes: First 10 columns are numeric predictive values    :Target: Column 11 is a\n","quantitative measure of disease progression one year after baseline    :Attribute Information:\n","- age     age in years       - sex       - bmi     body mass index       - bp      average blood\n","pressure       - s1      tc, total serum cholesterol       - s2      ldl, low-density lipoproteins\n","- s3      hdl, high-density lipoproteins       - s4      tch, total cholesterol / HDL       - s5\n","ltg, possibly log of serum triglycerides level       - s6      glu, blood sugar level  Note: Each of\n","these 10 feature variables have been mean centered and scaled by the standard deviation times\n","`n_samples` (i.e. the sum of squares of each column totals 1).  Source URL:\n","https://www4.stat.ncsu.edu/~boos/var.select/diabetes.html  For more information see: Bradley Efron,\n","Trevor Hastie, Iain Johnstone and Robert Tibshirani (2004) \"Least Angle Regression,\" Annals of\n","Statistics (with discussion), 407-499.\n","(https://web.stanford.edu/~hastie/Papers/LARS/LeastAngle_2002.pdf)\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wi0RHeAT8hk4","executionInfo":{"status":"ok","timestamp":1663518488019,"user_tz":300,"elapsed":1138,"user":{"displayName":"Harpreet SIngh","userId":"15106381096049879330"}},"outputId":"7475bef4-08e0-44aa-a5fe-6705323bb9b1"},"source":["model  = # code here\n","param_grid = # code here\n","grid = GridSearchCV(model, param_grid=param_grid, n_jobs=-1, return_train_score=True)\n","grid.fit(X_train, y_train)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["GridSearchCV(estimator=Pipeline(steps=[('polynomialfeatures',\n","                                        PolynomialFeatures()),\n","                                       ('standardscaler', StandardScaler()),\n","                                       ('kneighborsregressor',\n","                                        KNeighborsRegressor())]),\n","             n_jobs=-1,\n","             param_grid={'kneighborsregressor__n_neighbors': range(1, 10),\n","                         'polynomialfeatures__degree': [1, 2, 3]},\n","             return_train_score=True)"]},"metadata":{},"execution_count":55}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rf8zk9TAUsQo","executionInfo":{"status":"ok","timestamp":1663518495312,"user_tz":300,"elapsed":123,"user":{"displayName":"Harpreet SIngh","userId":"15106381096049879330"}},"outputId":"c0750df9-d485-4c6e-a083-8c20892cdb97"},"source":["print(f\"best mean cross-validation score: {grid.best_score_}\")\n","print(f\"best parameters: {grid.best_params_}\")\n","\n","# We can check the accuracy score of training dataset and test dataset.\n","print(f\"train-set score: {grid.score(X_train, y_train):.3f}\")\n","print(f\"test-set score: {grid.score(X_test, y_test):.3f}\")"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["best mean cross-validation score: 0.41862723410123037\n","best parameters: {'kneighborsregressor__n_neighbors': 9, 'polynomialfeatures__degree': 1}\n","train-set score: 0.550\n","test-set score: 0.414\n"]}]},{"cell_type":"markdown","metadata":{"id":"ZRcnChymKscc"},"source":["### <font color = 'pickle'>**Using Named Steps and Multiple Models**\n","**Task**: Create a pipeline with folllowing\n","- scale all variables follwed by regression.\n","  - for scaling pipeline should evaluate StandardScaler(), MinMaxScaler(), 'passthrough' as options for scaling.\n","  - pipeline should evaluate Ridge(), Lasso() as option fpr regression\n","  - Both Ridge() and Lasso() has a hyperparamter alpha. Specify the range np.logspace() for these hyperparameter."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fIfduzegVvlP","executionInfo":{"status":"ok","timestamp":1663518583576,"user_tz":300,"elapsed":835,"user":{"displayName":"Harpreet SIngh","userId":"15106381096049879330"}},"outputId":"7762546a-82fd-4dc6-95f7-502cef37a36f"},"source":["model = # code here\n","\n","param_grid = # code here\n","\n","grid = GridSearchCV(model, param_grid, n_jobs=-1, return_train_score=True)\n","grid.fit(X_train, y_train)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["GridSearchCV(estimator=Pipeline(steps=[('scaler', StandardScaler()),\n","                                       ('regressor', Ridge())]),\n","             n_jobs=-1,\n","             param_grid={'regressor': [Ridge(), Lasso()],\n","                         'regressor__alpha': array([1.e-03, 1.e-02, 1.e-01, 1.e+00, 1.e+01, 1.e+02, 1.e+03]),\n","                         'scaler': [StandardScaler(), MinMaxScaler(),\n","                                    'passthrough']},\n","             return_train_score=True)"]},"metadata":{},"execution_count":57}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4fBWp72sX1gu","executionInfo":{"status":"ok","timestamp":1663518596021,"user_tz":300,"elapsed":96,"user":{"displayName":"Harpreet SIngh","userId":"15106381096049879330"}},"outputId":"791fbbf5-803c-4c57-c6d5-901c698763f9"},"source":["print(f\"best mean cross-validation score: {grid.best_score_}\")\n","print(f\"best parameters: {grid.best_params_}\")\n","\n","# We can check the accuracy score of training dataset and test dataset.\n","print(f\"train-set score: {grid.score(X_train, y_train):.3f}\")\n","print(f\"test-set score: {grid.score(X_test, y_test):.3f}\")"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["best mean cross-validation score: 0.48071210439622797\n","best parameters: {'regressor': Ridge(), 'regressor__alpha': 1.0, 'scaler': MinMaxScaler()}\n","train-set score: 0.533\n","test-set score: 0.438\n"]}]},{"cell_type":"markdown","metadata":{"id":"UhtPjYSSLBvo"},"source":["### <font color = 'pickle'>**Multiple Models with Different Hyper Parameters**\n","\n","**Task**: Create a pipeline with folllowing\n","\n","- The ppipeline should evaluate following two options\n","  - (1) scaler followed by Ridge(). For Ridge Regression , you will tune hyperparameter alpha and evaluate following values: [0.1, 1]. For scaler give the following options -- StandardScaler(), MinMaxScaler(), 'passthrough'\n","\n","  - (2) scaler followed by DecisionTreeRegressor().  For DecisionTreeRegressor(), you will tune hyperparameter max_depth and evaluate following values [2, 3, 4]. For scaler you will only use 'passthrough'.\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"URLPkHJfYGep","executionInfo":{"status":"ok","timestamp":1663518672332,"user_tz":300,"elapsed":4755,"user":{"displayName":"Harpreet SIngh","userId":"15106381096049879330"}},"outputId":"516d7ebd-bdf2-4d15-bd5b-9bb77d73c368"},"source":["model  = Pipeline([('scaler', StandardScaler()), ('regressor', Ridge())])\n","\n","param_grid = [{'regressor': [DecisionTreeRegressor()],\n","               'regressor__max_depth': [2, 3, 4],\n","               'scaler': ['passthrough']},\n","              {'regressor': [Ridge()],\n","               'regressor__alpha': [0.1, 1],\n","               'scaler': [StandardScaler(), MinMaxScaler(), 'passthrough']}\n","             ]\n","grid = GridSearchCV(model, param_grid, n_jobs=-1, return_train_score=True)\n","grid.fit(X_train, y_train)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["GridSearchCV(estimator=Pipeline(steps=[('scaler', StandardScaler()),\n","                                       ('regressor', Ridge())]),\n","             n_jobs=-1,\n","             param_grid=[{'regressor': [DecisionTreeRegressor()],\n","                          'regressor__max_depth': [2, 3, 4],\n","                          'scaler': ['passthrough']},\n","                         {'regressor': [Ridge(alpha=1)],\n","                          'regressor__alpha': [0.1, 1],\n","                          'scaler': [StandardScaler(), MinMaxScaler(),\n","                                     'passthrough']}],\n","             return_train_score=True)"]},"metadata":{},"execution_count":59}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uGZZeLBGxYp0","executionInfo":{"status":"ok","timestamp":1663518675679,"user_tz":300,"elapsed":118,"user":{"displayName":"Harpreet SIngh","userId":"15106381096049879330"}},"outputId":"607fae2f-c4e7-416d-eceb-64815bc668e9"},"source":["print(f\"best mean cross-validation score: {grid.best_score_}\")\n","print(f\"best parameters: {grid.best_params_}\")\n","\n","# We can check the accuracy score of training dataset and test dataset.\n","print(f\"train-set score: {grid.score(X_train, y_train):.3f}\")\n","print(f\"test-set score: {grid.score(X_test, y_test):.3f}\")"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["best mean cross-validation score: 0.48071210439622797\n","best parameters: {'regressor': Ridge(alpha=1), 'regressor__alpha': 1, 'scaler': MinMaxScaler()}\n","train-set score: 0.533\n","test-set score: 0.438\n"]}]},{"cell_type":"markdown","metadata":{"id":"z7ZPdzRjKsyn"},"source":["###  <font color = 'pickle'>**Different PreProcessing Steps for Different Variables**"]},{"cell_type":"markdown","metadata":{"id":"Jf4Tb78M_-bV"},"source":["You can download the Titanic dataset using the commands below and see it’s description at https://www.openml.org/d/40945"]},{"cell_type":"code","metadata":{"id":"P5oA3Rf6AtyZ"},"source":["X, y = fetch_openml(\"Titanic\", version=1, as_frame=True, return_X_y=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"e-stJUgAANf9","executionInfo":{"status":"ok","timestamp":1663518705300,"user_tz":300,"elapsed":141,"user":{"displayName":"Harpreet SIngh","userId":"15106381096049879330"}},"outputId":"950dca07-194e-4bb5-bf2c-57097d4eb439"},"source":["X.info()"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 1309 entries, 0 to 1308\n","Data columns (total 13 columns):\n"," #   Column     Non-Null Count  Dtype   \n","---  ------     --------------  -----   \n"," 0   pclass     1309 non-null   float64 \n"," 1   name       1309 non-null   object  \n"," 2   sex        1309 non-null   category\n"," 3   age        1046 non-null   float64 \n"," 4   sibsp      1309 non-null   float64 \n"," 5   parch      1309 non-null   float64 \n"," 6   ticket     1309 non-null   object  \n"," 7   fare       1308 non-null   float64 \n"," 8   cabin      295 non-null    object  \n"," 9   embarked   1307 non-null   category\n"," 10  boat       486 non-null    object  \n"," 11  body       121 non-null    float64 \n"," 12  home.dest  745 non-null    object  \n","dtypes: category(2), float64(6), object(5)\n","memory usage: 115.4+ KB\n"]}]},{"cell_type":"code","metadata":{"id":"WI5SL9O8Ady9"},"source":["X =  X[['pclass','sex','sibsp','parch']]\n","categorical = ['sex']\n","continuous=['pclass','sibsp','parch']\n","X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LrcUB5G-_GsN","executionInfo":{"status":"ok","timestamp":1663518713211,"user_tz":300,"elapsed":153,"user":{"displayName":"Harpreet SIngh","userId":"15106381096049879330"}},"outputId":"586e1b43-bf9e-4ddc-8662-8de756f8e295"},"source":["X_train.info()"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["<class 'pandas.core.frame.DataFrame'>\n","Int64Index: 981 entries, 747 to 684\n","Data columns (total 4 columns):\n"," #   Column  Non-Null Count  Dtype   \n","---  ------  --------------  -----   \n"," 0   pclass  981 non-null    float64 \n"," 1   sex     981 non-null    category\n"," 2   sibsp   981 non-null    float64 \n"," 3   parch   981 non-null    float64 \n","dtypes: category(1), float64(3)\n","memory usage: 31.7 KB\n"]}]},{"cell_type":"markdown","metadata":{"id":"eWo_55h4x8f0"},"source":["#### <font color = 'pickle'>**Column Transformer**\n","Task : Logistic with standar sacler (for continuous variables only) and onehot encoder (for categorical variable only)."]},{"cell_type":"code","metadata":{"id":"3G-fuk6mOk9E"},"source":["preprocess1 = make_column_transformer(\n","    (StandardScaler(),continuous),\n","    (OneHotEncoder(drop='first'), categorical),\n","    remainder='passthrough'\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tXl92N2zO59N"},"source":["from sklearn.linear_model import LogisticRegression\n","model = make_pipeline( preprocess1, LogisticRegression())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"r6jlcs9HPAIi","executionInfo":{"status":"ok","timestamp":1663518767710,"user_tz":300,"elapsed":1,"user":{"displayName":"Harpreet SIngh","userId":"15106381096049879330"}},"outputId":"7014cf02-8525-4037-9ccb-816d2e50c52c"},"source":["model.fit(X_train, y_train)\n","model.score(X_test, y_test)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.7865853658536586"]},"metadata":{},"execution_count":67}]},{"cell_type":"markdown","metadata":{"id":"xqEHX4JDyEXv"},"source":["#### <font color = 'pickle'>**Feature Engine**"]},{"cell_type":"code","metadata":{"id":"bbO-8_2EWDLD","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1663518794335,"user_tz":300,"elapsed":4897,"user":{"displayName":"Harpreet SIngh","userId":"15106381096049879330"}},"outputId":"6ddf5155-7053-4bc2-8811-ddbdbfd89db5"},"source":["!pip install feature_engine -qq"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[?25l\r\u001b[K     |█▏                              | 10 kB 13.9 MB/s eta 0:00:01\r\u001b[K     |██▍                             | 20 kB 4.5 MB/s eta 0:00:01\r\u001b[K     |███▋                            | 30 kB 6.5 MB/s eta 0:00:01\r\u001b[K     |████▊                           | 40 kB 3.1 MB/s eta 0:00:01\r\u001b[K     |██████                          | 51 kB 3.3 MB/s eta 0:00:01\r\u001b[K     |███████▏                        | 61 kB 3.9 MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 71 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 81 kB 4.8 MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 92 kB 4.8 MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 102 kB 4.0 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 112 kB 4.0 MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 122 kB 4.0 MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 133 kB 4.0 MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 143 kB 4.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 153 kB 4.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 163 kB 4.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 174 kB 4.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 184 kB 4.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 194 kB 4.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 204 kB 4.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 215 kB 4.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 225 kB 4.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 235 kB 4.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 245 kB 4.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 256 kB 4.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 266 kB 4.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 276 kB 4.0 MB/s \n","\u001b[?25h"]}]},{"cell_type":"code","metadata":{"id":"r1Ee8QvUV_vV"},"source":["from feature_engine.encoding import OneHotEncoder as fe_ohe\n","from feature_engine.wrappers import SklearnTransformerWrapper"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vtOFU-xsPh49"},"source":["model = Pipeline([\n","\n","    ('one_hot_encoder',\n","      # code here\n","    ('scalar',\n","      # code here,\n","    ('logreg',\n","     LogisticRegression())\n","])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WfgKQXZVWWe0","executionInfo":{"status":"ok","timestamp":1663518821505,"user_tz":300,"elapsed":1,"user":{"displayName":"Harpreet SIngh","userId":"15106381096049879330"}},"outputId":"9041d50d-803d-45e5-8a4b-30994172e2ee"},"source":["model.fit(X_train, y_train)\n","model.score(X_test, y_test)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.7865853658536586"]},"metadata":{},"execution_count":71}]},{"cell_type":"markdown","metadata":{"id":"F4t0dQSSMwGd"},"source":["### <font color = 'pickle'>**Different PreProcessing Steps and Different Models**\n","\n","Task : (1) Decsion Tree with One hot encoder (categorical) and (2) KNNRegression with standar sacler (continuous) and onehot encoder (categorical)."]},{"cell_type":"code","metadata":{"id":"Xc6D2-oCXkH1"},"source":["preprocess1= make_column_transformer(\n","    (OneHotEncoder(drop='first'), categorical),\n","    remainder='passthrough'\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3ZZXVJoKXoNq"},"source":["preprocess2 = make_column_transformer(\n","    (StandardScaler(),continuous),\n","    (OneHotEncoder(drop='first'), categorical),\n","    remainder='passthrough'\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AJ9t1M4OQB-B"},"source":["model  = Pipeline([('preprocessor', preprocess2), ('regressor', Ridge())])\n","\n","param_grid = [{'regressor': [DecisionTreeRegressor()],\n","               'preprocessor' : [preprocess1]},\n","\n","              {'regressor': [Ridge()],\n","               'preprocessor' : [preprocess2]}\n","\n","             ]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"s3trt7W_Yfr-","executionInfo":{"status":"ok","timestamp":1663518900848,"user_tz":300,"elapsed":687,"user":{"displayName":"Harpreet SIngh","userId":"15106381096049879330"}},"outputId":"0989ebb4-68bf-4e12-f923-c99a4ea82885"},"source":["grid = GridSearchCV(model, param_grid, n_jobs=-1, return_train_score=True)\n","grid.fit(X_train, y_train)\n","grid.score(X_test, y_test)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.3341705140118807"]},"metadata":{},"execution_count":75}]}]}