{"cells":[{"cell_type":"markdown","metadata":{"id":"gaGkLWaZZv45"},"source":["# <font color = 'pickle'>**Lecture : Intoduction to PyTorch Tensors**\n"]},{"cell_type":"markdown","metadata":{"id":"sa1p0E6Sduea"},"source":["# <font color = 'pickle'>**Importing PyTorch Library**"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":167,"status":"ok","timestamp":1723260904921,"user":{"displayName":"Akshat Patil","userId":"09657732965291711317"},"user_tz":300},"id":"mYXb-ESsbaut","outputId":"41a99af8-bbdd-4564-fbdc-726bac419b05"},"outputs":[{"output_type":"stream","name":"stdout","text":["Sat Aug 10 03:35:05 2024       \n","+---------------------------------------------------------------------------------------+\n","| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n","|-----------------------------------------+----------------------+----------------------+\n","| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n","|                                         |                      |               MIG M. |\n","|=========================================+======================+======================|\n","|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n","| N/A   51C    P8              10W /  70W |      0MiB / 15360MiB |      0%      Default |\n","|                                         |                      |                  N/A |\n","+-----------------------------------------+----------------------+----------------------+\n","                                                                                         \n","+---------------------------------------------------------------------------------------+\n","| Processes:                                                                            |\n","|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n","|        ID   ID                                                             Usage      |\n","|=======================================================================================|\n","|  No running processes found                                                           |\n","+---------------------------------------------------------------------------------------+\n"]}],"source":["!nvidia-smi"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"chOMYVyUVNof","executionInfo":{"status":"ok","timestamp":1723261201093,"user_tz":300,"elapsed":174,"user":{"displayName":"Akshat Patil","userId":"09657732965291711317"}}},"outputs":[],"source":["import torch\n","import numpy as np"]},{"cell_type":"markdown","metadata":{"id":"IHWosHogaREo"},"source":["# <font color = 'pickle'>**Tensors**\n","\n","- Tensors are the basic building blocks of any deep learning network.\n","\n","- They are used to represent all the different types of data be it images, sound files, text data etc.\n","\n","- Tensors are **order N-matrix**.\n","\n","\n","If N=1, tensor will basically be a **vector**.\n","If N=2, tensor will be a **2-d matrix**.\n","\n","Why Tensors and not NumPy arrays?\n","\n","- NumPy only supports CPU computation.\n","- Tensor class supports automatic differentiation."]},{"cell_type":"markdown","metadata":{"id":"EJREGcIPcIm2"},"source":["**Let us start by importing PyTorch library and understand some of the basic functions on tensors.**"]},{"cell_type":"markdown","source":["<img src = \"https://miro.medium.com/v2/resize:fit:891/0*jGB1CGQ9HdeUwlgB\" width =600 >"],"metadata":{"id":"TPUE3EKX2_Jr"}},{"cell_type":"markdown","metadata":{"id":"_9Va52NtqsTo"},"source":["## <font color = 'pickle'>**Scalar**\n","- rank-0 tensor\n"]},{"cell_type":"code","source":[],"metadata":{"id":"sBZ8-LXKbNge"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":177,"status":"ok","timestamp":1723261210029,"user":{"displayName":"Akshat Patil","userId":"09657732965291711317"},"user_tz":300},"id":"agJbamRzhFh8","outputId":"50b2c7dd-381d-43fe-b2a7-2dffd4f8205c"},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor(1.)\n","\n","size:\n","torch.Size([])\n","\n","number of dimensions:\n","0\n","\n","Data Type:\n","torch.float32\n"]}],"source":["t = torch.tensor(1.)\n","print(t)\n","print(\"\\nsize:\",t.shape, sep ='\\n')\n","print(\"\\nnumber of dimensions:\", t.dim(), sep = \"\\n\")\n","print(\"\\nData Type:\", t.dtype, sep = \"\\n\")"]},{"cell_type":"markdown","metadata":{"id":"i8JRukoprQvo"},"source":["## <font color = 'pickle'>**Vector**\n","- rank-1 tensor"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":168,"status":"ok","timestamp":1723261211729,"user":{"displayName":"Akshat Patil","userId":"09657732965291711317"},"user_tz":300},"id":"Jklq36xT3doI","outputId":"f6ea336b-b922-4f2c-f1ea-9aa6bf761996"},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([1., 2.])\n","\n","size:\n","torch.Size([2])\n","\n","number of dimensions:\n","1\n","\n","Data Type:\n","torch.float32\n"]}],"source":["t = torch.tensor([1., 2])\n","print(t)\n","print(\"\\nsize:\",t.shape, sep ='\\n')\n","print(\"\\nnumber of dimensions:\", t.dim(), sep = \"\\n\")\n","print(\"\\nData Type:\", t.dtype, sep = \"\\n\")"]},{"cell_type":"markdown","metadata":{"id":"SMnpra7xr6Kf"},"source":["## <font color = 'pickle'>**Matrix**\n","- rank 2 tensor\n","\n","Matrices are 2-d arrays with size `n x m`. Here, n: number of rows and m: number of columns.\n","\n","If `m = n`, then the matrix is known as a `square matrix`.\n","\n","Precisely, matrices can be represented as:\n","$$\\mathbf{X}=\\begin{bmatrix} x_{11} & x_{12} & \\cdots & x_{1n} \\\\ x_{21} & x_{22} & \\cdots & x_{2n} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ x_{m1} & x_{m2} & \\cdots & x_{mn} \\\\ \\end{bmatrix}$$\n","<br>\n","\n"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":175,"status":"ok","timestamp":1723261213846,"user":{"displayName":"Akshat Patil","userId":"09657732965291711317"},"user_tz":300},"id":"oonCHYyJr6Kg","outputId":"9a6d874c-bc8f-4926-8389-898f19a4e692"},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[1., 2., 3.],\n","        [4., 5., 6.]])\n","\n","size:\n","torch.Size([2, 3])\n","\n","number of dimensions:\n","2\n","\n","Data Type:\n","torch.float32\n"]}],"source":["t = torch.tensor([\n","     [1., 2, 3],\n","     [4, 5, 6]\n","    ])\n","print(t)\n","print(\"\\nsize:\",t.shape, sep ='\\n')\n","print(\"\\nnumber of dimensions:\", t.dim(), sep = \"\\n\")\n","print(\"\\nData Type:\", t.dtype, sep = \"\\n\")"]},{"cell_type":"markdown","metadata":{"id":"Hblncsd7wbVz"},"source":["\n","<img src = \"https://drive.google.com/uc?export=view&id=1822fQJQuXtzZ7DmO86pUXUj4aU9ZLor_\" width =600 >"]},{"cell_type":"markdown","metadata":{"id":"7USc0U8XsMbI"},"source":["## <font color = 'pickle'>**Higher Order Tensors**"]},{"cell_type":"markdown","metadata":{"id":"uuHbPHclzl7Z"},"source":["\n","### <font color = 'pickle'>**rank-3 tensor**"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":167,"status":"ok","timestamp":1723261218087,"user":{"displayName":"Akshat Patil","userId":"09657732965291711317"},"user_tz":300},"id":"QkAU2DQbsMbO","outputId":"fcd4dea2-d0ee-48a8-8369-f87b2526d406"},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[[1, 2],\n","         [3, 4]],\n","\n","        [[5, 6],\n","         [7, 8]],\n","\n","        [[5, 6],\n","         [7, 8]]])\n","\n","size:\n","torch.Size([3, 2, 2])\n","\n","number of dimensions:\n","3\n","\n","Data Type:\n","torch.int64\n"]}],"source":["t = torch.tensor([\n","    [[1, 2], [3,4]],\n","    [[5, 6], [7,8]],\n","    [[5, 6], [7,8]]\n","                  ])\n","print(t)\n","print(\"\\nsize:\",t.shape, sep ='\\n')\n","print(\"\\nnumber of dimensions:\", t.dim(), sep = \"\\n\")\n","print(\"\\nData Type:\", t.dtype, sep = \"\\n\")"]},{"cell_type":"markdown","metadata":{"id":"H2oM6QJXyF7m"},"source":["<img src = \"https://drive.google.com/uc?export=view&id=184X0Qjn0lwuJRSFoh_lEmR9v7yF7GxaA\" width =600 >\n","\n","Image source: https://dev.to/sandeepbalachandran/machine-learning-going-furthur-with-cnn-part-2-41km"]},{"cell_type":"markdown","metadata":{"id":"ojkafKrazhSM"},"source":["### <font color = 'pickle'>**rank-4 tensor**"]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":218,"status":"ok","timestamp":1723261220373,"user":{"displayName":"Akshat Patil","userId":"09657732965291711317"},"user_tz":300},"id":"RuHWcJh-zw8L","outputId":"9c9cd225-54db-43cb-cec6-5fc4314f87ff"},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[[[1, 2],\n","          [3, 4]],\n","\n","         [[5, 6],\n","          [7, 8]],\n","\n","         [[5, 6],\n","          [7, 8]]],\n","\n","\n","        [[[1, 2],\n","          [3, 4]],\n","\n","         [[5, 6],\n","          [7, 8]],\n","\n","         [[5, 6],\n","          [7, 8]]]])\n","\n","size:\n","torch.Size([2, 3, 2, 2])\n","\n","number of dimensions:\n","4\n","\n","Data Type:\n","torch.int64\n"]}],"source":["t1 = torch.stack((t,t))\n","print(t1)\n","# print(t)\n","print(\"\\nsize:\",t1.shape, sep ='\\n')\n","print(\"\\nnumber of dimensions:\", t1.dim(), sep = \"\\n\")\n","print(\"\\nData Type:\", t1.dtype, sep = \"\\n\")"]},{"cell_type":"markdown","metadata":{"id":"mH1bd2kaz5ZO"},"source":["<img src = \"https://drive.google.com/uc?export=view&id=189RzBY0oYuih-dZjNAT79FIVf3ZUp_HY\" width =600 >"]},{"cell_type":"markdown","metadata":{"id":"vCIviyFV2m04"},"source":[" # <font color = 'pickle'> **Python list**"]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":185,"status":"ok","timestamp":1723261235736,"user":{"displayName":"Akshat Patil","userId":"09657732965291711317"},"user_tz":300},"id":"3POAdGGfA12q","outputId":"1776d514-d3f1-4c0b-e010-ebc8fa22316c"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["int"]},"metadata":{},"execution_count":16}],"source":["scalar = 4\n","type(scalar)"]},{"cell_type":"code","execution_count":17,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1723261235926,"user":{"displayName":"Akshat Patil","userId":"09657732965291711317"},"user_tz":300},"id":"r8p1P6rC3IXq","outputId":"1aaa51eb-f2a7-4ba3-8b1d-36f9854280cb"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["list"]},"metadata":{},"execution_count":17}],"source":["my_list = [[1., 2], [3,4]]\n","type(my_list)"]},{"cell_type":"code","execution_count":18,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1723261236287,"user":{"displayName":"Akshat Patil","userId":"09657732965291711317"},"user_tz":300},"id":"1czex0Vr3MG-","outputId":"4a158f94-b87b-424e-bc69-da3027cda6c0"},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[[1, 2],\n","         [3, 4]],\n","\n","        [[5, 6],\n","         [7, 8]],\n","\n","        [[5, 6],\n","         [7, 8]]])\n","\n","Data Typr:\n","torch.float32\n"]}],"source":["my_tensor = torch.tensor([[1., 2], [3,4]])\n","type(my_tensor)\n","print(t)\n","print(\"\\nData Typr:\", my_tensor.dtype, sep = \"\\n\")"]},{"cell_type":"markdown","metadata":{"id":"yfg-TJpb2rmg"},"source":["# <font color = 'pickle'>**Difference between list and Array/tensor**</font>\n","\n","| <font size =5> Python List                       | <font size =5>Tensor/Array                     |\n","|-----------------------------------|----------------------------------|\n","| <font size =5>Mixed types allowed               | <font size =5>Same type required               |\n","|<font size =5> Elements can be added or removed  | <font size =5>Elements cannot be added or removed               \n","| <font size =5>Basic Python operations           | <font size =5>Supports mathematical operations                \n","|<font size =5>Numerical Computtaions are slow    |<font size =5>Numerical Computtaions are fast\n"]},{"cell_type":"markdown","metadata":{"id":"XfoOIXQK6Bm-"},"source":["\n"]},{"cell_type":"markdown","metadata":{"id":"Y6zZfHhsu92j"},"source":["# <font color = 'pickle'>**Conversion to other Python Objects**"]},{"cell_type":"code","execution_count":19,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":154,"status":"ok","timestamp":1723261252132,"user":{"displayName":"Akshat Patil","userId":"09657732965291711317"},"user_tz":300},"id":"bHmHIXZB-Aw7","outputId":"d3e2d645-1998-4783-cca7-16f0273566ae"},"outputs":[{"output_type":"stream","name":"stdout","text":["<class 'numpy.ndarray'>\n","<class 'torch.Tensor'>\n","torch.LongTensor\n"]}],"source":["# Initializing a tensor\n","t = torch.arange(10)\n","\n","# Converting tensor t to numpy array using numpy() mehod\n","arr = t.numpy()\n","\n","# Converting numpy array to tensor T using tensor() method\n","T = torch.tensor(arr)\n","\n","# Printing data type of arr and T\n","print(type(arr), type(T), T.type(), sep='\\n')"]},{"cell_type":"markdown","metadata":{"id":"K8ot3be10F1e"},"source":["We can also use torch.from_numpy() and torch.as_tensor() to convert numpy array to PyTorch Tensor. However, with these methods, the PyTorch tensor and the source NumPy array share the same memory. This means that changes to one affect the other. However, the torch.tensor() function always makes a copy."]},{"cell_type":"code","execution_count":20,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":177,"status":"ok","timestamp":1723261274408,"user":{"displayName":"Akshat Patil","userId":"09657732965291711317"},"user_tz":300},"id":"ZxfeLiNH1dB4","outputId":"c72ad382-9045-4adc-c7be-29e977cc1f29"},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor craeted using torch.from_numpy before changing np array: tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n","tensor craeted using torch.as_tensor before changing np array : tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n","tensor craeted using torch.tensor before changing np array    : tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n","\n","tensor craeted using torch.from_numpy after changing np array: tensor([   0,    1, 1000,    3,    4,    5,    6,    7,    8,    9])\n","tensor craeted using torch.as_tensor after changing np array : tensor([   0,    1, 1000,    3,    4,    5,    6,    7,    8,    9])\n","tensor craeted using torch.tensor after changing np array    : tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n"]}],"source":["my_ndarray = np.arange(10)\n","t_from_numpy = torch.from_numpy(my_ndarray)\n","t_as_tensor = torch.as_tensor(my_ndarray)\n","t_Tensor = torch.tensor(my_ndarray)\n","\n","print(f\"tensor craeted using torch.from_numpy before changing np array: {t_from_numpy}\")\n","print(f\"tensor craeted using torch.as_tensor before changing np array : {t_as_tensor}\")\n","print(f\"tensor craeted using torch.tensor before changing np array    : {t_Tensor}\")\n","\n","# change numpy array\n","my_ndarray[2] = 1000\n","\n","print()\n","print(f\"tensor craeted using torch.from_numpy after changing np array: {t_from_numpy}\")\n","print(f\"tensor craeted using torch.as_tensor after changing np array : {t_as_tensor}\")\n","print(f\"tensor craeted using torch.tensor after changing np array    : {t_Tensor}\")"]},{"cell_type":"code","execution_count":21,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":181,"status":"ok","timestamp":1723261297956,"user":{"displayName":"Akshat Patil","userId":"09657732965291711317"},"user_tz":300},"id":"g-ti5Hum-kCF","outputId":"73aec77a-1926-4f40-8d6e-078e4d6d7a64"},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([10.5000])\n","10.5\n"]}],"source":["# Initializing a size-1 tensor\n","t = torch.tensor([10.5])\n","\n","# Printing tensor\n","print(t)\n","\n","# Accessing element of tensor using item function\n","# item returns the value of the tensor as python number\n","# works only for tensors with single element\n","\n","print(t.item())"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":20,"status":"ok","timestamp":1694277195315,"user":{"displayName":"Akshat Patil","userId":"05034820051910580502"},"user_tz":300},"id":"BOV1rjjaRlck","outputId":"28236945-a1ff-4209-ebf7-e017c672d761"},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([10,  2])\n","[10, 2]\n"]}],"source":["# we can also convert the tensor to python list\n","t = torch.tensor([10, 2])\n","print(t)\n","print(t.tolist())"]},{"cell_type":"markdown","metadata":{"id":"5geGEsHIJZCb"},"source":["# <font color = 'pickle'>**Changing Shape of Tensors**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":19,"status":"ok","timestamp":1694277195316,"user":{"displayName":"Akshat Patil","userId":"05034820051910580502"},"user_tz":300},"id":"XfHgBWm1J5rg","outputId":"ad82ff4c-6761-48cd-ed02-e665743dd84d"},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n","\n","size:\n","torch.Size([10])\n"]}],"source":["t = torch.arange(10)\n","print(t)\n","print(\"\\nsize:\",t.shape, sep ='\\n')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":18,"status":"ok","timestamp":1694277195316,"user":{"displayName":"Akshat Patil","userId":"05034820051910580502"},"user_tz":300},"id":"FHsBhOyaKooN","outputId":"0c7b9652-26ec-402b-8e2b-17aa6b399915"},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[0, 1],\n","        [2, 3],\n","        [4, 5],\n","        [6, 7],\n","        [8, 9]])\n","\n","size:\n","torch.Size([5, 2])\n"]}],"source":["t = t.view(5,2)\n","print(t)\n","print(\"\\nsize:\",t.shape, sep ='\\n')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":16,"status":"ok","timestamp":1694277195316,"user":{"displayName":"Akshat Patil","userId":"05034820051910580502"},"user_tz":300},"id":"ZQG9qWI2LVU2","outputId":"e8d48a0e-9101-410a-ba76-97bcd9058884"},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[0, 1, 2, 3, 4],\n","        [5, 6, 7, 8, 9]])\n","\n","size:\n","torch.Size([2, 5])\n"]}],"source":["t = t.view(-1,5) # the size -1 is inferred from other dimensions\n","print(t)\n","print(\"\\nsize:\",t.shape, sep ='\\n')"]},{"cell_type":"markdown","metadata":{"id":"pmZk-J0TzwE0"},"source":["# <font color = 'pickle'>**Changing datatype of Tensors**\n","When creating tensor we can pass the dtype as an argument. We can also change the datatype of tensors using to() and type() mehods. For a list of dtypes visit https://pytorch.org/docs/stable/tensor_attributes.html#torch.torch.dtype"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":14,"status":"ok","timestamp":1694277195316,"user":{"displayName":"Akshat Patil","userId":"05034820051910580502"},"user_tz":300},"id":"PoYt1PzhzuQw","outputId":"ba35d403-303b-4a93-e4c7-05c4941bb85b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Old: torch.int32\n","New: torch.int64\n","Newer: torch.int32\n"]}],"source":["x = torch.tensor([8, 9, -3], dtype=torch.int)\n","\n","# we can use type() method or to() method to change the datatype\n","print(f\"Old: {x.dtype}\")\n","\n","# change the datatype to int64 using type() method\n","x = x.type(dtype=torch.int64)\n","print(f\"New: {x.dtype}\")\n","\n","# change the datatype to int32 using t0() method\n","x = x.to(dtype=torch.int32)\n","print(f\"Newer: {x.dtype}\")"]},{"cell_type":"markdown","metadata":{"id":"l1ugpAlJ6Lks"},"source":["# <font color = 'pickle'>**Saving Memory - inplace operations**"]},{"cell_type":"markdown","metadata":{"id":"Yhk6tvq_6THn"},"source":["In-place operation are operations that change the content of a given Tensor without making a copy.\n","\n","Operations that have a `_` suffix are in-place. For example: `.add_()`. Operations like += or *= are also inplace operations.\n","\n","We can also perform in-place opaeration usng the notation `Z[:] = <expression>`.\n","\n","As in-place operations do not make a copy, they can save memory. However, we need to use them carefully. They can be problematic when computing derivatives because of an immediate loss of history. We will learn about derivatives and computation graphs in coming lectures."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13,"status":"ok","timestamp":1694277195316,"user":{"displayName":"Akshat Patil","userId":"05034820051910580502"},"user_tz":300},"id":"x7iGg8QlaV3g","outputId":"94cb4085-978a-4885-e50c-9a2102875456"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["138105784457088"]},"metadata":{},"execution_count":19}],"source":["a = [[], [], []]\n","id(a)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10,"status":"ok","timestamp":1694277195316,"user":{"displayName":"Akshat Patil","userId":"05034820051910580502"},"user_tz":300},"id":"ti7JPc56aePM","outputId":"9bf4e538-103e-4d90-dad7-7874a57a9650"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["138105784520576"]},"metadata":{},"execution_count":20}],"source":["a = [[]*3]\n","id(a)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1694277195317,"user":{"displayName":"Akshat Patil","userId":"05034820051910580502"},"user_tz":300},"id":"p2ArQFtpxgPH","outputId":"93934cb5-3dfe-4878-c10a-c5e40fcf1554"},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor(10)\n","138105784540736\n","tensor(11)\n","138105784540736\n","tensor(12)\n","138105784535376\n"]}],"source":["a = torch.tensor(10)\n","print(a)\n","print(id(a))\n","a += 1\n","print(a)\n","print(id(a))\n","a = a + 1\n","print(a)\n","print(id(a))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":223,"status":"ok","timestamp":1694277195534,"user":{"displayName":"Akshat Patil","userId":"05034820051910580502"},"user_tz":300},"id":"qxbDy9WmHOxD","outputId":"c399eb49-a082-4651-a06a-5e9861e0f92d"},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor(10)\n","138105784542576\n","tensor(11)\n","138105784542576\n","tensor(12)\n","138105784542656\n"]}],"source":["b = torch.tensor(10)\n","print(b)\n","print(id(b))\n","b.add_(1)\n","print(b)\n","print(id(b))\n","b = b.add(1)\n","print(b)\n","print(id(b))"]},{"cell_type":"markdown","metadata":{"id":"X-nm1NrfbvIU"},"source":["## <font color = 'pickle'>**1) Checking gpu**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1694277195534,"user":{"displayName":"Akshat Patil","userId":"05034820051910580502"},"user_tz":300},"id":"1TFzHz0Thbjt","outputId":"92d4b660-da3e-4346-bb9f-9d1237282bd7"},"outputs":[{"output_type":"stream","name":"stdout","text":["cuda:0\n"]}],"source":["# check if gpu is availaible\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","print(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1694277195535,"user":{"displayName":"Akshat Patil","userId":"05034820051910580502"},"user_tz":300},"id":"W_drUk69u5ky","outputId":"2ae44b33-decf-4a05-8ec0-149c9942d837"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([1, 2, 3, 4])"]},"metadata":{},"execution_count":24}],"source":["# create a tensor\n","X = torch.tensor([1, 2, 3, 4])\n","X"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1694277195535,"user":{"displayName":"Akshat Patil","userId":"05034820051910580502"},"user_tz":300},"id":"qiYTr-LTvAeT","outputId":"6dc009f5-69b3-4688-d775-7928cd3a5809"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["device(type='cpu')"]},"metadata":{},"execution_count":25}],"source":["# check the device attribute of the tensor\n","X.device"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lTU-niOavIgd"},"outputs":[],"source":["# move the tensor to gpu\n","X = X.to(device=0)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1694277200550,"user":{"displayName":"Akshat Patil","userId":"05034820051910580502"},"user_tz":300},"id":"I8XlGGYXbxZy","outputId":"12193fe5-9726-438f-f1b3-d3c2eb0517d3"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["device(type='cuda', index=0)"]},"metadata":{},"execution_count":27}],"source":["X.device"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TahnwIQOvWZV"},"outputs":[],"source":["# it is more efficient to create the tensor on gpu directly\n","Y = torch.tensor([1, 2, 3], device=device)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1694277200550,"user":{"displayName":"Akshat Patil","userId":"05034820051910580502"},"user_tz":300},"id":"YWQv_dEwviH7","outputId":"6f415a96-08e4-4d63-f68d-0997d1e351c1"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["device(type='cuda', index=0)"]},"metadata":{},"execution_count":29}],"source":["# check the device attribute of the tensor\n","Y.device"]},{"cell_type":"markdown","metadata":{"id":"mJyoahoFNVtP"},"source":["## <font color = 'pickle'>**2) Memory allocation of in-place operations**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":987,"status":"ok","timestamp":1694277201535,"user":{"displayName":"Akshat Patil","userId":"05034820051910580502"},"user_tz":300},"id":"pqdfZVuhw5PG","outputId":"5d9821bb-a1cb-41a5-bba1-55ef380eb242"},"outputs":[{"output_type":"stream","name":"stdout","text":["cuda:0\n","initial memory location of tensor t1 is : 138102452805264\n","initial memory location of x is : 138102452805264\n","tensor([[True, True, True,  ..., True, True, True],\n","        [True, True, True,  ..., True, True, True],\n","        [True, True, True,  ..., True, True, True],\n","        ...,\n","        [True, True, True,  ..., True, True, True],\n","        [True, True, True,  ..., True, True, True],\n","        [True, True, True,  ..., True, True, True]], device='cuda:0')\n","final memory location of tensor t1 is: 138102452805264\n","final location of x is : 138102452805264\n","0.0\n"]}],"source":["# create tensor\n","t1 = torch.randn(10000, 10000, device=\"cpu\")\n","\n","# move tensor to gpu\n","t1 = t1.to(device)\n","print(t1.device)\n","\n","# we can use id() function to get memory location of tensor\n","print(f\"initial memory location of tensor t1 is : {id(t1)}\")\n","\n","x = t1\n","print(f\"initial memory location of x is : {id(x)}\")\n","\n","# Waits for everything to finish running\n","torch.cuda.synchronize()\n","\n","# initial memory allocated\n","start_memory = torch.cuda.memory_allocated()\n","\n","# inplace operation\n","t1 += 0.1\n","t1.add_(0.1)\n","# since the operation was inplace when we update t1 it will update x as well\n","print(x == t1)\n","\n","print(f\"final memory location of tensor t1 is: {id(t1)}\")\n","print(f\"final location of x is : {id(x)}\")\n","\n","# totall memory allocated after function call\n","end_memory = torch.cuda.memory_allocated()\n","\n","# memory allocated because of function call\n","memory_allocated = end_memory - start_memory\n","print(memory_allocated / 1024**2)"]},{"cell_type":"markdown","metadata":{"id":"Ti7xX-TSLp_3"},"source":["From the above example wecan see that both x and t1 has same memory location. When we ue in-place operation on t1, it also updates x"]},{"cell_type":"markdown","metadata":{"id":"yyQSz0iiNsgw"},"source":["## <font color = 'pickle'>**3) Memory allocation of out-of-place operations**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":940,"status":"ok","timestamp":1694277202473,"user":{"displayName":"Akshat Patil","userId":"05034820051910580502"},"user_tz":300},"id":"d3NPUZxTxcmL","outputId":"1db76e08-91aa-47c0-c398-b81324e14d1b"},"outputs":[{"output_type":"stream","name":"stdout","text":["cuda:0\n","initial memory location of tensor t2 138105784537936\n","final memory location of y is : 138105784537936\n","tensor([[False, False, False,  ..., False, False, False],\n","        [False, False, False,  ..., False, False, False],\n","        [False, False, False,  ..., False, False, False],\n","        ...,\n","        [False, False, False,  ..., False, False, False],\n","        [False, False, False,  ..., False, False, False],\n","        [False, False, False,  ..., False, False, False]], device='cuda:0')\n","final memory location of tensor t2 138105784546656\n","final memory location of y is : 138105784537936\n","382.0\n"]}],"source":["# create tensor\n","t2 = torch.randn(10000, 10000, device=\"cpu\")\n","\n","# move tensor to gpu\n","t2 = t2.to(device)\n","print(t2.device)\n","\n","# we can use id() function to get memory location of tensor\n","print(f\"initial memory location of tensor t2 {id(t2)}\")\n","\n","y = t2\n","print(f\"final memory location of y is : {id(y)}\")\n","\n","# Waits for everything to finish running\n","torch.cuda.synchronize()\n","\n","# initial memory allocated\n","start_memory = torch.cuda.memory_allocated()\n","\n","# out-place opertaions\n","t2 = t2 + 0.1\n","\n","# since the operation was not inplace when we update t2 it will not update y\n","print(y == t2)\n","\n","# we can use id() function to get memory location of tensor\n","print(f\"final memory location of tensor t2 {id(t2)}\")\n","print(f\"final memory location of y is : {id(y)}\")\n","\n","# totall memory allocated after function call\n","end_memory = torch.cuda.memory_allocated()\n","\n","# memory allocated because of function call\n","memory_allocated = end_memory - start_memory\n","print(memory_allocated / 1024**2)"]},{"cell_type":"markdown","metadata":{"id":"9kli0fOuMGCM"},"source":["From the above example we can see that initially both y and t2 has same memory location. After running t2 = t2 + 0.1, we will find that id(t2) points to a different location. That is because Python first evaluates t2 + 0.1, allocating new memory for the result and then makes t2 point to this new location in memory. Since we have not done in-place operation, updating t2 does not effect y. y still points to the same memory location."]},{"cell_type":"markdown","metadata":{"id":"7aJjhm1lMQ3m"},"source":["# <font color = 'pickle'>**Linear Algebra**"]},{"cell_type":"markdown","metadata":{"id":"E5a8iDgbMajD"},"source":["## <font color = 'pickle'>**Dot product**\n","Dot product of 2 vectors x and y  is given by the summation of product of elements at the same position.\n","\n","If we have 2 vectors x: [1, 2, 3, 4] and y: [1, 1, 2, 1]\n","\n","(x.y) will be 1x1 + 2x1 + 3x2 + 4x1 = 13"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1694277202473,"user":{"displayName":"Akshat Patil","userId":"05034820051910580502"},"user_tz":300},"id":"KD4_9ZUxQ2Hp","outputId":"6fd33e03-b6bd-4f31-99f3-f50bd91a9309"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(0.)"]},"metadata":{},"execution_count":32}],"source":["# Initializing 2 tensors\n","x = torch.Tensor([0, -1, 1, 0])\n","y = torch.Tensor([0, 1, 1, 0])\n","\n","# Performing Dot product\n","torch.dot(x, y)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1694277202473,"user":{"displayName":"Akshat Patil","userId":"05034820051910580502"},"user_tz":300},"id":"0NRdtGrvSGCj","outputId":"aa3ec62d-98a4-4504-8c5c-dd5baef7118e"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(0.)"]},"metadata":{},"execution_count":33}],"source":["# Dot Product is equal to sum of products at the same position, thus the expression below will give similar result\n","torch.sum(x * y)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":119,"status":"ok","timestamp":1694277202590,"user":{"displayName":"Akshat Patil","userId":"05034820051910580502"},"user_tz":300},"id":"Cp5-XFyNZKSt","outputId":"ca1cc579-cbd1-408e-a5b7-5ec6d34cde03"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(2.)"]},"metadata":{},"execution_count":34}],"source":["# Initializing 2 tensors\n","x = torch.Tensor([1, 0, 0, 1])\n","y = torch.Tensor([1, 0, 0, 1])\n","\n","# Performing Dot product\n","torch.dot(x, y)"]},{"cell_type":"markdown","metadata":{"id":"BS-6BCNSOPLx"},"source":["## <font color = 'pickle'>**Dot product vs. for Loop in Python**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"olJVVgxAOx8j"},"outputs":[],"source":["import time\n","n = 1000000\n","a = torch.arange(n)\n","b = torch.arange(n)\n","\n","def pytorch_dot(x, y):\n","    return x.dot(y)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6990,"status":"ok","timestamp":1694277209577,"user":{"displayName":"Akshat Patil","userId":"05034820051910580502"},"user_tz":300},"id":"-HGjTW_FSE5l","outputId":"30be772b-0453-4d6d-c80b-af055e3fc7e3"},"outputs":[{"output_type":"stream","name":"stdout","text":["872 µs ± 197 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"]}],"source":["%timeit pytorch_dot(a,b)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1694277209578,"user":{"displayName":"Akshat Patil","userId":"05034820051910580502"},"user_tz":300},"id":"PHTeeIIddLkz","outputId":"199046d4-c3bd-4bf5-8279-6973891111e9"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["[(1, 3), (2, 4)]"]},"metadata":{},"execution_count":37}],"source":["x = [1,2]\n","y = [3,4]\n","list(zip(x,y))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0K2kSXHeSYbU"},"outputs":[],"source":["a1 = a.tolist()\n","b1 = b.tolist()\n","def plain_python(x, y):\n","    output = 0\n","    for x_j, y_j in zip(x, y):\n","        output += x_j * y_j\n","    return output\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Lo8KxBAQS5kX","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1694277265457,"user_tz":300,"elapsed":55697,"user":{"displayName":"Akshat Patil","userId":"05034820051910580502"}},"outputId":"1d40e749-f96f-4d18-81f1-d09d1f0dfbc9"},"outputs":[{"output_type":"stream","name":"stdout","text":["6.95 s ± 399 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"]}],"source":["%timeit plain_python(a,b)"]},{"cell_type":"markdown","metadata":{"id":"Rk8Kt4uJSBOp"},"source":["**Output 1:**\n","\n","921 µs ± 182 µs per loop: This tells you that the code took an average of 921 microseconds (µs) to run for each loop, with a standard deviation of 182 microseconds. The standard deviation gives an indication of the variability in the timing across different runs, which can be affected by other processes running on the computer at the same time.\n","\n","(mean ± std. dev. of 7 runs, 1000 loops each): This part provides details about how the timing was measured. The code was run 7 times, and each of those runs consisted of 1000 loops. The mean and standard deviation were calculated from these 7 runs.\n","\n","**Output 2:**\n","6.78 s ± 392 ms per loop: This tells you that the code took an average of 6.78 seconds to run for each loop, with a standard deviation of 392 milliseconds. Since 1 second equals 1000 milliseconds, this standard deviation is less than half a second.\n","\n","(mean ± std. dev. of 7 runs, 1 loop each): Similar to Output 1, this part tells you that the code was run 7 times, and each of those runs consisted of just 1 loop. The mean and standard deviation were calculated from these 7 runs.\n","\n","**In comparison, Output 1 suggests a much faster execution time (in the order of microseconds) compared to Output 2 (in the order of seconds).**\n","\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"bUzXoOeG_rfX"},"source":["## <font color = 'pickle'>**Operations on Metrices**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":22,"status":"ok","timestamp":1694277265465,"user":{"displayName":"Akshat Patil","userId":"05034820051910580502"},"user_tz":300},"id":"46A32IAZV0ky","outputId":"41fe895a-78fa-47e4-abe7-e0407fb01ef4"},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[ 0,  1,  2,  3,  4],\n","        [ 5,  6,  7,  8,  9],\n","        [10, 11, 12, 13, 14],\n","        [15, 16, 17, 18, 19],\n","        [20, 21, 22, 23, 24]])\n","tensor([[ 0,  1,  2,  3,  4],\n","        [ 5,  6,  7,  8,  9],\n","        [10, 11, 12, 13, 14],\n","        [15, 16, 17, 18, 19],\n","        [20, 21, 22, 23, 24]])\n"]}],"source":["# Creating 2 matrices\n","\n","# First matrix\n","A = torch.arange(0, 25).reshape(5, 5)\n","\n","# Second matrix : copy of A\n","B = A.clone()\n","\n","print(A)\n","print(B)"]},{"cell_type":"markdown","metadata":{"id":"D7ENj3dtV9yR"},"source":["### <font color = 'pickle'>**Addition of 2 matrices**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":21,"status":"ok","timestamp":1694277265466,"user":{"displayName":"Akshat Patil","userId":"05034820051910580502"},"user_tz":300},"id":"0VFFcFWq5ZJZ","outputId":"a56f8dfc-8c36-4e4b-840e-e57e33403604"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[ 0,  2,  4,  6,  8],\n","        [10, 12, 14, 16, 18],\n","        [20, 22, 24, 26, 28],\n","        [30, 32, 34, 36, 38],\n","        [40, 42, 44, 46, 48]])"]},"metadata":{},"execution_count":41}],"source":["# Addition of 2 matrices\n","A + B"]},{"cell_type":"markdown","metadata":{"id":"nbOc6KIXWD_N"},"source":["### <font color = 'pickle'>**Subtraction of 2 matrices**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":18,"status":"ok","timestamp":1694277265466,"user":{"displayName":"Akshat Patil","userId":"05034820051910580502"},"user_tz":300},"id":"oycCtJiY5jPM","outputId":"f5d428d8-37f7-4625-8a13-98344ff16795"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[0, 0, 0, 0, 0],\n","        [0, 0, 0, 0, 0],\n","        [0, 0, 0, 0, 0],\n","        [0, 0, 0, 0, 0],\n","        [0, 0, 0, 0, 0]])"]},"metadata":{},"execution_count":42}],"source":["# Subtraction of 2 matrices\n","A - B"]},{"cell_type":"markdown","metadata":{"id":"dogJlUvrWYhx"},"source":["### <font color = 'pickle'>**Multiplying Matrices with Scalars**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":142,"status":"ok","timestamp":1694277265593,"user":{"displayName":"Akshat Patil","userId":"05034820051910580502"},"user_tz":300},"id":"ISBiKAus-AKu","outputId":"05b39ef3-59b9-4100-8aac-b4295e419447"},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[ 2,  3,  4,  5,  6],\n","        [ 7,  8,  9, 10, 11],\n","        [12, 13, 14, 15, 16],\n","        [17, 18, 19, 20, 21],\n","        [22, 23, 24, 25, 26]])\n","\n","tensor([[ 0,  2,  4,  6,  8],\n","        [10, 12, 14, 16, 18],\n","        [20, 22, 24, 26, 28],\n","        [30, 32, 34, 36, 38],\n","        [40, 42, 44, 46, 48]])\n"]}],"source":["# Each element of matrix can be aded or multiplied by a scalar (broadcasting)\n","# This operation will not change the shape of a matrix or a Tensor\n","a = 2\n","print(a + A)\n","print()\n","print(a * A)"]},{"cell_type":"markdown","metadata":{"id":"l_iPeyTzWlXA"},"source":["### <font color = 'pickle'>**Transpose of a Matrix**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1694277265593,"user":{"displayName":"Akshat Patil","userId":"05034820051910580502"},"user_tz":300},"id":"sHAObZ7g5nhj","outputId":"76e8be0b-8100-44f2-b767-c5c57fd93ede"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[ 0,  5, 10, 15, 20],\n","        [ 1,  6, 11, 16, 21],\n","        [ 2,  7, 12, 17, 22],\n","        [ 3,  8, 13, 18, 23],\n","        [ 4,  9, 14, 19, 24]])"]},"metadata":{},"execution_count":44}],"source":["# Transpose of a matrix : Elements of the rows and columns get interchanged a[i][j] becomes a[j][i]\n","# Transpose is a special case of permute\n","A.T"]},{"cell_type":"markdown","metadata":{"id":"tsAkl0FDWQ-T"},"source":["### <font color = 'pickle'>**Hadamard product**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1694277265593,"user":{"displayName":"Akshat Patil","userId":"05034820051910580502"},"user_tz":300},"id":"7aua8kx79WOV","outputId":"140c176e-cd87-4dff-dd42-67ccb63fc699"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[  0,   1,   4,   9,  16],\n","        [ 25,  36,  49,  64,  81],\n","        [100, 121, 144, 169, 196],\n","        [225, 256, 289, 324, 361],\n","        [400, 441, 484, 529, 576]])"]},"metadata":{},"execution_count":45}],"source":["# Elementwise multiplication of two metrices is called Hadamard product\n","A * B"]},{"cell_type":"markdown","metadata":{"id":"Ys0KKwAFTHpk"},"source":["### <font color = 'pickle'>**Matrix Multiplication**\n","\n","Matrix multiplication is a binary operation on 2 matrices which gives us a matrix which is the product of the 2 matrices.\n","\n","If we are given 2 matrices $A$ of shape $(m * n)$ and $B$ of shape $(q * p)$, **we can perform matrix multiplication only when $n = q$** and the resultant product matrix will have shape $(m * p)$.\n","\n","Suppose we are given 2 matrices $A (m * n)$ and $B (n * p)$:\n","\n","$$\\mathbf{A}=\\begin{bmatrix}\n"," a_{11} & a_{12} & \\cdots & a_{1n} \\\\\n"," a_{21} & a_{22} & \\cdots & a_{2n} \\\\\n","\\vdots & \\vdots & \\ddots & \\vdots \\\\\n"," a_{m1} & a_{m2} & \\cdots & a_{mn} \\\\\n","\\end{bmatrix},\\quad\n","\\mathbf{B}=\\begin{bmatrix}\n"," b_{11} & b_{12} & \\cdots & b_{1p} \\\\\n"," b_{21} & b_{22} & \\cdots & b_{2p} \\\\\n","\\vdots & \\vdots & \\ddots & \\vdots \\\\\n"," b_{n1} & b_{n2} & \\cdots & b_{np} \\\\\n","\\end{bmatrix}$$\n","\n","Then after performing matrix multiplication, the resultant matrix C = AB will be:\n","\n","$$\\mathbf{C}=\\begin{bmatrix}\n"," c_{11} & c_{12} & \\cdots & c_{1p} \\\\\n"," c_{21} & c_{22} & \\cdots & c_{2p} \\\\\n","\\vdots & \\vdots & \\ddots & \\vdots \\\\\n"," c_{mp} & c_{mp} & \\cdots & c_{mp} \\\\\n","\\end{bmatrix}$$\n","\n","Here, $c_{ij} = a_{i1}b_{1j} + a_{i2}b_{2j} + ... a_{in}b_{b_nj} = \\sum_{k = 1}^n a_{ik}b_{kj}$\n","\n","for, $i = 1,....m$ and $j = 1,...p$\n","\n","\n","Thus, each element of C, $c_{ij}$ is obtained by dot product of $i^{th}$ row of $A$ and $j^{th}$ column of $B$.\n","\n","**Example** :\n","  1. Let $A$ be a matrix of (4, 3) dimensions.\n","  2. Let $B$ be another matrix of (3, 2) dimensions.\n","  3. Let us denote denote the matrix multiplication of $A$ and $B$ with $C$.\n","  5. Then the dimension of $C$ = (number of rows of $A$,number of columns of $B$)\n","     \n","    dimension of  $C$ = (4, 2)\n","\n","The figure given below will give a good example of matrix multplication :\n","\n","<img src = \"https://drive.google.com/uc?view=export&id=176DF50XdtwkqU5wvxtWuD75sRDHvSJBf\" width =\"250\"/>\n","\n","We can perform matrix multiplication in the following way using PyTorch:"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1694277265593,"user":{"displayName":"Akshat Patil","userId":"05034820051910580502"},"user_tz":300},"id":"KAGYwiI4SykF","outputId":"c2075d5b-d8bf-40d2-c590-904cff365891"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[10., 10.],\n","        [35., 35.]], dtype=torch.float64)"]},"metadata":{},"execution_count":46}],"source":["# Initializing 2 matrices\n","A = torch.arange(0, 10, dtype=float).reshape(2, 5)\n","B = torch.ones(5, 2, dtype=float)\n","\n","# Matrix-Matrix Multiplication using mm function of PyTorch\n","torch.mm(A, B)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1694277265594,"user":{"displayName":"Akshat Patil","userId":"05034820051910580502"},"user_tz":300},"id":"2BaJkVZc1gGE","outputId":"547790e5-2f78-42ce-cae3-c22e926f571d"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[10., 10.],\n","        [35., 35.]], dtype=torch.float64)"]},"metadata":{},"execution_count":47}],"source":["A @ B"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":175},"executionInfo":{"elapsed":111,"status":"error","timestamp":1694277265701,"user":{"displayName":"Akshat Patil","userId":"05034820051910580502"},"user_tz":300},"id":"Bi0LdqQL1iwQ","outputId":"69946d0d-14f8-481a-c783-606b113953be"},"outputs":[{"output_type":"error","ename":"RuntimeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-48-a4cedde81ed0>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mA\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mB\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (5) must match the size of tensor b (2) at non-singleton dimension 1"]}],"source":["A * B"]},{"cell_type":"markdown","metadata":{"id":"cI4L4lCPbCar"},"source":["#### <font color = 'pickle'>**Prediction on Multiple Training Examples via Matrix Multiplication**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":93,"status":"ok","timestamp":1694277272306,"user":{"displayName":"Akshat Patil","userId":"05034820051910580502"},"user_tz":300},"id":"_1-orC5SbYgs","outputId":"edc2df2f-2063-4e66-fdd2-d7722fc23476"},"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([5, 2]) torch.Size([2, 1]) torch.Size([1])\n","tensor([0.])\n","tensor([[0.2000],\n","        [9.3000]])\n","tensor([[1.8000, 9.2000],\n","        [0.2000, 3.3000],\n","        [5.2000, 3.4000],\n","        [3.4000, 4.5000],\n","        [6.1000, 7.1000]])\n"]}],"source":["bias = torch.tensor([0.])\n","theta = torch.tensor([0.2, 9.3])\n","theta = theta.view(-1,1)\n","X = torch.tensor(\n","   [[1.8, 9.2],\n","    [0.2, 3.3],\n","    [5.2, 3.4],\n","    [3.4, 4.5],\n","    [6.1, 7.1]]\n",")\n","print(X.shape, theta.shape, bias.shape)\n","print( bias,theta, X, sep='\\n')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":96,"status":"ok","timestamp":1694277274205,"user":{"displayName":"Akshat Patil","userId":"05034820051910580502"},"user_tz":300},"id":"KLTjgJt9cfSu","outputId":"3892b887-b958-4c8b-dffc-31d3b200e9c0"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[85.9200],\n","        [30.7300],\n","        [32.6600],\n","        [42.5300],\n","        [67.2500]])"]},"metadata":{},"execution_count":50}],"source":["predictions = X.matmul(theta) + bias\n","predictions"]},{"cell_type":"markdown","metadata":{"id":"EUh62jYWUZvj"},"source":["# <font color = 'pickle'>**Self Study**"]},{"cell_type":"markdown","metadata":{"id":"khxj8DIV5Eic"},"source":["## <font color = 'pickle'>**Broadcasting - Operations on tensors of different  size**\n","\n","Broadcasting describes how a tensor has to be treated during arithematic operation. If we have tensors of different sizes, we can broadcast the smaller array across the larger one so that they can have comaptible shapes.\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"u9J4-PQRPRtk"},"source":["### <font color = 'pickle'>**Broadcasting Examples**</font>"]},{"cell_type":"markdown","metadata":{"id":"1ZxySDAIPtMW"},"source":["#### <font color = 'pickle'>**Broadcasting with a scalar**</font>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HIuRF2yiPcAn"},"outputs":[],"source":["t= torch.tensor([1,-2, 4])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":107,"status":"ok","timestamp":1694277277457,"user":{"displayName":"Akshat Patil","userId":"05034820051910580502"},"user_tz":300},"id":"AjfLaYt5PiHz","outputId":"13f4787a-eb8a-4c15-96c0-07714d75ad8d"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([ True, False,  True])"]},"metadata":{},"execution_count":52}],"source":["t > 0"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":109,"status":"ok","timestamp":1694277278474,"user":{"displayName":"Akshat Patil","userId":"05034820051910580502"},"user_tz":300},"id":"nUqLAPSoPjmC","outputId":"370946b9-9a68-4b9a-8373-9c39264ae9cf"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[24, 32, 28],\n","        [26, 34, 26],\n","        [28, 36, 24]])"]},"metadata":{},"execution_count":53}],"source":["t2 = torch.tensor([[12, 16, 14], [13, 17, 13], [14, 18, 12]])\n","t2 * 2"]},{"cell_type":"markdown","metadata":{"id":"TCNMwObZQEDR"},"source":["#### <font color = 'pickle'>**Broadcasting a vector to matrix**</font>"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1694277280060,"user":{"displayName":"Akshat Patil","userId":"05034820051910580502"},"user_tz":300},"id":"XowS3mFpQUdc","outputId":"4f79a3ef-f803-439a-f396-dd32b21faf3a"},"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([3, 3])\n","torch.Size([3])\n","tensor([[13, 18, 17],\n","        [14, 19, 16],\n","        [15, 20, 15]])\n"]}],"source":["t2 = torch.tensor([[12, 16, 14], [13, 17, 13], [14, 18, 12]])\n","t1 = torch.tensor([1, 2, 3])\n","print(t2.shape)\n","print(t1.shape)\n","print(t1 + t2)"]},{"cell_type":"markdown","metadata":{"id":"H9Vet8dAPu5O"},"source":["### <font color = 'pickle'>**1) Understanding how broadcasting works**\n","\n","* The following image describes how a tensor of 2 dimensional tensor will be added to a 1 dimensional tensor\n","<img src=\"https://drive.google.com/uc?export=view&id=1QG2GO1owGpyXbcugJFVFGb4o_buV4s3j\" width=\"500\"/>"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":99,"status":"ok","timestamp":1694277282453,"user":{"displayName":"Akshat Patil","userId":"05034820051910580502"},"user_tz":300},"id":"bR4BtZeAPu5Q","outputId":"50c530b2-9925-4949-d5a2-f80cad3383ae"},"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([3, 3])\n","torch.Size([3])\n"]}],"source":["# create tensor\n","t2 = torch.tensor([[12, 16, 14], [13, 17, 13], [14, 18, 12]])\n","t1 = torch.tensor([1, 2, 3])\n","print(t2.shape)\n","print(t1.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":149,"status":"ok","timestamp":1694277284119,"user":{"displayName":"Akshat Patil","userId":"05034820051910580502"},"user_tz":300},"id":"QFyWgq3Ae1QV","outputId":"256bd0a3-6fc5-4798-8bad-935f786ed3dd"},"outputs":[{"output_type":"stream","name":"stdout","text":[" 1\n"," 2\n"," 3\n","[torch.storage.TypedStorage(dtype=torch.int64, device=cpu) of size 3]\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-56-623047f164ba>:1: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n","  print(t1.storage())\n"]}],"source":["print(t1.storage())"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":108,"status":"ok","timestamp":1694277285408,"user":{"displayName":"Akshat Patil","userId":"05034820051910580502"},"user_tz":300},"id":"aC7ihRDUPu5S","outputId":"9e710e83-6bb7-4c11-c191-38ef9f4ebcb2"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([3, 3])"]},"metadata":{},"execution_count":57}],"source":["t1_mod = t1.expand_as(t2)\n","t1_mod.shape"]},{"cell_type":"markdown","metadata":{"id":"PchW_tUmMdIw"},"source":["Although it appears as though we are copying the rows, we are actually not duplicating them."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":121,"status":"ok","timestamp":1694277287105,"user":{"displayName":"Akshat Patil","userId":"05034820051910580502"},"user_tz":300},"id":"WvOz2svpMoo2","outputId":"b8a4b5b3-8442-45bc-90b4-aad7eb85ea2f"},"outputs":[{"output_type":"stream","name":"stdout","text":[" 1\n"," 2\n"," 3\n","[torch.storage.TypedStorage(dtype=torch.int64, device=cpu) of size 3]\n"]}],"source":["print(t1_mod.storage())"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":100,"status":"ok","timestamp":1694277288658,"user":{"displayName":"Akshat Patil","userId":"05034820051910580502"},"user_tz":300},"id":"se8ZvnApPu5U","outputId":"5b5e9d81-235c-4bb1-da9f-09888fd6e10b"},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[13, 18, 17],\n","        [14, 19, 16],\n","        [15, 20, 15]])\n"]}],"source":["print(t1_mod + t2)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":95,"status":"ok","timestamp":1694277290195,"user":{"displayName":"Akshat Patil","userId":"05034820051910580502"},"user_tz":300},"id":"D-H5uc89Pu5V","outputId":"a0f681af-95df-4f32-c5ce-78b7506472f6"},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[13, 18, 17],\n","        [14, 19, 16],\n","        [15, 20, 15]])\n"]}],"source":["# we can check that it gives us the same result if we simply add t1 and t2\n","# so broadcasting is an efficient way of performing operations on tensors of unequal sizes\n","print(t1 + t2)"]},{"cell_type":"markdown","metadata":{"id":"zyhE_dEUQwLk"},"source":["### <font color = 'pickle'>**2) Rules for Broadcasting**</font>\n","Broadcasting can only happen if the two tensors are broadcastable. Conditions for broadcasting:\n","\n","- Each tensor has at least one dimension.\n","\n","- When iterating over the dimension sizes, starting at the trailing dimension, the dimension sizes must either be equal, one of them is 1, or one of them does not exist.\n","\n","Examples:\n","\n","1. `t1(5, 8, 10) t2(5, 8, 10)`\n","Same size -> Broadcasting possible.\n","2. `t1((0,)) t2(5, 8, 10)`\n","t1 doesn't have atleast one dimension -> Broadcasting not possible.\n","3. `t1(5, 8, 10, 1) t2(8, 1, 1)` Broadcasting possible. Reasons:\n","  - 1st trailing position : both have size 1\n","  - 2nd trailing position : t2 has size 1\n","  - 3rd trailing position : both have size 8\n","  - 4th training position: t2 size doesn't exist but t2 has atleast 1 dimension."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":107,"status":"ok","timestamp":1694277292317,"user":{"displayName":"Akshat Patil","userId":"05034820051910580502"},"user_tz":300},"id":"x1BuDjiW4MNx","outputId":"bfe73a9f-c6ab-469d-9ae9-cb33ab69cb14"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([5, 8, 10, 1])"]},"metadata":{},"execution_count":61}],"source":["# Broadcasting\n","t1 = torch.empty(5, 8, 10, 1)\n","t2 = torch.empty(  8, 1, 1,)\n","(t1 + t2).size()"]},{"cell_type":"markdown","metadata":{"id":"kjZwjy9d8wbD"},"source":["The dimensions after broadcasting will be:\n","\n","- If the number of dimensions are\n"," not equal, prepend 1 to the dimensions of the tensor with fewer dimensions to make them equal length.\n","\n","- Then, for each dimension size, the resulting dimension size is the max of the sizes along that dimension."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":104,"status":"ok","timestamp":1694277295333,"user":{"displayName":"Akshat Patil","userId":"05034820051910580502"},"user_tz":300},"id":"AqqPPTU78v83","outputId":"252b09dc-623d-44a9-8040-5ef84e858596"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([3, 1, 7])"]},"metadata":{},"execution_count":62}],"source":["# Another example for broadcasting\n","t1 = torch.empty(1)\n","t2 = torch.empty(3, 1, 7)\n","(t1 + t2).size()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":210},"executionInfo":{"elapsed":110,"status":"error","timestamp":1694277296524,"user":{"displayName":"Akshat Patil","userId":"05034820051910580502"},"user_tz":300},"id":"02HFhB1S9QfX","outputId":"6759c235-9ae1-4c01-bce1-9f99b6f98c33"},"outputs":[{"output_type":"error","ename":"RuntimeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-63-869bb92903e6>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mt1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mt2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;34m(\u001b[0m\u001b[0mt1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mt2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (8) must match the size of tensor b (3) at non-singleton dimension 1"]}],"source":["# Example where broadcasting is not possible\n","t1 = torch.empty(5, 8, 10, 1)\n","t2 = torch.empty( 3, 1, 1)\n","(t1 + t2).size()"]},{"cell_type":"markdown","metadata":{"id":"wru9LKgg9f5G"},"source":["Here, at third trailing position sizes are not equal and none of them is 1, thus broadcasting is not possible."]},{"cell_type":"markdown","metadata":{"id":"UeOLV3qj7DTS"},"source":["## <font color = 'pickle'>**Reduction**\n","\n","We can calculate the sum of all elemnets of a vector or a matrix of any shape. This can be done using the ***sum*** function.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":103,"status":"ok","timestamp":1694277302647,"user":{"displayName":"Akshat Patil","userId":"05034820051910580502"},"user_tz":300},"id":"RleGFr-96dnx","outputId":"4ee18198-d583-45bd-bc19-ef704fa2996e"},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([0, 1, 2, 3, 4])\n","tensor(10)\n"]}],"source":["# Creating a vector\n","x = torch.arange(5)\n","print(x)\n","\n","# This will do summation of all the elements of the vector : 0 + 1 + 2 + 3 + 4 = 10\n","print(x.sum())"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":106,"status":"ok","timestamp":1694277304714,"user":{"displayName":"Akshat Patil","userId":"05034820051910580502"},"user_tz":300},"id":"YC7ffhAU9bnO","outputId":"18779b61-6cc2-4894-9a16-6edb74efef5e"},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[0., 1., 2., 3., 4.],\n","        [5., 6., 7., 8., 9.]])\n","tensor(45.)\n","tensor(4.5000)\n"]}],"source":["# Creating a matrix\n","X = torch.arange(0, 10).reshape(2, 5)\n","X = X.to(torch.float32)\n","print(X)\n","\n","# This will do summation of all the elements of the matrix\n","print(X.sum())\n","# This will takle the mean of all teh elements\n","print(X.mean())"]},{"cell_type":"markdown","metadata":{"id":"IRQX5pa6-r6V"},"source":["We can also calculate the mean or average of all elements in a vector or a matrix by dividing the sum of elements by no. of elements."]},{"cell_type":"markdown","metadata":{"id":"k6Ti1nNrBP4f"},"source":["By default, invoking the sum/mean finction on a tensor will give us a scaler (reduces the tensor along all its axes)\n","\n","We can also calculate sum, along the rows or columns by specifying the value of parameter \"axis\".\n","\n","axis = 0 will calculate sum along the rows while axis = 1 will calculate sum along the columns.\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":98,"status":"ok","timestamp":1694277306813,"user":{"displayName":"Akshat Patil","userId":"05034820051910580502"},"user_tz":300},"id":"ifrBSeGm_P6Z","outputId":"d188036d-8ac0-4225-e285-7bddaf3ecf98"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[ 0.,  1.,  2.],\n","        [ 3.,  4.,  5.],\n","        [ 6.,  7.,  8.],\n","        [ 9., 10., 11.],\n","        [12., 13., 14.]], dtype=torch.float64)"]},"metadata":{},"execution_count":66}],"source":["# Creating a matrix A\n","A = torch.arange(0, 15, dtype = float).reshape(5, 3)\n","A"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":98,"status":"ok","timestamp":1694277308421,"user":{"displayName":"Akshat Patil","userId":"05034820051910580502"},"user_tz":300},"id":"MSk9Q9uyQHL7","outputId":"9820ce2a-5b93-4f36-f265-630cc981ffbe"},"outputs":[{"output_type":"stream","name":"stdout","text":["Shape before redictiontorch.Size([5, 3])\n"]},{"output_type":"execute_result","data":{"text/plain":["(tensor([30., 35., 40.], dtype=torch.float64), torch.Size([3]))"]},"metadata":{},"execution_count":67}],"source":["# Sum of elements along axis = 0\n","# row sum for each column\n","# Since we are taking sum along axis = 0, the input tensor reduces along axis 0\n","# The shape of A was ([5,3])\n","# After invoking sum along axis = 0, the shape reduces to ([3])\n","print(f'Shape before rediction{A.shape}')\n","A.sum(axis = 0), A.sum(axis=0).shape"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":93,"status":"ok","timestamp":1694277310049,"user":{"displayName":"Akshat Patil","userId":"05034820051910580502"},"user_tz":300},"id":"YRsJKUOcQG7a","outputId":"0e7b87a4-4a57-4ea0-d5fb-7437ab21c859"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(tensor([ 3., 12., 21., 30., 39.], dtype=torch.float64), torch.Size([5]))"]},"metadata":{},"execution_count":68}],"source":["# Sum of elements along axis =  1\n","# column sum for each row\n","# Since we are taking sum along axis = 1, the input tensor reduces along axis 1\n","# The shape of A was ([5,3])\n","# After invoking sum along axis = 1, the shape redices to ([5])\n","A.sum(axis = 1), A.sum(axis = 1).shape"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":210},"executionInfo":{"elapsed":113,"status":"error","timestamp":1694277311192,"user":{"displayName":"Akshat Patil","userId":"05034820051910580502"},"user_tz":300},"id":"421spORkPFph","outputId":"0828becf-a97d-4063-ba2c-5d4b9fb37805"},"outputs":[{"output_type":"error","ename":"RuntimeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-69-d93b8912f840>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Rules of broadcasting:  A and A.sum(axis=1) are not broadcastable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (3) must match the size of tensor b (5) at non-singleton dimension 1"]}],"source":["# Rules of broadcasting:  A and A.sum(axis=1) are not broadcastable\n","print(A/A.sum(axis=1))\n","\n"]},{"cell_type":"markdown","metadata":{"id":"Pt8xjJ8w8sba"},"source":["## <font color = 'pickle'>**Non-Reduction Sum**"]},{"cell_type":"markdown","metadata":{"id":"YEp87iCYDX3h"},"source":["As seem in above examples, invoking sum() or mean() will reduce number of dimensions. We can keep number of axis unchanged by passing argument keepdims = True."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":105,"status":"ok","timestamp":1694277313256,"user":{"displayName":"Akshat Patil","userId":"05034820051910580502"},"user_tz":300},"id":"KGZhGYCLIM0S","outputId":"7faa76dc-d4f2-4ad0-f094-68e97cf28560"},"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([5, 1])\n","tensor([[ 3.],\n","        [12.],\n","        [21.],\n","        [30.],\n","        [39.]], dtype=torch.float64)\n"]}],"source":["# When we pass argument keepdim=True, the shape will now be ([5,1]. The output has 2-dimensions\n","# if we do not pass the argument keepdim=True, the shape will be ([5]). The output has one-dimension\n","sum_A_0 = A.sum(axis=1, keepdim=True)\n","print(sum_A_0.shape)\n","print(sum_A_0)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":101,"status":"ok","timestamp":1694277315584,"user":{"displayName":"Akshat Patil","userId":"05034820051910580502"},"user_tz":300},"id":"zQQoa2RlJn-J","outputId":"399790ee-bb36-42e3-94fc-33c7d1cd3bd9"},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[0.0000, 0.3333, 0.6667],\n","        [0.2500, 0.3333, 0.4167],\n","        [0.2857, 0.3333, 0.3810],\n","        [0.3000, 0.3333, 0.3667],\n","        [0.3077, 0.3333, 0.3590]], dtype=torch.float64)\n"]}],"source":["# Let us now try operation : A/(sum(A, axis=0))\n","print(A/A.sum(axis=1, keepdim=True))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":92,"status":"ok","timestamp":1694277317206,"user":{"displayName":"Akshat Patil","userId":"05034820051910580502"},"user_tz":300},"id":"2LexuntQ6KMY","outputId":"bba43a19-f1df-4b3f-991d-8c5a6f1f3c44"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[ 0.,  1.,  2.],\n","        [ 3.,  4.,  5.],\n","        [ 6.,  7.,  8.],\n","        [ 9., 10., 11.],\n","        [12., 13., 14.]], dtype=torch.float64)"]},"metadata":{},"execution_count":72}],"source":["A"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"01wpGjMFbj3e"},"outputs":[],"source":["A = torch.randint(0, 10 , (2,3,4))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":95,"status":"ok","timestamp":1694277322377,"user":{"displayName":"Akshat Patil","userId":"05034820051910580502"},"user_tz":300},"id":"FYWuI5K4b0DB","outputId":"47bd6e9d-e929-4ffc-a0d4-cc3f98c412c5"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[[0, 7, 9, 0],\n","         [0, 0, 1, 6],\n","         [5, 3, 6, 2]],\n","\n","        [[2, 0, 6, 3],\n","         [6, 7, 0, 2],\n","         [6, 2, 9, 1]]])"]},"metadata":{},"execution_count":76}],"source":["  A"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":107,"status":"ok","timestamp":1694277323554,"user":{"displayName":"Akshat Patil","userId":"05034820051910580502"},"user_tz":300},"id":"VCKO6sBgb3Kd","outputId":"8083c60f-9e73-4b21-8bed-015a12df656e"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([2, 3, 4])"]},"metadata":{},"execution_count":77}],"source":["A.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7w5hzOz3b5t8"},"outputs":[],"source":["B = A.sum(axis = 1, keepdim=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2,"status":"ok","timestamp":1694277325392,"user":{"displayName":"Akshat Patil","userId":"05034820051910580502"},"user_tz":300},"id":"cw9CKv3ub_4r","outputId":"7fbfc98c-a137-4d8a-d1cc-4f16753a3ae2"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([2, 1, 4])"]},"metadata":{},"execution_count":79}],"source":["B.shape"]},{"cell_type":"code","source":["B"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Uec3x34iIH0A","executionInfo":{"status":"ok","timestamp":1694277327694,"user_tz":300,"elapsed":166,"user":{"displayName":"Akshat Patil","userId":"05034820051910580502"}},"outputId":"7d42c2ab-28b6-4419-cc86-e92a0ba44709"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[[ 5, 10, 16,  8]],\n","\n","        [[14,  9, 15,  6]]])"]},"metadata":{},"execution_count":80}]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":100,"status":"ok","timestamp":1694277329775,"user":{"displayName":"Akshat Patil","userId":"05034820051910580502"},"user_tz":300},"id":"pyENwjkxDSkP","outputId":"dea2486c-8496-4756-f57d-16bd61446af1"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[[ 0,  7,  9,  0],\n","         [ 0,  0,  1,  6],\n","         [ 5,  3,  6,  2]],\n","\n","        [[ 2,  7, 15,  3],\n","         [ 6,  7,  1,  8],\n","         [11,  5, 15,  3]]])"]},"metadata":{},"execution_count":81}],"source":["# Cumulative sum of elements along rows\n","A.cumsum(axis = 0)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":99,"status":"ok","timestamp":1694277331434,"user":{"displayName":"Akshat Patil","userId":"05034820051910580502"},"user_tz":300},"id":"8OYWH9PMAt4g","outputId":"77583228-176b-4501-a7bc-1e6ffdfce2b7"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[[ 0,  7,  9,  0],\n","         [ 0,  7, 10,  6],\n","         [ 5, 10, 16,  8]],\n","\n","        [[ 2,  0,  6,  3],\n","         [ 8,  7,  6,  5],\n","         [14,  9, 15,  6]]])"]},"metadata":{},"execution_count":82}],"source":["# Cumulative sum of elements along columns\n","A.cumsum(axis = 1)"]},{"cell_type":"markdown","metadata":{"id":"v_4VmMUwl826"},"source":["## <font color = 'pickle'>**Accessing elements of a Tensor**\n","\n","We can access individual elements of a Tensor using **index values**. Indexing always **starts from 0**.\n","\n","For example if the tensor is: `[10, 12, 31, 34]`\n","\n","Index of 10 is 0, index of 12 is 1 and so on."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":130,"status":"ok","timestamp":1694277370634,"user":{"displayName":"Akshat Patil","userId":"05034820051910580502"},"user_tz":300},"id":"G6nbzmQxkkIp","outputId":"a5483cf5-046d-40b0-87ee-f0cbb251917a"},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[1, 2, 5],\n","        [7, 8, 9]])\n","tensor([1, 2, 5])\n"]}],"source":["t1 = torch.tensor([[1, 2, 5], [7, 8, 9]])\n","\n","# Printing all elements\n","print(t1)\n","\n","# Get the first row\n","print(t1[0])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":183,"status":"ok","timestamp":1694277373113,"user":{"displayName":"Akshat Patil","userId":"05034820051910580502"},"user_tz":300},"id":"03U-6P0JFbse","outputId":"2a812e76-7164-4e63-8d42-a4a2b7229571"},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor(7)\n"]}],"source":["# Get the first element of the second row\n","print(t1[1][0])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":175,"status":"ok","timestamp":1694277374264,"user":{"displayName":"Akshat Patil","userId":"05034820051910580502"},"user_tz":300},"id":"-DeI6GRw31bK","outputId":"e22f8f13-f2da-40da-ada8-2020a263895f"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(7)"]},"metadata":{},"execution_count":85}],"source":["# Get the first element of the second row\n","t1[1, 0]"]},{"cell_type":"markdown","metadata":{"id":"MDgRsn4wuoVA"},"source":["## <font color = 'pickle'>**Accessing a Sub-Tensor**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":97,"status":"ok","timestamp":1694277378459,"user":{"displayName":"Akshat Patil","userId":"05034820051910580502"},"user_tz":300},"id":"CwK4E8muunYb","outputId":"f8edfd7f-8e49-446f-cccb-84091f697daa"},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([ 9, 13, 21, 45])\n","tensor([ 9, 21])\n"]}],"source":["t1 = torch.tensor([1, 5, 9, 13, 21, 45, 67, 34])\n","\n","# Specify [from: to : step)\n","# by default step is 1\n","# here from is inclusive but to is not\n","\n","# Get a subarray [9, 13, 21, 45]\n","# index 2 (i.e 9) is inclusive but index 6 (i.e. 67) is not and step size is 1\n","print(t1[2:6])\n","\n","# Get a subarray [9, 21]\n","print(t1[2:6:2])  # step size is 2"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":106,"status":"ok","timestamp":1694277379822,"user":{"displayName":"Akshat Patil","userId":"05034820051910580502"},"user_tz":300},"id":"knosW3KhFZA0","outputId":"f7b63918-7634-4322-9904-7bb82dc8a7c9"},"outputs":[{"output_type":"stream","name":"stdout","text":["array and subarray before modifying subarray\n","tensor([ 1,  5,  9, 13, 21, 45, 67, 34])\n","tensor([ 9, 13, 21, 45])\n","\n","array and subarray after modifying subarray\n","tensor([  1,   5, 100,  13,  21,  45,  67,  34])\n","tensor([100,  13,  21,  45])\n"]}],"source":["# subarrays created using slicing and indexing do not create a copy,\n","# modifying the subarray modifies the original tensor as well\n","\n","t1 = torch.tensor([1, 5, 9, 13, 21, 45, 67, 34])\n","t2 = t1[2:6]\n","print(\"array and subarray before modifying subarray\")\n","print(t1)\n","print(t2)\n","\n","# modify subarray\n","\n","t2[0] = 100\n","print(\"\\narray and subarray after modifying subarray\")\n","print(t1)\n","print(t2)"]},{"cell_type":"markdown","metadata":{"id":"o6iUsJ8uGhWG"},"source":["<font color = 'indianred'> **As we can see above, modifying subarray, modifes the original array as well**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":111,"status":"ok","timestamp":1694277382429,"user":{"displayName":"Akshat Patil","userId":"05034820051910580502"},"user_tz":300},"id":"APgdHOel8Jmi","outputId":"b1f5d2a7-a29f-4237-d2c6-ff30d6f99a75"},"outputs":[{"output_type":"stream","name":"stdout","text":["array and subarray before modifying subarray\n","tensor([ 1,  5,  9, 13, 21, 45, 67, 34])\n","tensor([ 9, 13, 21, 45])\n","\n","array and subarray after modifying subarray\n","tensor([ 1,  5,  9, 13, 21, 45, 67, 34])\n","tensor([100,  13,  21,  45])\n"]}],"source":["# use clone() method if yu specifically want to create a copy\n","\n","t1 = torch.tensor([1, 5, 9, 13, 21, 45, 67, 34])\n","t2 = t1[2:6].clone()\n","print(\"array and subarray before modifying subarray\")\n","print(t1)\n","print(t2)\n","\n","# modify subarray\n","\n","t2[0] = 100\n","print(\"\\narray and subarray after modifying subarray\")\n","print(t1)\n","print(t2)"]},{"cell_type":"markdown","metadata":{"id":"1pE-pEt88b5R"},"source":["<font color = 'indianred'> **SInce we created  a copy, modifying subarray, does not modify the original array as well**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":114,"status":"ok","timestamp":1694277384070,"user":{"displayName":"Akshat Patil","userId":"05034820051910580502"},"user_tz":300},"id":"qV1DADJLGKcJ","outputId":"77c77478-c131-4506-8164-4886b842bdab"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[ 1,  2,  3,  4],\n","        [ 5,  6,  7,  8],\n","        [ 9, 10, 11, 12],\n","        [13, 14, 15, 16]])"]},"metadata":{},"execution_count":89}],"source":["t1 = torch.tensor([[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12], [13, 14, 15, 16]])\n","t1"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":103,"status":"ok","timestamp":1694277385386,"user":{"displayName":"Akshat Patil","userId":"05034820051910580502"},"user_tz":300},"id":"0R7kNTmtGmIj","outputId":"16ec6bc0-0644-4825-d7c1-41d7da21c1e4"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[ 6,  7],\n","        [10, 11]])"]},"metadata":{},"execution_count":90}],"source":["# get the sub array [[6,7], [10,11]]\n","t1[1:3, 1:3]"]},{"cell_type":"markdown","metadata":{"id":"nhLHAqPLix2O"},"source":["## <font color = 'pickle'>**Operations on tensors of same size**\n","We can call element-wise operations on any two tensors of the same shape."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":194,"status":"ok","timestamp":1694277414440,"user":{"displayName":"Akshat Patil","userId":"05034820051910580502"},"user_tz":300},"id":"fXtbVUp8IAUZ","outputId":"6bf08525-2303-4c65-b1dc-0a32221e5cb6"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(tensor([ 3.,  4.,  6., 10.]),\n"," tensor([-1.,  0.,  2.,  6.]),\n"," tensor([ 2.,  4.,  8., 16.]),\n"," tensor([0.5000, 1.0000, 2.0000, 4.0000]),\n"," tensor([ 1.,  4., 16., 64.]))"]},"metadata":{},"execution_count":91}],"source":["x = torch.tensor([1.0, 2, 4, 8])\n","y = torch.tensor([2, 2, 2, 2])\n","x + y, x - y, x * y, x / y, x**y  # The ** operator is exponentiation"]},{"cell_type":"markdown","metadata":{"id":"NsU8FU-9s3kl"},"source":["## <font color = 'pickle'>**Changing the shape of tensors**"]},{"cell_type":"markdown","metadata":{"id":"4Dw-lJ1Lu3fT"},"source":["### <font color = 'pickle'>**1) Reshape**\n","\n","If we want to change the shape of our tensor, without affecting the elements present, we can use the ***reshape*** function."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":132,"status":"ok","timestamp":1694278904829,"user":{"displayName":"Akshat Patil","userId":"05034820051910580502"},"user_tz":300},"id":"SlxcKezfxMVK","outputId":"d75570ab-ab8b-46d7-884a-a7ff164198ab"},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n","tensor([[0, 1, 2, 3, 4],\n","        [5, 6, 7, 8, 9]])\n"]}],"source":["# Initializing a tensor with 10 elements from 0 to 9\n","t = torch.arange(10)\n","print(t)\n","\n","# Changing the shape of tensor t from 1x10 to 2x5\n","tr = t.reshape(2, 5)\n","print(tr)"]},{"cell_type":"markdown","metadata":{"id":"00BapNUKxlMQ"},"source":["If we have to specify just 1 dimension in reshape function and want the function to calculate the second dimension itself, we can write `-1` in place of second dimension.\n","\n","For 2 rows, we will write `reshape(2,-1)`\n","\n","For 5 columns, we will write `reshape(-1,5)`."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":103,"status":"ok","timestamp":1694278920075,"user":{"displayName":"Akshat Patil","userId":"05034820051910580502"},"user_tz":300},"id":"9CYpAZaAxkxc","outputId":"0076667d-56a0-4ea0-a8bf-d6c1bc175a8e"},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[0, 1, 2, 3, 4],\n","        [5, 6, 7, 8, 9]])\n"]}],"source":["# Changing the shape of tensor t from 1 row to 2 rows\n","tr1 = t.reshape(2, -1)\n","print(tr1)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":118,"status":"ok","timestamp":1694278921484,"user":{"displayName":"Akshat Patil","userId":"05034820051910580502"},"user_tz":300},"id":"D6qaekDtPp9u","outputId":"3444af71-45a3-4050-d37c-fa0318492f07"},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[0, 1, 2, 3, 4],\n","        [5, 6, 7, 8, 9]])\n"]}],"source":["# Changing the shape of tensor t from 10 columns to 5 columns\n","tr2 = t.reshape(-1, 5)\n","print(tr2)"]},{"cell_type":"markdown","metadata":{"id":"AWCEvFyfu7Xl"},"source":["### <font color = 'pickle'>**2) View**\n","\n","We can allow a tensor to be a view of an existing tensor. It performs the same operation as reshape. The only difference is that View will not create a copy and will allow us to perform fast and memory efficient computations whereas reshape may or may not share the same memory. There's a good discussion of the differences [here](https://stackoverflow.com/questions/49643225/whats-the-difference-between-reshape-and-view-in-pytorch).\n","\n","Line from above link \" *Another difference is that reshape() can operate on both contiguous and non-contiguous tensor while view() can only operate on contiguous tensor* \"\n","\n","[Definition of contiguous](https://stackoverflow.com/questions/26998223/what-is-the-difference-between-contiguous-and-non-contiguous-arrays/26999092#26999092)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":106,"status":"ok","timestamp":1694279683079,"user":{"displayName":"Akshat Patil","userId":"05034820051910580502"},"user_tz":300},"id":"WVrQF7xw25-f","outputId":"7443305f-1dfe-4f85-aa55-735b4151db05"},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n","shape:\n","torch.Size([10])\n","\n","tensor([[0, 1, 2, 3, 4],\n","        [5, 6, 7, 8, 9]])\n","shape:\n","torch.Size([2, 5])\n"]}],"source":["# Initializing a tensor with 10 elements from 0 to 9\n","t = torch.arange(10)\n","print(t,'shape:', t.shape, sep='\\n', end = '\\n\\n')\n","# Changing the shape of tensor t from 1x10 to 2x5\n","t = t.view(2, 5)\n","print(t,'shape:', t.shape, sep='\\n')"]},{"cell_type":"markdown","metadata":{"id":"WOZJuWP84T7N"},"source":["Views can reflect changes from the base tensor."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":112,"status":"ok","timestamp":1694279748600,"user":{"displayName":"Akshat Patil","userId":"05034820051910580502"},"user_tz":300},"id":"Yfl2ob_w3X88","outputId":"ffdba652-0d3b-452a-ddb0-f8932731f6fd"},"outputs":[{"output_type":"stream","name":"stdout","text":["before changing the base tensor\n","tensor([[0, 1, 2, 3, 4],\n","        [5, 6, 7, 8, 9]])\n","\n","after changing the base tensor\n"," tensor([[67,  1,  2,  3,  4],\n","        [ 5,  6,  7,  8,  9]])\n"]}],"source":["t = torch.arange(10)\n","\n","# Create a view of tensor t\n","tr = t.view(2, 5)\n","\n","# Before change in base tensor\n","print(f\"before changing the base tensor\\n{tr}\")\n","\n","# Modifying element of base tensor\n","t[0] = 67\n","\n","# After change in base tensor\n","print(f\"\\nafter changing the base tensor\\n {tr}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":117,"status":"ok","timestamp":1694279753733,"user":{"displayName":"Akshat Patil","userId":"05034820051910580502"},"user_tz":300},"id":"FQUVywTktsOf","outputId":"230aa9c4-7293-4b36-b219-a34c0416b0c8"},"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([2, 10])\n"]}],"source":["# we can use -1 with view as well.\n","t = torch.rand((4, 5))\n","t1 = t.view(2, -1)\n","print(t1.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":104,"status":"ok","timestamp":1694279961984,"user":{"displayName":"Akshat Patil","userId":"05034820051910580502"},"user_tz":300},"id":"FvbN4qgNQFEs","outputId":"3bcdf5fd-4262-4da8-da39-42868c66fc9f"},"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([60])\n","torch.Size([60])\n"]}],"source":["# we can also flatten the tensor (convert the tensor to one dimensional tensor) by using view(-1)\n","# this gives the same result as method flatten()\n","t = torch.rand((4, 5, 3))\n","t2 = t.view(-1)\n","t3 = t.flatten()\n","print(t2.shape)\n","print(t3.shape)"]},{"cell_type":"markdown","metadata":{"id":"UCacRHTqv-SN"},"source":["### <font color = 'pickle'> **3) Adding and removing dimensions of size 1**\n","- Insert a dimension of size 1 at a specific location (location specified by dim) using `torch.unsqueeze(dim)`\n","- Remove a dimension of size 1 at a specific location (location specified by dim) using `torch.squeeze(dim)`\n","- Remove all dimensions of size 1 using `torch.squeeze()`\n","- Insert dimenion of size 1 using `None `keyword\n","- Remove dimenion of size 1 using `0 `keyword"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":152,"status":"ok","timestamp":1694280139768,"user":{"displayName":"Akshat Patil","userId":"05034820051910580502"},"user_tz":300},"id":"bjzYf6InwH6_","outputId":"b2a23624-33d3-4e40-aa2b-18927a0491a4"},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[1., 1.],\n","        [1., 1.]])\n"]},{"output_type":"execute_result","data":{"text/plain":["torch.Size([2, 2])"]},"metadata":{},"execution_count":126}],"source":["# Initialize an tensor\n","t1 = torch.ones(2, 2)\n","print(t1)\n","t1.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":140,"status":"ok","timestamp":1694280142378,"user":{"displayName":"Akshat Patil","userId":"05034820051910580502"},"user_tz":300},"id":"X0NYoZm0wqRF","outputId":"39a7d7c4-03a7-434c-9785-e43b04f750d1"},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[[1., 1.],\n","         [1., 1.]]])\n"]},{"output_type":"execute_result","data":{"text/plain":["torch.Size([1, 2, 2])"]},"metadata":{},"execution_count":127}],"source":["# add dimension of size 1 in the beginning using unsqueeze method and argument dim = 0\n","t1 = t1.unsqueeze(dim=0)\n","print(t1)\n","t1.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":168,"status":"ok","timestamp":1694280160773,"user":{"displayName":"Akshat Patil","userId":"05034820051910580502"},"user_tz":300},"id":"w2F1MRj3xBYX","outputId":"8739691e-a19f-46cc-c85a-995a19a6f675"},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[[[1.],\n","          [1.]],\n","\n","         [[1.],\n","          [1.]]]])\n","torch.Size([1, 2, 2, 1])\n"]}],"source":["# add dimesnion of size 1 at the end usin unsqueeze method and dim = 3\n","t1 = t1.unsqueeze(dim=3)\n","print(t1)\n","print(t1.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":107,"status":"ok","timestamp":1694280171321,"user":{"displayName":"Akshat Patil","userId":"05034820051910580502"},"user_tz":300},"id":"o_9kJKcjxffh","outputId":"e1d36a18-992e-4b50-b60e-0b65bc3075c3"},"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([2, 2, 5])\n","torch.Size([2, 1, 2, 5])\n"]}],"source":["# We can add new dimesnion at any place\n","t1 = torch.arange(20).view(2, 2, 5)\n","print(t1.shape)\n","t1 = t1.unsqueeze(dim=1)\n","print(t1.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":101,"status":"ok","timestamp":1694280178274,"user":{"displayName":"Akshat Patil","userId":"05034820051910580502"},"user_tz":300},"id":"clT8xFNkx_GL","outputId":"081eeacc-318e-474f-9515-a8fe17cd1709"},"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([2, 1, 2, 1, 5, 1])\n"]}],"source":["# we can also use None keyword to add dimension of size 1 at multiple locations\n","t1 = t1[:, :, :, None, :, None]\n","print(t1.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":132,"status":"ok","timestamp":1694280181617,"user":{"displayName":"Akshat Patil","userId":"05034820051910580502"},"user_tz":300},"id":"fVJMCxpPYgxP","outputId":"5e7711a9-2d88-4f1e-c30c-6aef86dd2908"},"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([2, 2, 1, 5, 1])\n"]}],"source":["# Remove a dimension of size 1 at a specific location using torch.squeeze(dim)\n","t1 = t1.squeeze(dim=1)\n","print(t1.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":146,"status":"ok","timestamp":1694280184041,"user":{"displayName":"Akshat Patil","userId":"05034820051910580502"},"user_tz":300},"id":"NedAKkzJYnhU","outputId":"6a3b2612-3a20-4e33-fba1-79b35220817d"},"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([2, 2, 5, 1])\n"]}],"source":["# Remove a dimension of size 1 at a specific location using 0 keyword\n","t1 = t1[:, :, 0]\n","print(t1.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":96,"status":"ok","timestamp":1694280186372,"user":{"displayName":"Akshat Patil","userId":"05034820051910580502"},"user_tz":300},"id":"r7fHrZ-TY2qN","outputId":"5e341c5f-bda8-4c75-f885-ecaada5d7e79"},"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([2, 2, 5])\n"]}],"source":["# Removing all dimensions of size 1 using torch.squeeze()\n","t1 = t1.squeeze()\n","print(t1.shape)"]},{"cell_type":"markdown","metadata":{"id":"4SewxjuW7O6Y"},"source":["### <font color = 'pickle'>**4) Adopting shape of other tensors**\n","We can use view_as(input) to adopt shape of other tensors"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":98,"status":"ok","timestamp":1694280209505,"user":{"displayName":"Akshat Patil","userId":"05034820051910580502"},"user_tz":300},"id":"rkxKGSH57eBt","outputId":"971b8b03-da02-4922-b3af-9f2c05c9c12c"},"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([2, 5])\n","torch.Size([2, 5])\n"]}],"source":["a = torch.arange(10).view(2, 5)\n","# create a tensor b filled with ones (10 elements) and has same shape as b\n","b = torch.ones(10).view_as(a)\n","print(a.shape)\n","print(b.shape)"]},{"cell_type":"markdown","metadata":{"id":"tMtrejzioBMJ"},"source":["### <font color = 'pickle'>**5) Permute**\n","\n","Permute function rearranges the original tensor according to the desired ordering and returns a new multidimensional rotated tensor.\n","\n","Let us consider an example:\n","\n","If the size of a tensor is (2, 3, 4),\n","\n","- First size is 2\n","- Second size is 3\n","- Third size is 4\n","\n","Now, in case of permute we will just change the ordering of the sizes. Thus if we write permute(0, 2, 1) the new tensor will have:\n","\n","- First size is 2 (1st size of previous)\n","- Second size is 4 (3rd size of previous)\n","- Third size is 3 (2nd size of previous)\n","\n","Pytorch's function permute() only permutes or in other words shuffles the order of the axes of a tensor whereas view() reshapes the tensor by reducing/expanding the size of each dimension.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":134,"status":"ok","timestamp":1694280328624,"user":{"displayName":"Akshat Patil","userId":"05034820051910580502"},"user_tz":300},"id":"1r4bB2APt7oR","outputId":"6fcbfdfa-68ae-41fe-d5d2-7f5044e6b071"},"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([2, 4])\n","tensor([[4, 9, 3, 0],\n","        [3, 9, 7, 3]])\n"]}],"source":["# Initilaize a tensor and print it's size and elements\n","torch.manual_seed(0)\n","t1 = torch.randint(0, 10, size=(2, 4))\n","print(t1.size())\n","print(t1)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":135,"status":"ok","timestamp":1694280330427,"user":{"displayName":"Akshat Patil","userId":"05034820051910580502"},"user_tz":300},"id":"33olxUBS7dcW","outputId":"a37d8158-3b3e-49e2-9ec8-8f03e28762bb"},"outputs":[{"output_type":"execute_result","data":{"text/plain":[" 4\n"," 9\n"," 3\n"," 0\n"," 3\n"," 9\n"," 7\n"," 3\n","[torch.storage.TypedStorage(dtype=torch.int64, device=cpu) of size 8]"]},"metadata":{},"execution_count":136}],"source":["t1.storage()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":105,"status":"ok","timestamp":1694280335532,"user":{"displayName":"Akshat Patil","userId":"05034820051910580502"},"user_tz":300},"id":"2FbSfoYX7qw0","outputId":"c89b3c12-4776-4056-b4ca-d1a3adf7a612"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":137}],"source":["t1.is_contiguous()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":135,"status":"ok","timestamp":1694280437446,"user":{"displayName":"Akshat Patil","userId":"05034820051910580502"},"user_tz":300},"id":"hW-TQkXr7yKL","outputId":"8ce48a73-0591-422c-fc6b-cee5d518cbba"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(4, 1)"]},"metadata":{},"execution_count":142}],"source":["t1.stride()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":140,"status":"ok","timestamp":1694280461838,"user":{"displayName":"Akshat Patil","userId":"05034820051910580502"},"user_tz":300},"id":"OC0qXNZNVz3_","outputId":"7d60ae14-5b83-46f0-d2b4-a16593785f68"},"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([4, 2])\n","\n","tensor([[4, 3],\n","        [9, 9],\n","        [3, 7],\n","        [0, 3]])\n"]}],"source":["# Permute the tensor and print it's size and elements\n","t1_p = t1.permute(1, 0)\n","print(t1_p.size())\n","print()\n","print(t1_p)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":154,"status":"ok","timestamp":1694280475217,"user":{"displayName":"Akshat Patil","userId":"05034820051910580502"},"user_tz":300},"id":"JmcbvT6t8E3h","outputId":"290dfe4b-0805-479d-ff21-4b869058f66e"},"outputs":[{"output_type":"execute_result","data":{"text/plain":[" 4\n"," 9\n"," 3\n"," 0\n"," 3\n"," 9\n"," 7\n"," 3\n","[torch.storage.TypedStorage(dtype=torch.int64, device=cpu) of size 8]"]},"metadata":{},"execution_count":144}],"source":["t1_p.storage()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":145,"status":"ok","timestamp":1694280480081,"user":{"displayName":"Akshat Patil","userId":"05034820051910580502"},"user_tz":300},"id":"aoDcYejx8VkU","outputId":"a9d8054d-51bc-4d5e-f886-536d4138460f"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(1, 4)"]},"metadata":{},"execution_count":145}],"source":["t1_p.stride()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":138,"status":"ok","timestamp":1694280486117,"user":{"displayName":"Akshat Patil","userId":"05034820051910580502"},"user_tz":300},"id":"QWm1K3dq8YFE","outputId":"c6653935-99e8-4817-8d52-1847d4b8891c"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["False"]},"metadata":{},"execution_count":146}],"source":["t1_p.is_contiguous()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":193},"executionInfo":{"elapsed":113,"status":"error","timestamp":1694280487740,"user":{"displayName":"Akshat Patil","userId":"05034820051910580502"},"user_tz":300},"id":"D4xvyGKg8t0w","outputId":"9d754e03-29fb-4c10-af96-646296be7e7b"},"outputs":[{"output_type":"error","ename":"RuntimeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-147-9e0e74491421>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mt1_p\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m: view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead."]}],"source":["t1_p.view(2, 4)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":100,"status":"ok","timestamp":1694280505021,"user":{"displayName":"Akshat Patil","userId":"05034820051910580502"},"user_tz":300},"id":"EZFobTRY86RF","outputId":"a1133bea-72a5-4532-ab60-8221fce432ea"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[4, 3, 9, 9],\n","        [3, 7, 0, 3]])"]},"metadata":{},"execution_count":148}],"source":["t1_p.reshape(2, 4)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":133,"status":"ok","timestamp":1694280543015,"user":{"displayName":"Akshat Patil","userId":"05034820051910580502"},"user_tz":300},"id":"YBaHNUYro71U","outputId":"19a70a14-526e-4aab-b2bc-6da55f17a0b4"},"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([2, 3, 4])\n","\n","tensor([[[0.4963, 0.7682, 0.0885, 0.1320],\n","         [0.3074, 0.6341, 0.4901, 0.8964],\n","         [0.4556, 0.6323, 0.3489, 0.4017]],\n","\n","        [[0.0223, 0.1689, 0.2939, 0.5185],\n","         [0.6977, 0.8000, 0.1610, 0.2823],\n","         [0.6816, 0.9152, 0.3971, 0.8742]]])\n"]}],"source":["# Initilaize a tensor and print it's size and elements\n","torch.manual_seed(0)\n","t2 = torch.rand(2, 3, 4)\n","print(t2.size())\n","print(f\"\\n{t2}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":102,"status":"ok","timestamp":1694280551710,"user":{"displayName":"Akshat Patil","userId":"05034820051910580502"},"user_tz":300},"id":"wCufxBysSUta","outputId":"7d3ccf3c-d0fc-40c4-a4cd-193495394144"},"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([2, 4, 3])\n","\n","tensor([[[0.4963, 0.3074, 0.4556],\n","         [0.7682, 0.6341, 0.6323],\n","         [0.0885, 0.4901, 0.3489],\n","         [0.1320, 0.8964, 0.4017]],\n","\n","        [[0.0223, 0.6977, 0.6816],\n","         [0.1689, 0.8000, 0.9152],\n","         [0.2939, 0.1610, 0.3971],\n","         [0.5185, 0.2823, 0.8742]]])\n"]}],"source":["# Permute the tensor and print it's size and elements - use permute (0, 2, 1)\n","t2_p = t2.permute(0, 2, 1)\n","print(t2_p.size())\n","print(f\"\\n{t2_p}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":140,"status":"ok","timestamp":1694280646475,"user":{"displayName":"Akshat Patil","userId":"05034820051910580502"},"user_tz":300},"id":"KDbnALFR2ut_","outputId":"0cb494db-674b-47c8-efc3-165f00a59f4d"},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[0, 1, 2],\n","        [3, 4, 5]])\n"]}],"source":["# difference between permute and view\n","x = torch.arange(3 * 2).view(2, 3)\n","print(x)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":162,"status":"ok","timestamp":1694280656163,"user":{"displayName":"Akshat Patil","userId":"05034820051910580502"},"user_tz":300},"id":"pr6fydJ1uuFo","outputId":"5a0915ec-addc-41a0-efa0-4d77e0dc059f"},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[0, 1],\n","        [2, 3],\n","        [4, 5]])\n"]}],"source":["# create a view (3, 2)\n","print(x.view(3, 2))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":100,"status":"ok","timestamp":1694280661344,"user":{"displayName":"Akshat Patil","userId":"05034820051910580502"},"user_tz":300},"id":"pK7vUWrQuyoy","outputId":"715a77c4-ef72-4271-d713-e355dc458da7"},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[0, 3],\n","        [1, 4],\n","        [2, 5]])\n"]}],"source":["# permute axis(1, 0)\n","print(x.permute(1, 0))"]},{"cell_type":"markdown","metadata":{"id":"f-HexBDq4PIi"},"source":["Question and answer taken from following reference: <br>\n","https://discuss.pytorch.org/t/different-between-permute-transpose-view-which-should-i-use/32916\n","\n","- (1) If I have a feature size of BxCxHxW, I want to reshape it to BxCxHW. Which one is a good option?\n","- (2) If I have a feature size of BxCxHxW, I want to change it to BxCxWxH . Which one is a good option?\n","- (3) If I have a feature size of BxCxH, I want to change it to BxCxHx1 . Which one is a good option?\n","\n","Solution:\n","- permute changes the order of dimensions aka axes, so 2 would be a use case. Transpose is a special case of permute, use it with 2d tensors.\n","- view can combine and split axes, so 1 and 3 can use view,\n","- note that view can fail for noncontiguous layouts (e.g. crop a picture using indexing), in these cases reshape will do the right thing,\n","- for adding dimensions of size 1 (case 3), there also are unsqueeze and indexing with None.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Nq3DaJK0F76y"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"wMkz0iL56Va7"},"source":["## <font color = 'pickle'>**Concatenating Tensors**\n","\n","We can use `torch.cat((tensors_to_concatenate), dim)` to concatenate tensors.\n","\n","The tensors must have the same shape (except in the concatenating dimension)."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":134,"status":"ok","timestamp":1694281104171,"user":{"displayName":"Akshat Patil","userId":"05034820051910580502"},"user_tz":300},"id":"TwWw4dEphsw_","outputId":"e6a180f9-d80d-4e0c-f748-88911c4a94c9"},"outputs":[{"output_type":"stream","name":"stdout","text":["shape of x1 is torch.Size([2, 5])\n","shape of x2 is torch.Size([4, 5])\n","shape of x3 is torch.Size([2, 3])\n","\n","x1\n",":tensor([[8, 1, 9, 6, 1],\n","        [0, 2, 9, 4, 3]])\n","\n","x2\n",":tensor([[1., 1., 1., 1., 1.],\n","        [1., 1., 1., 1., 1.],\n","        [1., 1., 1., 1., 1.],\n","        [1., 1., 1., 1., 1.]])\n","\n","x3\n",":tensor([[0., 0., 0.],\n","        [0., 0., 0.]])\n","\n","shape of x1_x2 is torch.Size([6, 5])\n","shape of x2_x3 is torch.Size([2, 8])\n","\n","x1_x2\n",":tensor([[8., 1., 9., 6., 1.],\n","        [0., 2., 9., 4., 3.],\n","        [1., 1., 1., 1., 1.],\n","        [1., 1., 1., 1., 1.],\n","        [1., 1., 1., 1., 1.],\n","        [1., 1., 1., 1., 1.]])\n","\n","x1_x3\n",":tensor([[8., 1., 9., 6., 1., 0., 0., 0.],\n","        [0., 2., 9., 4., 3., 0., 0., 0.]])\n"]}],"source":["# we can use torch\n","x1 = torch.randint(low=0, high=10, size=(2, 5))\n","x2 = torch.ones(4, 5)\n","x3 = torch.zeros(2, 3)\n","\n","# The tensors must have the same shape (except in the concatenating dimension)\n","# x1 and x2 have the same shape except for dim = 0, hence we can conactenate these along dim = 0\n","# x1 and x3 have the same shape except for dim = 1, hence we can conactenate these along dim = 1\n","# we cannot concatenate x2 and x3 along any dimension\n","\n","print(f\"shape of x1 is {x1.shape}\")\n","print(f\"shape of x2 is {x2.shape}\")\n","print(f\"shape of x3 is {x3.shape}\")\n","print(f\"\\nx1\\n:{x1}\")\n","print(f\"\\nx2\\n:{x2}\")\n","print(f\"\\nx3\\n:{x3}\\n\")\n","\n","x1_x2 = torch.cat((x1, x2), dim=0)\n","x1_x3 = torch.cat((x1, x3), dim=1)\n","print(f\"shape of x1_x2 is {x1_x2.shape}\")\n","print(f\"shape of x2_x3 is {x1_x3.shape}\")\n","print(f\"\\nx1_x2\\n:{x1_x2}\")\n","print(f\"\\nx1_x3\\n:{x1_x3}\")"]},{"cell_type":"markdown","metadata":{"id":"xX3tyHVrpN4s"},"source":["## <font color = 'pickle'>**Some commonly used Tensors**"]},{"cell_type":"markdown","metadata":{"id":"wpm1RohNpzKv"},"source":["###<font color = 'pickle'>**1) Tensor containing all zeros/ all ones/ or any value**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":141,"status":"ok","timestamp":1694281118845,"user":{"displayName":"Akshat Patil","userId":"05034820051910580502"},"user_tz":300},"id":"O8FzuQrEoB2T","outputId":"9861522a-7d67-420a-f5d9-c21472781384"},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([0., 0., 0., 0., 0.])\n","tensor([[[0., 0., 0.],\n","         [0., 0., 0.]],\n","\n","        [[0., 0., 0.],\n","         [0., 0., 0.]]])\n"]}],"source":["# Tensor containing all zeros, size = 10\n","z1 = torch.zeros(5)\n","\n","# Tensor containing all zeros, size = 2 X 2 X 3\n","z2 = torch.zeros(2, 2, 3)\n","\n","print(z1)\n","print(z2)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":89,"status":"ok","timestamp":1694281129406,"user":{"displayName":"Akshat Patil","userId":"05034820051910580502"},"user_tz":300},"id":"JMbGq7ceqba0","outputId":"04ea7521-4f6f-4a7b-dbba-cb466a03016a"},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([1., 1., 1., 1., 1., 1., 1.])\n","tensor([[[1., 1., 1.],\n","         [1., 1., 1.]]])\n"]}],"source":["# Tensor containing all ones, size = 7\n","z1 = torch.ones(7)\n","\n","# Tensor containing all ones, size = 1 X 2 X 3\n","z2 = torch.ones(1, 2, 3)\n","\n","print(z1)\n","print(z2)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":105,"status":"ok","timestamp":1694281154782,"user":{"displayName":"Akshat Patil","userId":"05034820051910580502"},"user_tz":300},"id":"EVREaBSAZK7w","outputId":"070e52e0-2b58-4433-f45c-a14cea29b5e3"},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[[5, 5, 5],\n","         [5, 5, 5]],\n","\n","        [[5, 5, 5],\n","         [5, 5, 5]]])\n"]}],"source":["# We can also use torch.full(size, fill_value) to create a tensor filled with any value\n","# Tensor containing all fives, size = 1 X 2 X 3\n","\n","z3 = torch.full(size=(2, 2, 3), fill_value=5)\n","print(z3)"]},{"cell_type":"markdown","metadata":{"id":"HxOWMc03qszf"},"source":["### <font color = 'pickle'>**2) Tensor with elements in a particular range**\n","Suppose we need a tensor with values `1, 2, 3, 4.....n. `\n","\n","We can simply specify the range and tensor will automatically get filled with these values."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":229,"status":"ok","timestamp":1694281423239,"user":{"displayName":"Akshat Patil","userId":"05034820051910580502"},"user_tz":300},"id":"Ydycx6TMqowD","outputId":"5710d9cf-6389-43a5-9f44-5964ae05c134"},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([1, 2, 3, 4, 5])\n"]}],"source":["# Creating a tensor with integers from 1 to 5 with space 1: [1, 2, 3, 4, 5]\n","# syntax arange(start, end, step) - create tensor with values in the interval [start, end).\n","# start is inclusive , end is not i.e. start <= values < end\n","tr1 = torch.arange(1, 6)\n","print(tr1)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":158,"status":"ok","timestamp":1694281426102,"user":{"displayName":"Akshat Patil","userId":"05034820051910580502"},"user_tz":300},"id":"I9n03rD_sFdS","outputId":"30f87952-e66a-4684-d9a3-10ff53421486"},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([ 0,  2,  4,  6,  8, 10])\n"]}],"source":["# Creating a tensor with integers from 0 to 10 with space 2 using \"step\" parameter: [0, 2, 4, 6, 8, 10]\n","tr2 = torch.arange(0, 11, step=2)\n","print(tr2)"]},{"cell_type":"markdown","metadata":{"id":"T4TOw7gUZ-He"},"source":["We can also use `torch.linspace()` to generate evenly spaced values between two numbers"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":191,"status":"ok","timestamp":1694281432590,"user":{"displayName":"Akshat Patil","userId":"05034820051910580502"},"user_tz":300},"id":"-9iboyquHrmW","outputId":"81729add-a84f-4484-b0ee-315880958b8a"},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([0.0000, 0.1111, 0.2222, 0.3333, 0.4444, 0.5556, 0.6667, 0.7778, 0.8889,\n","        1.0000])\n"]}],"source":["# Generate 10 evenly spaced values between 0 and 1 (both inclusive)\n","t1 = torch.linspace(0, 1, 10)\n","print(t1)"]},{"cell_type":"markdown","metadata":{"id":"sVqmTY7IzBmT"},"source":["###<font color = 'pickle'>**3) Tensor with elements from probability distribution**\n","\n","We can use the randn function to get elements from standard normal probabilty distribution i.e. normal dustribution with mean = 0 and variance = 1. If we want to select elements from normal ditsribution with different mean and variance then we should use torch.normal"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"N9dnwRY1zAoO"},"outputs":[],"source":["# for reproducabilty so that we get same results everytime we run this cell\n","torch.manual_seed(42)\n","\n","# Sample 500,000 values from standard normal distribution (mean = 0 , variance = 1)\n","t1 = torch.randn(500000)\n","\n","# Sample 500,000 values from normal distribution (mean = 5 , std = 2)\n","t2 = torch.normal(mean=5, std=2, size=(500000,))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":176,"status":"ok","timestamp":1694281487827,"user":{"displayName":"Akshat Patil","userId":"05034820051910580502"},"user_tz":300},"id":"Nty188mkR6Yx","outputId":"64ff1c93-741c-4f48-9b78-b168193a65a1"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mean and std of tensor using torch.randn\n","tensor(-0.0013)\n","tensor(1.0014)\n","\n","Mean and std of tensor using torch.normal\n","tensor(5.0002)\n","tensor(1.9971)\n"]}],"source":["print(\"Mean and std of tensor using torch.randn\")\n","print(torch.mean(t1))\n","print(torch.std(t1))\n","\n","print(\"\\nMean and std of tensor using torch.normal\")\n","print(torch.mean(t2))\n","print(torch.std(t2))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":203,"status":"ok","timestamp":1694281499754,"user":{"displayName":"Akshat Patil","userId":"05034820051910580502"},"user_tz":300},"id":"iyiCCPVZVg8H","outputId":"bb3fd84c-7bb1-4b06-cb0f-d85751009987"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[ 1.5410, -0.2934],\n","        [-2.1788,  0.5684],\n","        [-1.0845, -1.3986],\n","        [ 0.4033,  0.8380],\n","        [-0.7193, -0.4033]])"]},"metadata":{},"execution_count":177}],"source":["# for reproducabilty so that we get same results everytime we run this cell\n","torch.manual_seed(0)\n","\n","# we sampled 10 values from standard noemal distribution. (5, 2) is the shape.\n","t1 = torch.randn(5, 2)\n","t1"]},{"cell_type":"markdown","metadata":{"id":"SxiimV9UY7zu"},"source":["We can also sample from other distributions like torch.rand, torch.randint etc."]},{"cell_type":"markdown","metadata":{"id":"OIeaFS6SV5lS"},"source":["### <font color = 'pickle'>**4) Empty Tensor**\n","We can create uninitialized  tensors using torch.empty.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":96,"status":"ok","timestamp":1694281522246,"user":{"displayName":"Akshat Patil","userId":"05034820051910580502"},"user_tz":300},"id":"n7Z40rBVY3oA","outputId":"516a3cfe-6b36-4fd2-f646-dc4e2a9444a7"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[3.0266e+16, 4.5059e-41, 3.0266e+16, 4.5059e-41],\n","        [4.4842e-44, 0.0000e+00, 1.7937e-43, 0.0000e+00]])"]},"metadata":{},"execution_count":178}],"source":["# create empty tensor of shape (2, 4)\n","empty_tensor = torch.empty(2, 4)\n","empty_tensor"]},{"cell_type":"markdown","metadata":{"id":"UiNZ0HioJdXF"},"source":["### <font color = 'pickle'>**5) Commonly used tensors based on shape of other tensors**"]},{"cell_type":"markdown","metadata":{"id":"iJEmlIsUIuYu"},"source":["We can also use `torch.zeros_like(input)`, `torch.ones_like(input)`, `torch.full_like(input)` and `torch.empty_like(input)` to create tensors based on the shape of other tensors\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":113,"status":"ok","timestamp":1694281768820,"user":{"displayName":"Akshat Patil","userId":"05034820051910580502"},"user_tz":300},"id":"f4J69YwmJz75","outputId":"940ff651-25d4-467f-88a4-6ca4b073e5cb"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([2, 3])"]},"metadata":{},"execution_count":199}],"source":["input_tensor = torch.arange(6).view(2, 3)\n","input_tensor.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":100,"status":"ok","timestamp":1694281769995,"user":{"displayName":"Akshat Patil","userId":"05034820051910580502"},"user_tz":300},"id":"avYjEMStKF5g","outputId":"22b27fb8-42e1-4920-e43e-81dea5d729b7"},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[1, 1, 1],\n","        [1, 1, 1]])\n","tensor([[0, 0, 0],\n","        [0, 0, 0]])\n","tensor([[5, 5, 5],\n","        [5, 5, 5]])\n","tensor([[138106197446048,  99601731529456,               1],\n","        [ 99602753449152,               0,               0]])\n"]}],"source":["print(torch.ones_like(input_tensor))\n","print(torch.zeros_like(input_tensor))\n","print(torch.full_like(input_tensor, 5))\n","print(torch.empty_like(input_tensor))"]},{"cell_type":"markdown","metadata":{"id":"tsMg-Jujsm2W"},"source":["###<font color = 'pickle'> **6) Identity Matrix**\n","\n","Identity matrix is a matrix which has 1's along the diagnal and zeros everywhere else."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":98,"status":"ok","timestamp":1694281793751,"user":{"displayName":"Akshat Patil","userId":"05034820051910580502"},"user_tz":300},"id":"mM4w1jmDsOwR","outputId":"9b8f2497-e1c4-452a-b7d1-333924dcf2e7"},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[1., 0., 0.],\n","        [0., 1., 0.],\n","        [0., 0., 1.]])\n"]}],"source":["# Identity matrix of size 3\n","id_matrix = torch.eye(3)\n","print(id_matrix)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":148,"status":"ok","timestamp":1694281799420,"user":{"displayName":"Akshat Patil","userId":"05034820051910580502"},"user_tz":300},"id":"6PqCSmvwtBp4","outputId":"103d2fed-4bab-4acd-d353-2c4ef622b4e8"},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[1., 0., 0., 0., 0.],\n","        [0., 1., 0., 0., 0.],\n","        [0., 0., 1., 0., 0.],\n","        [0., 0., 0., 1., 0.],\n","        [0., 0., 0., 0., 1.]])\n"]}],"source":["# Identity matrix of size 5\n","id_matrix = torch.eye(5)\n","print(id_matrix)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CiBC1vbQDdMX"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"24OjlU8RnSie"},"source":["## <font color = 'pickle'>**Masks using binary tensors**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":93,"status":"ok","timestamp":1694281819576,"user":{"displayName":"Akshat Patil","userId":"05034820051910580502"},"user_tz":300},"id":"CxyRZVxGPgt_","outputId":"acf489ce-768b-4342-da7f-e4a2904805c4"},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([ True, False,  True, False,  True, False])\n","tensor([False,  True, False,  True, False,  True])\n"]}],"source":["# create a tensor which has probailities of events\n","prob = torch.tensor([0.7, 0.4, 0.6, 0.2, 0.8, 0.1])\n","\n","# Binary tensors\n","print(prob > 0.5)\n","print(prob <= 0.5)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":86,"status":"ok","timestamp":1694281821612,"user":{"displayName":"Akshat Patil","userId":"05034820051910580502"},"user_tz":300},"id":"ZwpKs1DRQd7J","outputId":"10f969cf-64d4-4981-9a1c-14a441156c75"},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([1., 0., 1., 0., 1., 0.])\n"]}],"source":["# create output tensor where output = 1 if prob >0.5 and 0 otherwise\n","# craete an empty output tensor of same shape as prob\n","output = torch.empty_like(prob)\n","\n","# update output tensor using the binary mask\n","output[prob > 0.5] = 1\n","output[prob <= 0.5] = 0\n","print(output)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FxTCjB994VN6"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}