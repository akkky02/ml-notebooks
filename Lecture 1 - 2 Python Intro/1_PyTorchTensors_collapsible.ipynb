{"cells":[{"cell_type":"markdown","metadata":{"id":"gaGkLWaZZv45"},"source":["# <font color = 'pickle'>**Lecture : Intoduction to PyTorch Tensors**\n"]},{"cell_type":"markdown","metadata":{"id":"sa1p0E6Sduea"},"source":["# <font color = 'pickle'>**Importing PyTorch Library**"]},{"cell_type":"code","source":["import torch\n","import numpy as np"],"metadata":{"id":"wPv_wHrSb0sl","executionInfo":{"status":"ok","timestamp":1723262280482,"user_tz":300,"elapsed":7599,"user":{"displayName":"Akshat Patil","userId":"09657732965291711317"}}},"execution_count":1,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"f46a5ee9"},"source":["\n","<details>\n","<summary>Click to expand code</summary>\n","\n","```python\n","!nvidia-smi\n"]},{"cell_type":"markdown","metadata":{"id":"1f4c264b"},"source":["\n","<details>\n","<summary>Click to expand code</summary>\n","\n","```python\n","import torch\n","import numpy as np\n"]},{"cell_type":"markdown","metadata":{"id":"IHWosHogaREo"},"source":["# <font color = 'pickle'>**Tensors**\n","\n","- Tensors are the basic building blocks of any deep learning network.\n","\n","- They are used to represent all the different types of data be it images, sound files, text data etc.\n","\n","- Tensors are **order N-matrix**.\n","\n","\n","If N=1, tensor will basically be a **vector**.\n","If N=2, tensor will be a **2-d matrix**.\n","\n","Why Tensors and not NumPy arrays?\n","\n","- NumPy only supports CPU computation.\n","- Tensor class supports automatic differentiation."]},{"cell_type":"markdown","metadata":{"id":"EJREGcIPcIm2"},"source":["**Let us start by importing PyTorch library and understand some of the basic functions on tensors.**"]},{"cell_type":"markdown","metadata":{"id":"TPUE3EKX2_Jr"},"source":["<img src = \"https://miro.medium.com/v2/resize:fit:891/0*jGB1CGQ9HdeUwlgB\" width =600 >"]},{"cell_type":"markdown","metadata":{"id":"_9Va52NtqsTo"},"source":["## <font color = 'pickle'>**Scalar**\n","- rank-0 tensor\n"]},{"cell_type":"code","source":["t = torch.tensor(1.)\n","print(t)\n","print(\"\\nsize:\",t.shape, sep ='\\n')\n","print(\"\\nnumber of dimensions:\", t.dim(), sep = \"\\n\")\n","print(\"\\nData Type:\", t.dtype, sep = \"\\n\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MvcokzI3b79j","executionInfo":{"status":"ok","timestamp":1723262399568,"user_tz":300,"elapsed":161,"user":{"displayName":"Akshat Patil","userId":"09657732965291711317"}},"outputId":"9a68ae0d-3c18-42f8-ad54-153238f7a800"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor(1.)\n","\n","size:\n","torch.Size([])\n","\n","number of dimensions:\n","0\n","\n","Data Type:\n","torch.float32\n"]}]},{"cell_type":"markdown","metadata":{"id":"f3da4962"},"source":["\n","<details>\n","<summary>Click to expand code</summary>\n","\n","```python\n","\n"]},{"cell_type":"markdown","metadata":{"id":"110ec8c1"},"source":["\n","<details>\n","<summary>Click to expand code</summary>\n","\n","```python\n","t = torch.tensor(1.)\n","print(t)\n","print(\"\\nsize:\",t.shape, sep ='\\n')\n","print(\"\\nnumber of dimensions:\", t.dim(), sep = \"\\n\")\n","print(\"\\nData Type:\", t.dtype, sep = \"\\n\")\n"]},{"cell_type":"markdown","metadata":{"id":"i8JRukoprQvo"},"source":["## <font color = 'pickle'>**Vector**\n","- rank-1 tensor"]},{"cell_type":"code","source":["t = torch.tensor([1,2.])\n","print(t)\n","print(\"\\nsize:\",t.shape, sep ='\\n')\n","print(\"\\nnumber of dimensions:\", t.dim(), sep = \"\\n\")\n","print(\"\\nData Type:\", t.dtype, sep = \"\\n\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IxRuwq-BcY_C","executionInfo":{"status":"ok","timestamp":1723262440768,"user_tz":300,"elapsed":185,"user":{"displayName":"Akshat Patil","userId":"09657732965291711317"}},"outputId":"e73a41f3-11ae-491f-e106-1d6ea99ef6d2"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([1., 2.])\n","\n","size:\n","torch.Size([2])\n","\n","number of dimensions:\n","1\n","\n","Data Type:\n","torch.float32\n"]}]},{"cell_type":"markdown","metadata":{"id":"dec889a9"},"source":["\n","<details>\n","<summary>Click to expand code</summary>\n","\n","```python\n","t = torch.tensor([1., 2])\n","print(t)\n","print(\"\\nsize:\",t.shape, sep ='\\n')\n","print(\"\\nnumber of dimensions:\", t.dim(), sep = \"\\n\")\n","print(\"\\nData Type:\", t.dtype, sep = \"\\n\")\n"]},{"cell_type":"markdown","metadata":{"id":"SMnpra7xr6Kf"},"source":["## <font color = 'pickle'>**Matrix**\n","- rank 2 tensor\n","\n","Matrices are 2-d arrays with size `n x m`. Here, n: number of rows and m: number of columns.\n","\n","If `m = n`, then the matrix is known as a `square matrix`.\n","\n","Precisely, matrices can be represented as:\n","$$\\mathbf{X}=\\begin{bmatrix} x_{11} & x_{12} & \\cdots & x_{1n} \\\\ x_{21} & x_{22} & \\cdots & x_{2n} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ x_{m1} & x_{m2} & \\cdots & x_{mn} \\\\ \\end{bmatrix}$$\n","<br>\n","\n"]},{"cell_type":"code","source":["t = torch.tensor([\n","     [1., 2, 3],\n","     [4, 5, 6]\n","    ])\n","print(t)\n","print(\"\\nsize:\",t.shape, sep ='\\n')\n","print(\"\\nnumber of dimensions:\", t.dim(), sep = \"\\n\")\n","print(\"\\nData Type:\", t.dtype, sep = \"\\n\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZeUI2qzQcnDc","executionInfo":{"status":"ok","timestamp":1723262465317,"user_tz":300,"elapsed":158,"user":{"displayName":"Akshat Patil","userId":"09657732965291711317"}},"outputId":"1d86252d-408d-4f99-f7ce-585601a363d4"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[1., 2., 3.],\n","        [4., 5., 6.]])\n","\n","size:\n","torch.Size([2, 3])\n","\n","number of dimensions:\n","2\n","\n","Data Type:\n","torch.float32\n"]}]},{"cell_type":"markdown","metadata":{"id":"b89e05e0"},"source":["\n","<details>\n","<summary>Click to expand code</summary>\n","\n","```python\n","t = torch.tensor([\n","     [1., 2, 3],\n","     [4, 5, 6]\n","    ])\n","print(t)\n","print(\"\\nsize:\",t.shape, sep ='\\n')\n","print(\"\\nnumber of dimensions:\", t.dim(), sep = \"\\n\")\n","print(\"\\nData Type:\", t.dtype, sep = \"\\n\")\n"]},{"cell_type":"markdown","metadata":{"id":"Hblncsd7wbVz"},"source":["\n","<img src = \"https://drive.google.com/uc?export=view&id=1822fQJQuXtzZ7DmO86pUXUj4aU9ZLor_\" width =600 >"]},{"cell_type":"markdown","metadata":{"id":"7USc0U8XsMbI"},"source":["## <font color = 'pickle'>**Higher Order Tensors**"]},{"cell_type":"markdown","metadata":{"id":"uuHbPHclzl7Z"},"source":["\n","### <font color = 'pickle'>**rank-3 tensor**"]},{"cell_type":"code","source":["t = torch.tensor([\n","    [[1, 2], [3,4]],\n","    [[5, 6], [7,8]],\n","    [[5, 6], [7,8]]\n","                  ])\n","print(t)\n","print(\"\\nsize:\",t.shape, sep ='\\n')\n","print(\"\\nnumber of dimensions:\", t.dim(), sep = \"\\n\")\n","print(\"\\nData Type:\", t.dtype, sep = \"\\n\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"APHGIKb3dPrt","executionInfo":{"status":"ok","timestamp":1723262631538,"user_tz":300,"elapsed":207,"user":{"displayName":"Akshat Patil","userId":"09657732965291711317"}},"outputId":"44e6ddf9-bec7-4f78-fda3-5d5aea2c8435"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[[1, 2],\n","         [3, 4]],\n","\n","        [[5, 6],\n","         [7, 8]],\n","\n","        [[5, 6],\n","         [7, 8]]])\n","\n","size:\n","torch.Size([3, 2, 2])\n","\n","number of dimensions:\n","3\n","\n","Data Type:\n","torch.int64\n"]}]},{"cell_type":"markdown","metadata":{"id":"8498f5d5"},"source":["\n","<details>\n","<summary>Click to expand code</summary>\n","\n","```python\n","t = torch.tensor([\n","    [[1, 2], [3,4]],\n","    [[5, 6], [7,8]],\n","    [[5, 6], [7,8]]\n","                  ])\n","print(t)\n","print(\"\\nsize:\",t.shape, sep ='\\n')\n","print(\"\\nnumber of dimensions:\", t.dim(), sep = \"\\n\")\n","print(\"\\nData Type:\", t.dtype, sep = \"\\n\")\n"]},{"cell_type":"markdown","metadata":{"id":"H2oM6QJXyF7m"},"source":["<img src = \"https://drive.google.com/uc?export=view&id=184X0Qjn0lwuJRSFoh_lEmR9v7yF7GxaA\" width =600 >\n","\n","Image source: https://dev.to/sandeepbalachandran/machine-learning-going-furthur-with-cnn-part-2-41km"]},{"cell_type":"markdown","metadata":{"id":"ojkafKrazhSM"},"source":["### <font color = 'pickle'>**rank-4 tensor**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0e82c43e"},"outputs":[],"source":["# print(t)"]},{"cell_type":"code","source":["t1 = torch.stack((t,t))\n","print(t1)\n","# print(t)\n","print(\"\\nsize:\",t1.shape, sep ='\\n')\n","print(\"\\nnumber of dimensions:\", t1.dim(), sep = \"\\n\")\n","print(\"\\nData Type:\", t1.dtype, sep = \"\\n\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gMVP65asdZGx","executionInfo":{"status":"ok","timestamp":1723262670706,"user_tz":300,"elapsed":198,"user":{"displayName":"Akshat Patil","userId":"09657732965291711317"}},"outputId":"a5d0ce34-8872-4e48-8108-66675e5228f5"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[[[1, 2],\n","          [3, 4]],\n","\n","         [[5, 6],\n","          [7, 8]],\n","\n","         [[5, 6],\n","          [7, 8]]],\n","\n","\n","        [[[1, 2],\n","          [3, 4]],\n","\n","         [[5, 6],\n","          [7, 8]],\n","\n","         [[5, 6],\n","          [7, 8]]]])\n","\n","size:\n","torch.Size([2, 3, 2, 2])\n","\n","number of dimensions:\n","4\n","\n","Data Type:\n","torch.int64\n"]}]},{"cell_type":"markdown","metadata":{"id":"58980eda"},"source":["\n","<details>\n","<summary>Click to expand code</summary>\n","\n","```python\n","t1 = torch.stack((t,t))\n","print(t1)\n","# print(t)\n","print(\"\\nsize:\",t1.shape, sep ='\\n')\n","print(\"\\nnumber of dimensions:\", t1.dim(), sep = \"\\n\")\n","print(\"\\nData Type:\", t1.dtype, sep = \"\\n\")\n"]},{"cell_type":"markdown","metadata":{"id":"mH1bd2kaz5ZO"},"source":["<img src = \"https://drive.google.com/uc?export=view&id=189RzBY0oYuih-dZjNAT79FIVf3ZUp_HY\" width =600 >"]},{"cell_type":"markdown","metadata":{"id":"vCIviyFV2m04"},"source":[" # <font color = 'pickle'> **Python list**"]},{"cell_type":"markdown","metadata":{"id":"3486c624"},"source":["\n","<details>\n","<summary>Click to expand code</summary>\n","\n","```python\n","scalar = 4\n","type(scalar)\n"]},{"cell_type":"markdown","metadata":{"id":"03df73e9"},"source":["\n","<details>\n","<summary>Click to expand code</summary>\n","\n","```python\n","my_list = [[1., 2], [3,4]]\n","type(my_list)\n"]},{"cell_type":"markdown","metadata":{"id":"0abd3f84"},"source":["\n","<details>\n","<summary>Click to expand code</summary>\n","\n","```python\n","my_tensor = torch.tensor([[1., 2], [3,4]])\n","type(my_tensor)\n","print(t)\n","print(\"\\nData Typr:\", my_tensor.dtype, sep = \"\\n\")\n"]},{"cell_type":"markdown","metadata":{"id":"yfg-TJpb2rmg"},"source":["# <font color = 'pickle'>**Difference between list and Array/tensor**</font>\n","\n","| <font size =5> Python List                       | <font size =5>Tensor/Array                     |\n","|-----------------------------------|----------------------------------|\n","| <font size =5>Mixed types allowed               | <font size =5>Same type required               |\n","|<font size =5> Elements can be added or removed  | <font size =5>Elements cannot be added or removed               \n","| <font size =5>Basic Python operations           | <font size =5>Supports mathematical operations                \n","|<font size =5>Numerical Computtaions are slow    |<font size =5>Numerical Computtaions are fast\n"]},{"cell_type":"markdown","metadata":{"id":"XfoOIXQK6Bm-"},"source":["\n"]},{"cell_type":"markdown","metadata":{"id":"Y6zZfHhsu92j"},"source":["# <font color = 'pickle'>**Conversion to other Python Objects**"]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6fd95613","executionInfo":{"status":"ok","timestamp":1723262815853,"user_tz":300,"elapsed":170,"user":{"displayName":"Akshat Patil","userId":"09657732965291711317"}},"outputId":"87ba05d4-5de0-45b0-bfe1-42911479c20a"},"outputs":[{"output_type":"stream","name":"stdout","text":["<class 'numpy.ndarray'> <class 'torch.Tensor'>\n"]}],"source":["# Initializing a tensor\n","t=torch.arange(10.0)\n","# Converting tensor t to numpy array using numpy() mehod\n","arr = t.numpy()\n","# Converting numpy array to tensor T using tensor() method\n","T = torch.from_numpy(arr)\n","# Printing data type of arr and T\n","print(type(arr),type(T))"]},{"cell_type":"markdown","metadata":{"id":"c4ae0823"},"source":["\n","<details>\n","<summary>Click to expand code</summary>\n","\n","```python\n","# Initializing a tensor\n","t = torch.arange(10)\n","\n","# Converting tensor t to numpy array using numpy() mehod\n","arr = t.numpy()\n","\n","# Converting numpy array to tensor T using tensor() method\n","T = torch.tensor(arr)\n","\n","# Printing data type of arr and T\n","print(type(arr), type(T), T.type(), sep='\\n')\n"]},{"cell_type":"markdown","metadata":{"id":"K8ot3be10F1e"},"source":["We can also use torch.from_numpy() and torch.as_tensor() to convert numpy array to PyTorch Tensor. However, with these methods, the PyTorch tensor and the source NumPy array share the same memory. This means that changes to one affect the other. However, the torch.tensor() function always makes a copy."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4cac4a60"},"outputs":[],"source":["# change numpy array"]},{"cell_type":"markdown","metadata":{"id":"7c9d9013"},"source":["\n","<details>\n","<summary>Click to expand code</summary>\n","\n","```python\n","my_ndarray = np.arange(10)\n","t_from_numpy = torch.from_numpy(my_ndarray)\n","t_as_tensor = torch.as_tensor(my_ndarray)\n","t_Tensor = torch.tensor(my_ndarray)\n","\n","print(f\"tensor craeted using torch.from_numpy before changing np array: {t_from_numpy}\")\n","print(f\"tensor craeted using torch.as_tensor before changing np array : {t_as_tensor}\")\n","print(f\"tensor craeted using torch.tensor before changing np array    : {t_Tensor}\")\n","\n","# change numpy array\n","my_ndarray[2] = 1000\n","\n","print()\n","print(f\"tensor craeted using torch.from_numpy after changing np array: {t_from_numpy}\")\n","print(f\"tensor craeted using torch.as_tensor after changing np array : {t_as_tensor}\")\n","print(f\"tensor craeted using torch.tensor after changing np array    : {t_Tensor}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ae706d53"},"outputs":[],"source":["# Initializing a size-1 tensor\n","# Printing tensor\n","# Accessing element of tensor using item function\n","# item returns the value of the tensor as python number\n","# works only for tensors with single element"]},{"cell_type":"markdown","metadata":{"id":"a0d64bce"},"source":["\n","<details>\n","<summary>Click to expand code</summary>\n","\n","```python\n","# Initializing a size-1 tensor\n","t = torch.tensor([10.5])\n","\n","# Printing tensor\n","print(t)\n","\n","# Accessing element of tensor using item function\n","# item returns the value of the tensor as python number\n","# works only for tensors with single element\n","\n","print(t.item())\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"577081f3"},"outputs":[],"source":["# we can also convert the tensor to python list"]},{"cell_type":"markdown","metadata":{"id":"37a65767"},"source":["\n","<details>\n","<summary>Click to expand code</summary>\n","\n","```python\n","# we can also convert the tensor to python list\n","t = torch.tensor([10, 2])\n","print(t)\n","print(t.tolist())\n"]},{"cell_type":"markdown","metadata":{"id":"5geGEsHIJZCb"},"source":["# <font color = 'pickle'>**Changing Shape of Tensors**"]},{"cell_type":"markdown","metadata":{"id":"9b190bdb"},"source":["\n","<details>\n","<summary>Click to expand code</summary>\n","\n","```python\n","t = torch.arange(10)\n","print(t)\n","print(\"\\nsize:\",t.shape, sep ='\\n')\n"]},{"cell_type":"markdown","metadata":{"id":"f5c08cb9"},"source":["\n","<details>\n","<summary>Click to expand code</summary>\n","\n","```python\n","t = t.view(5,2)\n","print(t)\n","print(\"\\nsize:\",t.shape, sep ='\\n')\n"]},{"cell_type":"markdown","metadata":{"id":"ba59d2b8"},"source":["\n","<details>\n","<summary>Click to expand code</summary>\n","\n","```python\n","t = t.view(-1,5) # the size -1 is inferred from other dimensions\n","print(t)\n","print(\"\\nsize:\",t.shape, sep ='\\n')\n"]},{"cell_type":"markdown","metadata":{"id":"pmZk-J0TzwE0"},"source":["# <font color = 'pickle'>**Changing datatype of Tensors**\n","When creating tensor we can pass the dtype as an argument. We can also change the datatype of tensors using to() and type() mehods. For a list of dtypes visit https://pytorch.org/docs/stable/tensor_attributes.html#torch.torch.dtype"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"b7ab7501"},"outputs":[],"source":["# we can use type() method or to() method to change the datatype\n","# change the datatype to int64 using type() method\n","# change the datatype to int32 using t0() method"]},{"cell_type":"markdown","metadata":{"id":"7e58400d"},"source":["\n","<details>\n","<summary>Click to expand code</summary>\n","\n","```python\n","x = torch.tensor([8, 9, -3], dtype=torch.int)\n","\n","# we can use type() method or to() method to change the datatype\n","print(f\"Old: {x.dtype}\")\n","\n","# change the datatype to int64 using type() method\n","x = x.type(dtype=torch.int64)\n","print(f\"New: {x.dtype}\")\n","\n","# change the datatype to int32 using t0() method\n","x = x.to(dtype=torch.int32)\n","print(f\"Newer: {x.dtype}\")\n"]},{"cell_type":"markdown","metadata":{"id":"l1ugpAlJ6Lks"},"source":["# <font color = 'pickle'>**Saving Memory - inplace operations**"]},{"cell_type":"markdown","metadata":{"id":"Yhk6tvq_6THn"},"source":["In-place operation are operations that change the content of a given Tensor without making a copy.\n","\n","Operations that have a `_` suffix are in-place. For example: `.add_()`. Operations like += or *= are also inplace operations.\n","\n","We can also perform in-place opaeration usng the notation `Z[:] = <expression>`.\n","\n","As in-place operations do not make a copy, they can save memory. However, we need to use them carefully. They can be problematic when computing derivatives because of an immediate loss of history. We will learn about derivatives and computation graphs in coming lectures."]},{"cell_type":"markdown","metadata":{"id":"3dc05d12"},"source":["\n","<details>\n","<summary>Click to expand code</summary>\n","\n","```python\n","a = [[], [], []]\n","id(a)\n"]},{"cell_type":"markdown","metadata":{"id":"b50d084c"},"source":["\n","<details>\n","<summary>Click to expand code</summary>\n","\n","```python\n","a = [[]*3]\n","id(a)\n"]},{"cell_type":"markdown","metadata":{"id":"1a9de201"},"source":["\n","<details>\n","<summary>Click to expand code</summary>\n","\n","```python\n","a = torch.tensor(10)\n","print(a)\n","print(id(a))\n","a += 1\n","print(a)\n","print(id(a))\n","a = a + 1\n","print(a)\n","print(id(a))\n"]},{"cell_type":"markdown","metadata":{"id":"946d7cb7"},"source":["\n","<details>\n","<summary>Click to expand code</summary>\n","\n","```python\n","b = torch.tensor(10)\n","print(b)\n","print(id(b))\n","b.add_(1)\n","print(b)\n","print(id(b))\n","b = b.add(1)\n","print(b)\n","print(id(b))\n"]},{"cell_type":"markdown","metadata":{"id":"X-nm1NrfbvIU"},"source":["## <font color = 'pickle'>**1) Checking gpu**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cdabc4b3"},"outputs":[],"source":["# check if gpu is availaible"]},{"cell_type":"markdown","metadata":{"id":"abdf01ef"},"source":["\n","<details>\n","<summary>Click to expand code</summary>\n","\n","```python\n","# check if gpu is availaible\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","print(device)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4671d687"},"outputs":[],"source":["# create a tensor"]},{"cell_type":"markdown","metadata":{"id":"ad3d6b1d"},"source":["\n","<details>\n","<summary>Click to expand code</summary>\n","\n","```python\n","# create a tensor\n","X = torch.tensor([1, 2, 3, 4])\n","X\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"70a90646"},"outputs":[],"source":["# check the device attribute of the tensor"]},{"cell_type":"markdown","metadata":{"id":"1bf8ccf6"},"source":["\n","<details>\n","<summary>Click to expand code</summary>\n","\n","```python\n","# check the device attribute of the tensor\n","X.device\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"11b4c980"},"outputs":[],"source":["# move the tensor to gpu"]},{"cell_type":"markdown","metadata":{"id":"8a5f0d50"},"source":["\n","<details>\n","<summary>Click to expand code</summary>\n","\n","```python\n","# move the tensor to gpu\n","X = X.to(device=0)\n"]},{"cell_type":"markdown","metadata":{"id":"4cff28ce"},"source":["\n","<details>\n","<summary>Click to expand code</summary>\n","\n","```python\n","X.device\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6ecaf8b7"},"outputs":[],"source":["# it is more efficient to create the tensor on gpu directly"]},{"cell_type":"markdown","metadata":{"id":"bdee78c1"},"source":["\n","<details>\n","<summary>Click to expand code</summary>\n","\n","```python\n","# it is more efficient to create the tensor on gpu directly\n","Y = torch.tensor([1, 2, 3], device=device)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"a4ccb127"},"outputs":[],"source":["# check the device attribute of the tensor"]},{"cell_type":"markdown","metadata":{"id":"69e72ef6"},"source":["\n","<details>\n","<summary>Click to expand code</summary>\n","\n","```python\n","# check the device attribute of the tensor\n","Y.device\n"]},{"cell_type":"markdown","metadata":{"id":"mJyoahoFNVtP"},"source":["## <font color = 'pickle'>**2) Memory allocation of in-place operations**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"febf8641"},"outputs":[],"source":["# create tensor\n","# move tensor to gpu\n","# we can use id() function to get memory location of tensor\n","# Waits for everything to finish running\n","# initial memory allocated\n","# inplace operation\n","# since the operation was inplace when we update t1 it will update x as well\n","# totall memory allocated after function call\n","# memory allocated because of function call"]},{"cell_type":"markdown","metadata":{"id":"6a8203be"},"source":["\n","<details>\n","<summary>Click to expand code</summary>\n","\n","```python\n","# create tensor\n","t1 = torch.randn(10000, 10000, device=\"cpu\")\n","\n","# move tensor to gpu\n","t1 = t1.to(device)\n","print(t1.device)\n","\n","# we can use id() function to get memory location of tensor\n","print(f\"initial memory location of tensor t1 is : {id(t1)}\")\n","\n","x = t1\n","print(f\"initial memory location of x is : {id(x)}\")\n","\n","# Waits for everything to finish running\n","torch.cuda.synchronize()\n","\n","# initial memory allocated\n","start_memory = torch.cuda.memory_allocated()\n","\n","# inplace operation\n","t1 += 0.1\n","t1.add_(0.1)\n","# since the operation was inplace when we update t1 it will update x as well\n","print(x == t1)\n","\n","print(f\"final memory location of tensor t1 is: {id(t1)}\")\n","print(f\"final location of x is : {id(x)}\")\n","\n","# totall memory allocated after function call\n","end_memory = torch.cuda.memory_allocated()\n","\n","# memory allocated because of function call\n","memory_allocated = end_memory - start_memory\n","print(memory_allocated / 1024**2)\n"]},{"cell_type":"markdown","metadata":{"id":"Ti7xX-TSLp_3"},"source":["From the above example wecan see that both x and t1 has same memory location. When we ue in-place operation on t1, it also updates x"]},{"cell_type":"markdown","metadata":{"id":"yyQSz0iiNsgw"},"source":["## <font color = 'pickle'>**3) Memory allocation of out-of-place operations**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"e9e267cb"},"outputs":[],"source":["# create tensor\n","# move tensor to gpu\n","# we can use id() function to get memory location of tensor\n","# Waits for everything to finish running\n","# initial memory allocated\n","# out-place opertaions\n","# since the operation was not inplace when we update t2 it will not update y\n","# we can use id() function to get memory location of tensor\n","# totall memory allocated after function call\n","# memory allocated because of function call"]},{"cell_type":"markdown","metadata":{"id":"2eb4635e"},"source":["\n","<details>\n","<summary>Click to expand code</summary>\n","\n","```python\n","# create tensor\n","t2 = torch.randn(10000, 10000, device=\"cpu\")\n","\n","# move tensor to gpu\n","t2 = t2.to(device)\n","print(t2.device)\n","\n","# we can use id() function to get memory location of tensor\n","print(f\"initial memory location of tensor t2 {id(t2)}\")\n","\n","y = t2\n","print(f\"final memory location of y is : {id(y)}\")\n","\n","# Waits for everything to finish running\n","torch.cuda.synchronize()\n","\n","# initial memory allocated\n","start_memory = torch.cuda.memory_allocated()\n","\n","# out-place opertaions\n","t2 = t2 + 0.1\n","\n","# since the operation was not inplace when we update t2 it will not update y\n","print(y == t2)\n","\n","# we can use id() function to get memory location of tensor\n","print(f\"final memory location of tensor t2 {id(t2)}\")\n","print(f\"final memory location of y is : {id(y)}\")\n","\n","# totall memory allocated after function call\n","end_memory = torch.cuda.memory_allocated()\n","\n","# memory allocated because of function call\n","memory_allocated = end_memory - start_memory\n","print(memory_allocated / 1024**2)\n"]},{"cell_type":"markdown","metadata":{"id":"9kli0fOuMGCM"},"source":["From the above example we can see that initially both y and t2 has same memory location. After running t2 = t2 + 0.1, we will find that id(t2) points to a different location. That is because Python first evaluates t2 + 0.1, allocating new memory for the result and then makes t2 point to this new location in memory. Since we have not done in-place operation, updating t2 does not effect y. y still points to the same memory location."]},{"cell_type":"markdown","metadata":{"id":"7aJjhm1lMQ3m"},"source":["# <font color = 'pickle'>**Linear Algebra**"]},{"cell_type":"markdown","metadata":{"id":"E5a8iDgbMajD"},"source":["## <font color = 'pickle'>**Dot product**\n","Dot product of 2 vectors x and y  is given by the summation of product of elements at the same position.\n","\n","If we have 2 vectors x: [1, 2, 3, 4] and y: [1, 1, 2, 1]\n","\n","(x.y) will be 1x1 + 2x1 + 3x2 + 4x1 = 13"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"c6b3243d"},"outputs":[],"source":["# Initializing 2 tensors\n","# Performing Dot product"]},{"cell_type":"markdown","metadata":{"id":"a3284cb1"},"source":["\n","<details>\n","<summary>Click to expand code</summary>\n","\n","```python\n","# Initializing 2 tensors\n","x = torch.Tensor([0, -1, 1, 0])\n","y = torch.Tensor([0, 1, 1, 0])\n","\n","# Performing Dot product\n","torch.dot(x, y)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ee894d72"},"outputs":[],"source":["# Dot Product is equal to sum of products at the same position, thus the expression below will give similar result"]},{"cell_type":"markdown","metadata":{"id":"2593e9c4"},"source":["\n","<details>\n","<summary>Click to expand code</summary>\n","\n","```python\n","# Dot Product is equal to sum of products at the same position, thus the expression below will give similar result\n","torch.sum(x * y)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1315e858"},"outputs":[],"source":["# Initializing 2 tensors\n","# Performing Dot product"]},{"cell_type":"markdown","metadata":{"id":"e3158f46"},"source":["\n","<details>\n","<summary>Click to expand code</summary>\n","\n","```python\n","# Initializing 2 tensors\n","x = torch.Tensor([1, 0, 0, 1])\n","y = torch.Tensor([1, 0, 0, 1])\n","\n","# Performing Dot product\n","torch.dot(x, y)\n"]},{"cell_type":"markdown","metadata":{"id":"BS-6BCNSOPLx"},"source":["## <font color = 'pickle'>**Dot product vs. for Loop in Python**"]},{"cell_type":"markdown","metadata":{"id":"0e023d22"},"source":["\n","<details>\n","<summary>Click to expand code</summary>\n","\n","```python\n","import time\n","n = 1000000\n","a = torch.arange(n)\n","b = torch.arange(n)\n","\n","def pytorch_dot(x, y):\n","    return x.dot(y)\n"]},{"cell_type":"markdown","metadata":{"id":"1eac3944"},"source":["\n","<details>\n","<summary>Click to expand code</summary>\n","\n","```python\n","%timeit pytorch_dot(a,b)\n"]},{"cell_type":"markdown","metadata":{"id":"9a0923df"},"source":["\n","<details>\n","<summary>Click to expand code</summary>\n","\n","```python\n","x = [1,2]\n","y = [3,4]\n","list(zip(x,y))\n"]},{"cell_type":"markdown","metadata":{"id":"8b1c51ee"},"source":["\n","<details>\n","<summary>Click to expand code</summary>\n","\n","```python\n","a1 = a.tolist()\n","b1 = b.tolist()\n","def plain_python(x, y):\n","    output = 0\n","    for x_j, y_j in zip(x, y):\n","        output += x_j * y_j\n","    return output\n","\n"]},{"cell_type":"markdown","metadata":{"id":"9ce74be3"},"source":["\n","<details>\n","<summary>Click to expand code</summary>\n","\n","```python\n","%timeit plain_python(a,b)\n"]},{"cell_type":"markdown","metadata":{"id":"Rk8Kt4uJSBOp"},"source":["**Output 1:**\n","\n","921 µs ± 182 µs per loop: This tells you that the code took an average of 921 microseconds (µs) to run for each loop, with a standard deviation of 182 microseconds. The standard deviation gives an indication of the variability in the timing across different runs, which can be affected by other processes running on the computer at the same time.\n","\n","(mean ± std. dev. of 7 runs, 1000 loops each): This part provides details about how the timing was measured. The code was run 7 times, and each of those runs consisted of 1000 loops. The mean and standard deviation were calculated from these 7 runs.\n","\n","**Output 2:**\n","6.78 s ± 392 ms per loop: This tells you that the code took an average of 6.78 seconds to run for each loop, with a standard deviation of 392 milliseconds. Since 1 second equals 1000 milliseconds, this standard deviation is less than half a second.\n","\n","(mean ± std. dev. of 7 runs, 1 loop each): Similar to Output 1, this part tells you that the code was run 7 times, and each of those runs consisted of just 1 loop. The mean and standard deviation were calculated from these 7 runs.\n","\n","**In comparison, Output 1 suggests a much faster execution time (in the order of microseconds) compared to Output 2 (in the order of seconds).**\n","\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"bUzXoOeG_rfX"},"source":["## <font color = 'pickle'>**Operations on Metrices**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fb84c871"},"outputs":[],"source":["# Creating 2 matrices\n","# First matrix\n","# Second matrix : copy of A"]},{"cell_type":"markdown","metadata":{"id":"d3f2a2e4"},"source":["\n","<details>\n","<summary>Click to expand code</summary>\n","\n","```python\n","# Creating 2 matrices\n","\n","# First matrix\n","A = torch.arange(0, 25).reshape(5, 5)\n","\n","# Second matrix : copy of A\n","B = A.clone()\n","\n","print(A)\n","print(B)\n"]},{"cell_type":"markdown","metadata":{"id":"D7ENj3dtV9yR"},"source":["### <font color = 'pickle'>**Addition of 2 matrices**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"c7aab427"},"outputs":[],"source":["# Addition of 2 matrices"]},{"cell_type":"markdown","metadata":{"id":"133cc22b"},"source":["\n","<details>\n","<summary>Click to expand code</summary>\n","\n","```python\n","# Addition of 2 matrices\n","A + B\n"]},{"cell_type":"markdown","metadata":{"id":"nbOc6KIXWD_N"},"source":["### <font color = 'pickle'>**Subtraction of 2 matrices**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"58dd128d"},"outputs":[],"source":["# Subtraction of 2 matrices"]},{"cell_type":"markdown","metadata":{"id":"e661d540"},"source":["\n","<details>\n","<summary>Click to expand code</summary>\n","\n","```python\n","# Subtraction of 2 matrices\n","A - B\n"]},{"cell_type":"markdown","metadata":{"id":"dogJlUvrWYhx"},"source":["### <font color = 'pickle'>**Multiplying Matrices with Scalars**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ff9e30ad"},"outputs":[],"source":["# Each element of matrix can be aded or multiplied by a scalar (broadcasting)\n","# This operation will not change the shape of a matrix or a Tensor"]},{"cell_type":"markdown","metadata":{"id":"925c2fff"},"source":["\n","<details>\n","<summary>Click to expand code</summary>\n","\n","```python\n","# Each element of matrix can be aded or multiplied by a scalar (broadcasting)\n","# This operation will not change the shape of a matrix or a Tensor\n","a = 2\n","print(a + A)\n","print()\n","print(a * A)\n"]},{"cell_type":"markdown","metadata":{"id":"l_iPeyTzWlXA"},"source":["### <font color = 'pickle'>**Transpose of a Matrix**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7258e72f"},"outputs":[],"source":["# Transpose of a matrix : Elements of the rows and columns get interchanged a[i][j] becomes a[j][i]\n","# Transpose is a special case of permute"]},{"cell_type":"markdown","metadata":{"id":"c420d597"},"source":["\n","<details>\n","<summary>Click to expand code</summary>\n","\n","```python\n","# Transpose of a matrix : Elements of the rows and columns get interchanged a[i][j] becomes a[j][i]\n","# Transpose is a special case of permute\n","A.T\n"]},{"cell_type":"markdown","metadata":{"id":"tsAkl0FDWQ-T"},"source":["### <font color = 'pickle'>**Hadamard product**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"350fb7c5"},"outputs":[],"source":["# Elementwise multiplication of two metrices is called Hadamard product"]},{"cell_type":"markdown","metadata":{"id":"07063c0c"},"source":["\n","<details>\n","<summary>Click to expand code</summary>\n","\n","```python\n","# Elementwise multiplication of two metrices is called Hadamard product\n","A * B\n"]},{"cell_type":"markdown","metadata":{"id":"Ys0KKwAFTHpk"},"source":["### <font color = 'pickle'>**Matrix Multiplication**\n","\n","Matrix multiplication is a binary operation on 2 matrices which gives us a matrix which is the product of the 2 matrices.\n","\n","If we are given 2 matrices $A$ of shape $(m * n)$ and $B$ of shape $(q * p)$, **we can perform matrix multiplication only when $n = q$** and the resultant product matrix will have shape $(m * p)$.\n","\n","Suppose we are given 2 matrices $A (m * n)$ and $B (n * p)$:\n","\n","$$\\mathbf{A}=\\begin{bmatrix}\n"," a_{11} & a_{12} & \\cdots & a_{1n} \\\\\n"," a_{21} & a_{22} & \\cdots & a_{2n} \\\\\n","\\vdots & \\vdots & \\ddots & \\vdots \\\\\n"," a_{m1} & a_{m2} & \\cdots & a_{mn} \\\\\n","\\end{bmatrix},\\quad\n","\\mathbf{B}=\\begin{bmatrix}\n"," b_{11} & b_{12} & \\cdots & b_{1p} \\\\\n"," b_{21} & b_{22} & \\cdots & b_{2p} \\\\\n","\\vdots & \\vdots & \\ddots & \\vdots \\\\\n"," b_{n1} & b_{n2} & \\cdots & b_{np} \\\\\n","\\end{bmatrix}$$\n","\n","Then after performing matrix multiplication, the resultant matrix C = AB will be:\n","\n","$$\\mathbf{C}=\\begin{bmatrix}\n"," c_{11} & c_{12} & \\cdots & c_{1p} \\\\\n"," c_{21} & c_{22} & \\cdots & c_{2p} \\\\\n","\\vdots & \\vdots & \\ddots & \\vdots \\\\\n"," c_{mp} & c_{mp} & \\cdots & c_{mp} \\\\\n","\\end{bmatrix}$$\n","\n","Here, $c_{ij} = a_{i1}b_{1j} + a_{i2}b_{2j} + ... a_{in}b_{b_nj} = \\sum_{k = 1}^n a_{ik}b_{kj}$\n","\n","for, $i = 1,....m$ and $j = 1,...p$\n","\n","\n","Thus, each element of C, $c_{ij}$ is obtained by dot product of $i^{th}$ row of $A$ and $j^{th}$ column of $B$.\n","\n","**Example** :\n","  1. Let $A$ be a matrix of (4, 3) dimensions.\n","  2. Let $B$ be another matrix of (3, 2) dimensions.\n","  3. Let us denote denote the matrix multiplication of $A$ and $B$ with $C$.\n","  5. Then the dimension of $C$ = (number of rows of $A$,number of columns of $B$)\n","     \n","    dimension of  $C$ = (4, 2)\n","\n","The figure given below will give a good example of matrix multplication :\n","\n","<img src = \"https://drive.google.com/uc?view=export&id=176DF50XdtwkqU5wvxtWuD75sRDHvSJBf\" width =\"250\"/>\n","\n","We can perform matrix multiplication in the following way using PyTorch:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6bfc7746"},"outputs":[],"source":["# Initializing 2 matrices\n","# Matrix-Matrix Multiplication using mm function of PyTorch"]},{"cell_type":"markdown","metadata":{"id":"0ea8d057"},"source":["\n","<details>\n","<summary>Click to expand code</summary>\n","\n","```python\n","# Initializing 2 matrices\n","A = torch.arange(0, 10, dtype=float).reshape(2, 5)\n","B = torch.ones(5, 2, dtype=float)\n","\n","# Matrix-Matrix Multiplication using mm function of PyTorch\n","torch.mm(A, B)\n"]},{"cell_type":"markdown","metadata":{"id":"d987a905"},"source":["\n","<details>\n","<summary>Click to expand code</summary>\n","\n","```python\n","A @ B\n"]},{"cell_type":"markdown","metadata":{"id":"2f25c1da"},"source":["\n","<details>\n","<summary>Click to expand code</summary>\n","\n","```python\n","A * B\n"]},{"cell_type":"markdown","metadata":{"id":"cI4L4lCPbCar"},"source":["#### <font color = 'pickle'>**Prediction on Multiple Training Examples via Matrix Multiplication**"]},{"cell_type":"markdown","metadata":{"id":"53bc9f1f"},"source":["\n","<details>\n","<summary>Click to expand code</summary>\n","\n","```python\n","bias = torch.tensor([0.])\n","theta = torch.tensor([0.2, 9.3])\n","theta = theta.view(-1,1)\n","X = torch.tensor(\n","   [[1.8, 9.2],\n","    [0.2, 3.3],\n","    [5.2, 3.4],\n","    [3.4, 4.5],\n","    [6.1, 7.1]]\n",")\n","print(X.shape, theta.shape, bias.shape)\n","print( bias,theta, X, sep='\\n')\n"]},{"cell_type":"markdown","metadata":{"id":"ccf35778"},"source":["\n","<details>\n","<summary>Click to expand code</summary>\n","\n","```python\n","predictions = X.matmul(theta) + bias\n","predictions\n"]},{"cell_type":"markdown","metadata":{"id":"EUh62jYWUZvj"},"source":["# <font color = 'pickle'>**Self Study**"]},{"cell_type":"markdown","metadata":{"id":"khxj8DIV5Eic"},"source":["## <font color = 'pickle'>**Broadcasting - Operations on tensors of different  size**\n","\n","Broadcasting describes how a tensor has to be treated during arithematic operation. If we have tensors of different sizes, we can broadcast the smaller array across the larger one so that they can have comaptible shapes.\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"u9J4-PQRPRtk"},"source":["### <font color = 'pickle'>**Broadcasting Examples**</font>"]},{"cell_type":"markdown","metadata":{"id":"1ZxySDAIPtMW"},"source":["#### <font color = 'pickle'>**Broadcasting with a scalar**</font>"]},{"cell_type":"markdown","metadata":{"id":"38511cc6"},"source":["\n","<details>\n","<summary>Click to expand code</summary>\n","\n","```python\n","t= torch.tensor([1,-2, 4])\n"]},{"cell_type":"markdown","metadata":{"id":"88fbce88"},"source":["\n","<details>\n","<summary>Click to expand code</summary>\n","\n","```python\n","t > 0\n"]},{"cell_type":"markdown","metadata":{"id":"22c85ad9"},"source":["\n","<details>\n","<summary>Click to expand code</summary>\n","\n","```python\n","t2 = torch.tensor([[12, 16, 14], [13, 17, 13], [14, 18, 12]])\n","t2 * 2\n"]},{"cell_type":"markdown","metadata":{"id":"TCNMwObZQEDR"},"source":["#### <font color = 'pickle'>**Broadcasting a vector to matrix**</font>"]},{"cell_type":"markdown","metadata":{"id":"4e818f51"},"source":["\n","<details>\n","<summary>Click to expand code</summary>\n","\n","```python\n","t2 = torch.tensor([[12, 16, 14], [13, 17, 13], [14, 18, 12]])\n","t1 = torch.tensor([1, 2, 3])\n","print(t2.shape)\n","print(t1.shape)\n","print(t1 + t2)\n"]},{"cell_type":"markdown","metadata":{"id":"H9Vet8dAPu5O"},"source":["### <font color = 'pickle'>**1) Understanding how broadcasting works**\n","\n","* The following image describes how a tensor of 2 dimensional tensor will be added to a 1 dimensional tensor\n","<img src=\"https://drive.google.com/uc?export=view&id=1QG2GO1owGpyXbcugJFVFGb4o_buV4s3j\" width=\"500\"/>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"c2b35772"},"outputs":[],"source":["# create tensor"]},{"cell_type":"markdown","metadata":{"id":"d3e3bb12"},"source":["\n","<details>\n","<summary>Click to expand code</summary>\n","\n","```python\n","# create tensor\n","t2 = torch.tensor([[12, 16, 14], [13, 17, 13], [14, 18, 12]])\n","t1 = torch.tensor([1, 2, 3])\n","print(t2.shape)\n","print(t1.shape)\n"]},{"cell_type":"markdown","metadata":{"id":"cd38f1d6"},"source":["\n","<details>\n","<summary>Click to expand code</summary>\n","\n","```python\n","print(t1.storage())\n"]},{"cell_type":"markdown","metadata":{"id":"3b4685e4"},"source":["\n","<details>\n","<summary>Click to expand code</summary>\n","\n","```python\n","t1_mod = t1.expand_as(t2)\n","t1_mod.shape\n"]},{"cell_type":"markdown","metadata":{"id":"PchW_tUmMdIw"},"source":["Although it appears as though we are copying the rows, we are actually not duplicating them."]},{"cell_type":"markdown","metadata":{"id":"a8062362"},"source":["\n","<details>\n","<summary>Click to expand code</summary>\n","\n","```python\n","print(t1_mod.storage())\n"]},{"cell_type":"markdown","metadata":{"id":"544fdcea"},"source":["\n","<details>\n","<summary>Click to expand code</summary>\n","\n","```python\n","print(t1_mod + t2)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"70e0aecb"},"outputs":[],"source":["# we can check that it gives us the same result if we simply add t1 and t2\n","# so broadcasting is an efficient way of performing operations on tensors of unequal sizes"]},{"cell_type":"markdown","metadata":{"id":"a2b7dfbb"},"source":["\n","<details>\n","<summary>Click to expand code</summary>\n","\n","```python\n","# we can check that it gives us the same result if we simply add t1 and t2\n","# so broadcasting is an efficient way of performing operations on tensors of unequal sizes\n","print(t1 + t2)\n"]},{"cell_type":"markdown","metadata":{"id":"zyhE_dEUQwLk"},"source":["### <font color = 'pickle'>**2) Rules for Broadcasting**</font>\n","Broadcasting can only happen if the two tensors are broadcastable. Conditions for broadcasting:\n","\n","- Each tensor has at least one dimension.\n","\n","- When iterating over the dimension sizes, starting at the trailing dimension, the dimension sizes must either be equal, one of them is 1, or one of them does not exist.\n","\n","Examples:\n","\n","1. `t1(5, 8, 10) t2(5, 8, 10)`\n","Same size -> Broadcasting possible.\n","2. `t1((0,)) t2(5, 8, 10)`\n","t1 doesn't have atleast one dimension -> Broadcasting not possible.\n","3. `t1(5, 8, 10, 1) t2(8, 1, 1)` Broadcasting possible. Reasons:\n","  - 1st trailing position : both have size 1\n","  - 2nd trailing position : t2 has size 1\n","  - 3rd trailing position : both have size 8\n","  - 4th training position: t2 size doesn't exist but t2 has atleast 1 dimension."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"c86e28bc"},"outputs":[],"source":["# Broadcasting"]},{"cell_type":"markdown","metadata":{"id":"3cef36ca"},"source":["\n","<details>\n","<summary>Click to expand code</summary>\n","\n","```python\n","# Broadcasting\n","t1 = torch.empty(5, 8, 10, 1)\n","t2 = torch.empty(  8, 1, 1,)\n","(t1 + t2).size()\n"]},{"cell_type":"markdown","metadata":{"id":"kjZwjy9d8wbD"},"source":["The dimensions after broadcasting will be:\n","\n","- If the number of dimensions are\n"," not equal, prepend 1 to the dimensions of the tensor with fewer dimensions to make them equal length.\n","\n","- Then, for each dimension size, the resulting dimension size is the max of the sizes along that dimension."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"51b1eead"},"outputs":[],"source":["# Another example for broadcasting"]},{"cell_type":"markdown","metadata":{"id":"909d4b16"},"source":["\n","<details>\n","<summary>Click to expand code</summary>\n","\n","```python\n","# Another example for broadcasting\n","t1 = torch.empty(1)\n","t2 = torch.empty(3, 1, 7)\n","(t1 + t2).size()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bb1f5aac"},"outputs":[],"source":["# Example where broadcasting is not possible"]},{"cell_type":"markdown","metadata":{"id":"e397be24"},"source":["\n","<details>\n","<summary>Click to expand code</summary>\n","\n","```python\n","# Example where broadcasting is not possible\n","t1 = torch.empty(5, 8, 10, 1)\n","t2 = torch.empty( 3, 1, 1)\n","(t1 + t2).size()\n"]},{"cell_type":"markdown","metadata":{"id":"wru9LKgg9f5G"},"source":["Here, at third trailing position sizes are not equal and none of them is 1, thus broadcasting is not possible."]},{"cell_type":"markdown","metadata":{"id":"UeOLV3qj7DTS"},"source":["## <font color = 'pickle'>**Reduction**\n","\n","We can calculate the sum of all elemnets of a vector or a matrix of any shape. This can be done using the ***sum*** function.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"669071fc"},"outputs":[],"source":["# Creating a vector\n","# This will do summation of all the elements of the vector : 0 + 1 + 2 + 3 + 4 = 10"]},{"cell_type":"markdown","metadata":{"id":"da3156cb"},"source":["\n","<details>\n","<summary>Click to expand code</summary>\n","\n","```python\n","# Creating a vector\n","x = torch.arange(5)\n","print(x)\n","\n","# This will do summation of all the elements of the vector : 0 + 1 + 2 + 3 + 4 = 10\n","print(x.sum())\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"03b89d80"},"outputs":[],"source":["# Creating a matrix\n","# This will do summation of all the elements of the matrix\n","# This will takle the mean of all teh elements"]},{"cell_type":"markdown","metadata":{"id":"08afb14a"},"source":["\n","<details>\n","<summary>Click to expand code</summary>\n","\n","```python\n","# Creating a matrix\n","X = torch.arange(0, 10).reshape(2, 5)\n","X = X.to(torch.float32)\n","print(X)\n","\n","# This will do summation of all the elements of the matrix\n","print(X.sum())\n","# This will takle the mean of all teh elements\n","print(X.mean())\n"]},{"cell_type":"markdown","metadata":{"id":"IRQX5pa6-r6V"},"source":["We can also calculate the mean or average of all elements in a vector or a matrix by dividing the sum of elements by no. of elements."]},{"cell_type":"markdown","metadata":{"id":"k6Ti1nNrBP4f"},"source":["By default, invoking the sum/mean finction on a tensor will give us a scaler (reduces the tensor along all its axes)\n","\n","We can also calculate sum, along the rows or columns by specifying the value of parameter \"axis\".\n","\n","axis = 0 will calculate sum along the rows while axis = 1 will calculate sum along the columns.\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4485c230"},"outputs":[],"source":["# Creating a matrix A"]},{"cell_type":"markdown","metadata":{"id":"10e36093"},"source":["\n","<details>\n","<summary>Click to expand code</summary>\n","\n","```python\n","# Creating a matrix A\n","A = torch.arange(0, 15, dtype = float).reshape(5, 3)\n","A\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"275701da"},"outputs":[],"source":["# Sum of elements along axis = 0\n","# row sum for each column\n","# Since we are taking sum along axis = 0, the input tensor reduces along axis 0\n","# The shape of A was ([5,3])\n","# After invoking sum along axis = 0, the shape reduces to ([3])"]},{"cell_type":"markdown","metadata":{"id":"2e079a97"},"source":["\n","<details>\n","<summary>Click to expand code</summary>\n","\n","```python\n","# Sum of elements along axis = 0\n","# row sum for each column\n","# Since we are taking sum along axis = 0, the input tensor reduces along axis 0\n","# The shape of A was ([5,3])\n","# After invoking sum along axis = 0, the shape reduces to ([3])\n","print(f'Shape before rediction{A.shape}')\n","A.sum(axis = 0), A.sum(axis=0).shape\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f3800785"},"outputs":[],"source":["# Sum of elements along axis =  1\n","# column sum for each row\n","# Since we are taking sum along axis = 1, the input tensor reduces along axis 1\n","# The shape of A was ([5,3])\n","# After invoking sum along axis = 1, the shape redices to ([5])"]},{"cell_type":"markdown","metadata":{"id":"09f5ad15"},"source":["\n","<details>\n","<summary>Click to expand code</summary>\n","\n","```python\n","# Sum of elements along axis =  1\n","# column sum for each row\n","# Since we are taking sum along axis = 1, the input tensor reduces along axis 1\n","# The shape of A was ([5,3])\n","# After invoking sum along axis = 1, the shape redices to ([5])\n","A.sum(axis = 1), A.sum(axis = 1).shape\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"091fd2b7"},"outputs":[],"source":["# Rules of broadcasting:  A and A.sum(axis=1) are not broadcastable"]},{"cell_type":"markdown","metadata":{"id":"c86847fb"},"source":["\n","<details>\n","<summary>Click to expand code</summary>\n","\n","```python\n","# Rules of broadcasting:  A and A.sum(axis=1) are not broadcastable\n","print(A/A.sum(axis=1))\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"Pt8xjJ8w8sba"},"source":["## <font color = 'pickle'>**Non-Reduction Sum**"]},{"cell_type":"markdown","metadata":{"id":"YEp87iCYDX3h"},"source":["As seem in above examples, invoking sum() or mean() will reduce number of dimensions. We can keep number of axis unchanged by passing argument keepdims = True."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"019768f1"},"outputs":[],"source":["# When we pass argument keepdim=True, the shape will now be ([5,1]. The output has 2-dimensions\n","# if we do not pass the argument keepdim=True, the shape will be ([5]). The output has one-dimension"]},{"cell_type":"markdown","metadata":{"id":"d9ca67f5"},"source":["\n","<details>\n","<summary>Click to expand code</summary>\n","\n","```python\n","# When we pass argument keepdim=True, the shape will now be ([5,1]. The output has 2-dimensions\n","# if we do not pass the argument keepdim=True, the shape will be ([5]). The output has one-dimension\n","sum_A_0 = A.sum(axis=1, keepdim=True)\n","print(sum_A_0.shape)\n","print(sum_A_0)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"90bf6326"},"outputs":[],"source":["# Let us now try operation : A/(sum(A, axis=0))"]},{"cell_type":"markdown","metadata":{"id":"ba855c4a"},"source":["\n","<details>\n","<summary>Click to expand code</summary>\n","\n","```python\n","# Let us now try operation : A/(sum(A, axis=0))\n","print(A/A.sum(axis=1, keepdim=True))\n"]},{"cell_type":"markdown","metadata":{"id":"4a16448e"},"source":["\n","<details>\n","<summary>Click to expand code</summary>\n","\n","```python\n","A\n"]},{"cell_type":"markdown","metadata":{"id":"34863ecf"},"source":["\n","<details>\n","<summary>Click to expand code</summary>\n","\n","```python\n","A = torch.randint(0, 10 , (2,3,4))\n"]},{"cell_type":"markdown","metadata":{"id":"b9f3a4bf"},"source":["\n","<details>\n","<summary>Click to expand code</summary>\n","\n","```python\n","  A\n"]},{"cell_type":"markdown","metadata":{"id":"6a92b14e"},"source":["\n","<details>\n","<summary>Click to expand code</summary>\n","\n","```python\n","A.shape\n"]},{"cell_type":"markdown","metadata":{"id":"e3fdce96"},"source":["\n","<details>\n","<summary>Click to expand code</summary>\n","\n","```python\n","B = A.sum(axis = 1, keepdim=True)\n"]},{"cell_type":"markdown","metadata":{"id":"758d646e"},"source":["\n","<details>\n","<summary>Click to expand code</summary>\n","\n","```python\n","B.shape\n"]},{"cell_type":"markdown","metadata":{"id":"7d266dcb"},"source":["\n","<details>\n","<summary>Click to expand code</summary>\n","\n","```python\n","B\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ccce7e33"},"outputs":[],"source":["# Cumulative sum of elements along rows"]},{"cell_type":"markdown","metadata":{"id":"b044d9d3"},"source":["\n","<details>\n","<summary>Click to expand code</summary>\n","\n","```python\n","# Cumulative sum of elements along rows\n","A.cumsum(axis = 0)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"29f653e3"},"outputs":[],"source":["# Cumulative sum of elements along columns"]},{"cell_type":"markdown","metadata":{"id":"cf0289a0"},"source":["\n","<details>\n","<summary>Click to expand code</summary>\n","\n","```python\n","# Cumulative sum of elements along columns\n","A.cumsum(axis = 1)\n"]},{"cell_type":"markdown","metadata":{"id":"v_4VmMUwl826"},"source":["## <font color = 'pickle'>**Accessing elements of a Tensor**\n","\n","We can access individual elements of a Tensor using **index values**. Indexing always **starts from 0**.\n","\n","For example if the tensor is: `[10, 12, 31, 34]`\n","\n","Index of 10 is 0, index of 12 is 1 and so on."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"e69a73dc"},"outputs":[],"source":["# Printing all elements\n","# Get the first row"]},{"cell_type":"markdown","metadata":{"id":"4404d54e"},"source":["\n","<details>\n","<summary>Click to expand code</summary>\n","\n","```python\n","t1 = torch.tensor([[1, 2, 5], [7, 8, 9]])\n","\n","# Printing all elements\n","print(t1)\n","\n","# Get the first row\n","print(t1[0])\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6cad8211"},"outputs":[],"source":["# Get the first element of the second row"]},{"cell_type":"markdown","metadata":{"id":"3f265489"},"source":["\n","<details>\n","<summary>Click to expand code</summary>\n","\n","```python\n","# Get the first element of the second row\n","print(t1[1][0])\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f0c17567"},"outputs":[],"source":["# Get the first element of the second row"]},{"cell_type":"markdown","metadata":{"id":"4420142f"},"source":["\n","<details>\n","<summary>Click to expand code</summary>\n","\n","```python\n","# Get the first element of the second row\n","t1[1, 0]\n"]},{"cell_type":"markdown","metadata":{"id":"MDgRsn4wuoVA"},"source":["## <font color = 'pickle'>**Accessing a Sub-Tensor**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8f955147"},"outputs":[],"source":["# Specify [from: to : step)\n","# by default step is 1\n","# here from is inclusive but to is not\n","# Get a subarray [9, 13, 21, 45]\n","# index 2 (i.e 9) is inclusive but index 6 (i.e. 67) is not and step size is 1\n","# Get a subarray [9, 21]"]},{"cell_type":"markdown","metadata":{"id":"998336be"},"source":["\n","<details>\n","<summary>Click to expand code</summary>\n","\n","```python\n","t1 = torch.tensor([1, 5, 9, 13, 21, 45, 67, 34])\n","\n","# Specify [from: to : step)\n","# by default step is 1\n","# here from is inclusive but to is not\n","\n","# Get a subarray [9, 13, 21, 45]\n","# index 2 (i.e 9) is inclusive but index 6 (i.e. 67) is not and step size is 1\n","print(t1[2:6])\n","\n","# Get a subarray [9, 21]\n","print(t1[2:6:2])  # step size is 2\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"319471ee"},"outputs":[],"source":["# subarrays created using slicing and indexing do not create a copy,\n","# modifying the subarray modifies the original tensor as well\n","# modify subarray"]},{"cell_type":"markdown","metadata":{"id":"9b6ff01f"},"source":["\n","<details>\n","<summary>Click to expand code</summary>\n","\n","```python\n","# subarrays created using slicing and indexing do not create a copy,\n","# modifying the subarray modifies the original tensor as well\n","\n","t1 = torch.tensor([1, 5, 9, 13, 21, 45, 67, 34])\n","t2 = t1[2:6]\n","print(\"array and subarray before modifying subarray\")\n","print(t1)\n","print(t2)\n","\n","# modify subarray\n","\n","t2[0] = 100\n","print(\"\\narray and subarray after modifying subarray\")\n","print(t1)\n","print(t2)\n"]},{"cell_type":"markdown","metadata":{"id":"o6iUsJ8uGhWG"},"source":["<font color = 'indianred'> **As we can see above, modifying subarray, modifes the original array as well**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"112176bf"},"outputs":[],"source":["# use clone() method if yu specifically want to create a copy\n","# modify subarray"]},{"cell_type":"markdown","metadata":{"id":"a0dcc68a"},"source":["\n","<details>\n","<summary>Click to expand code</summary>\n","\n","```python\n","# use clone() method if yu specifically want to create a copy\n","\n","t1 = torch.tensor([1, 5, 9, 13, 21, 45, 67, 34])\n","t2 = t1[2:6].clone()\n","print(\"array and subarray before modifying subarray\")\n","print(t1)\n","print(t2)\n","\n","# modify subarray\n","\n","t2[0] = 100\n","print(\"\\narray and subarray after modifying subarray\")\n","print(t1)\n","print(t2)\n"]},{"cell_type":"markdown","metadata":{"id":"1pE-pEt88b5R"},"source":["<font color = 'indianred'> **SInce we created  a copy, modifying subarray, does not modify the original array as well**"]},{"cell_type":"markdown","metadata":{"id":"b4177313"},"source":["\n","<details>\n","<summary>Click to expand code</summary>\n","\n","```python\n","t1 = torch.tensor([[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12], [13, 14, 15, 16]])\n","t1\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"82babe31"},"outputs":[],"source":["# get the sub array [[6,7], [10,11]]"]},{"cell_type":"markdown","metadata":{"id":"1b89cf2f"},"source":["\n","<details>\n","<summary>Click to expand code</summary>\n","\n","```python\n","# get the sub array [[6,7], [10,11]]\n","t1[1:3, 1:3]\n"]},{"cell_type":"markdown","metadata":{"id":"nhLHAqPLix2O"},"source":["## <font color = 'pickle'>**Operations on tensors of same size**\n","We can call element-wise operations on any two tensors of the same shape."]},{"cell_type":"markdown","metadata":{"id":"41113924"},"source":["\n","<details>\n","<summary>Click to expand code</summary>\n","\n","```python\n","x = torch.tensor([1.0, 2, 4, 8])\n","y = torch.tensor([2, 2, 2, 2])\n","x + y, x - y, x * y, x / y, x**y  # The ** operator is exponentiation\n"]},{"cell_type":"markdown","metadata":{"id":"NsU8FU-9s3kl"},"source":["## <font color = 'pickle'>**Changing the shape of tensors**"]},{"cell_type":"markdown","metadata":{"id":"4Dw-lJ1Lu3fT"},"source":["### <font color = 'pickle'>**1) Reshape**\n","\n","If we want to change the shape of our tensor, without affecting the elements present, we can use the ***reshape*** function."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"94d9a5f3"},"outputs":[],"source":["# Initializing a tensor with 10 elements from 0 to 9\n","# Changing the shape of tensor t from 1x10 to 2x5"]},{"cell_type":"markdown","metadata":{"id":"ba30a7cd"},"source":["\n","<details>\n","<summary>Click to expand code</summary>\n","\n","```python\n","# Initializing a tensor with 10 elements from 0 to 9\n","t = torch.arange(10)\n","print(t)\n","\n","# Changing the shape of tensor t from 1x10 to 2x5\n","tr = t.reshape(2, 5)\n","print(tr)\n"]},{"cell_type":"markdown","metadata":{"id":"00BapNUKxlMQ"},"source":["If we have to specify just 1 dimension in reshape function and want the function to calculate the second dimension itself, we can write `-1` in place of second dimension.\n","\n","For 2 rows, we will write `reshape(2,-1)`\n","\n","For 5 columns, we will write `reshape(-1,5)`."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"70d78068"},"outputs":[],"source":["# Changing the shape of tensor t from 1 row to 2 rows"]},{"cell_type":"markdown","metadata":{"id":"2875921e"},"source":["\n","<details>\n","<summary>Click to expand code</summary>\n","\n","```python\n","# Changing the shape of tensor t from 1 row to 2 rows\n","tr1 = t.reshape(2, -1)\n","print(tr1)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1c0dddcb"},"outputs":[],"source":["# Changing the shape of tensor t from 10 columns to 5 columns"]},{"cell_type":"markdown","metadata":{"id":"314e0ec1"},"source":["\n","<details>\n","<summary>Click to expand code</summary>\n","\n","```python\n","# Changing the shape of tensor t from 10 columns to 5 columns\n","tr2 = t.reshape(-1, 5)\n","print(tr2)\n"]},{"cell_type":"markdown","metadata":{"id":"AWCEvFyfu7Xl"},"source":["### <font color = 'pickle'>**2) View**\n","\n","We can allow a tensor to be a view of an existing tensor. It performs the same operation as reshape. The only difference is that View will not create a copy and will allow us to perform fast and memory efficient computations whereas reshape may or may not share the same memory. There's a good discussion of the differences [here](https://stackoverflow.com/questions/49643225/whats-the-difference-between-reshape-and-view-in-pytorch).\n","\n","Line from above link \" *Another difference is that reshape() can operate on both contiguous and non-contiguous tensor while view() can only operate on contiguous tensor* \"\n","\n","[Definition of contiguous](https://stackoverflow.com/questions/26998223/what-is-the-difference-between-contiguous-and-non-contiguous-arrays/26999092#26999092)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"63c0bfd9"},"outputs":[],"source":["# Initializing a tensor with 10 elements from 0 to 9\n","# Changing the shape of tensor t from 1x10 to 2x5"]},{"cell_type":"markdown","metadata":{"id":"864b138f"},"source":["\n","<details>\n","<summary>Click to expand code</summary>\n","\n","```python\n","# Initializing a tensor with 10 elements from 0 to 9\n","t = torch.arange(10)\n","print(t,'shape:', t.shape, sep='\\n', end = '\\n\\n')\n","# Changing the shape of tensor t from 1x10 to 2x5\n","t = t.view(2, 5)\n","print(t,'shape:', t.shape, sep='\\n')\n"]},{"cell_type":"markdown","metadata":{"id":"WOZJuWP84T7N"},"source":["Views can reflect changes from the base tensor."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"d2b70000"},"outputs":[],"source":["# Create a view of tensor t\n","# Before change in base tensor\n","# Modifying element of base tensor\n","# After change in base tensor"]},{"cell_type":"markdown","metadata":{"id":"c32fbe57"},"source":["\n","<details>\n","<summary>Click to expand code</summary>\n","\n","```python\n","t = torch.arange(10)\n","\n","# Create a view of tensor t\n","tr = t.view(2, 5)\n","\n","# Before change in base tensor\n","print(f\"before changing the base tensor\\n{tr}\")\n","\n","# Modifying element of base tensor\n","t[0] = 67\n","\n","# After change in base tensor\n","print(f\"\\nafter changing the base tensor\\n {tr}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f8e47005"},"outputs":[],"source":["# we can use -1 with view as well."]},{"cell_type":"markdown","metadata":{"id":"3e5b5eb0"},"source":["\n","<details>\n","<summary>Click to expand code</summary>\n","\n","```python\n","# we can use -1 with view as well.\n","t = torch.rand((4, 5))\n","t1 = t.view(2, -1)\n","print(t1.shape)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2258ecd5"},"outputs":[],"source":["# we can also flatten the tensor (convert the tensor to one dimensional tensor) by using view(-1)\n","# this gives the same result as method flatten()"]},{"cell_type":"markdown","metadata":{"id":"2ffaae9e"},"source":["\n","<details>\n","<summary>Click to expand code</summary>\n","\n","```python\n","# we can also flatten the tensor (convert the tensor to one dimensional tensor) by using view(-1)\n","# this gives the same result as method flatten()\n","t = torch.rand((4, 5, 3))\n","t2 = t.view(-1)\n","t3 = t.flatten()\n","print(t2.shape)\n","print(t3.shape)\n"]},{"cell_type":"markdown","metadata":{"id":"UCacRHTqv-SN"},"source":["### <font color = 'pickle'> **3) Adding and removing dimensions of size 1**\n","- Insert a dimension of size 1 at a specific location (location specified by dim) using `torch.unsqueeze(dim)`\n","- Remove a dimension of size 1 at a specific location (location specified by dim) using `torch.squeeze(dim)`\n","- Remove all dimensions of size 1 using `torch.squeeze()`\n","- Insert dimenion of size 1 using `None `keyword\n","- Remove dimenion of size 1 using `0 `keyword"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"befbd837"},"outputs":[],"source":["# Initialize an tensor"]},{"cell_type":"markdown","metadata":{"id":"f37dadbb"},"source":["\n","<details>\n","<summary>Click to expand code</summary>\n","\n","```python\n","# Initialize an tensor\n","t1 = torch.ones(2, 2)\n","print(t1)\n","t1.shape\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1a5cda01"},"outputs":[],"source":["# add dimension of size 1 in the beginning using unsqueeze method and argument dim = 0"]},{"cell_type":"markdown","metadata":{"id":"8e38f510"},"source":["\n","<details>\n","<summary>Click to expand code</summary>\n","\n","```python\n","# add dimension of size 1 in the beginning using unsqueeze method and argument dim = 0\n","t1 = t1.unsqueeze(dim=0)\n","print(t1)\n","t1.shape\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9a776d23"},"outputs":[],"source":["# add dimesnion of size 1 at the end usin unsqueeze method and dim = 3"]},{"cell_type":"markdown","metadata":{"id":"8ef2bded"},"source":["\n","<details>\n","<summary>Click to expand code</summary>\n","\n","```python\n","# add dimesnion of size 1 at the end usin unsqueeze method and dim = 3\n","t1 = t1.unsqueeze(dim=3)\n","print(t1)\n","print(t1.shape)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9a6201d0"},"outputs":[],"source":["# We can add new dimesnion at any place"]},{"cell_type":"markdown","metadata":{"id":"ef51b931"},"source":["\n","<details>\n","<summary>Click to expand code</summary>\n","\n","```python\n","# We can add new dimesnion at any place\n","t1 = torch.arange(20).view(2, 2, 5)\n","print(t1.shape)\n","t1 = t1.unsqueeze(dim=1)\n","print(t1.shape)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3b419812"},"outputs":[],"source":["# we can also use None keyword to add dimension of size 1 at multiple locations"]},{"cell_type":"markdown","metadata":{"id":"ac3396b2"},"source":["\n","<details>\n","<summary>Click to expand code</summary>\n","\n","```python\n","# we can also use None keyword to add dimension of size 1 at multiple locations\n","t1 = t1[:, :, :, None, :, None]\n","print(t1.shape)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8ceb8322"},"outputs":[],"source":["# Remove a dimension of size 1 at a specific location using torch.squeeze(dim)"]},{"cell_type":"markdown","metadata":{"id":"3e2f9d15"},"source":["\n","<details>\n","<summary>Click to expand code</summary>\n","\n","```python\n","# Remove a dimension of size 1 at a specific location using torch.squeeze(dim)\n","t1 = t1.squeeze(dim=1)\n","print(t1.shape)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7c180164"},"outputs":[],"source":["# Remove a dimension of size 1 at a specific location using 0 keyword"]},{"cell_type":"markdown","metadata":{"id":"8ba4591e"},"source":["\n","<details>\n","<summary>Click to expand code</summary>\n","\n","```python\n","# Remove a dimension of size 1 at a specific location using 0 keyword\n","t1 = t1[:, :, 0]\n","print(t1.shape)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ce371ccd"},"outputs":[],"source":["# Removing all dimensions of size 1 using torch.squeeze()"]},{"cell_type":"markdown","metadata":{"id":"f584dd8f"},"source":["\n","<details>\n","<summary>Click to expand code</summary>\n","\n","```python\n","# Removing all dimensions of size 1 using torch.squeeze()\n","t1 = t1.squeeze()\n","print(t1.shape)\n"]},{"cell_type":"markdown","metadata":{"id":"4SewxjuW7O6Y"},"source":["### <font color = 'pickle'>**4) Adopting shape of other tensors**\n","We can use view_as(input) to adopt shape of other tensors"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"caa20e36"},"outputs":[],"source":["# create a tensor b filled with ones (10 elements) and has same shape as b"]},{"cell_type":"markdown","metadata":{"id":"635131f8"},"source":["\n","<details>\n","<summary>Click to expand code</summary>\n","\n","```python\n","a = torch.arange(10).view(2, 5)\n","# create a tensor b filled with ones (10 elements) and has same shape as b\n","b = torch.ones(10).view_as(a)\n","print(a.shape)\n","print(b.shape)\n"]},{"cell_type":"markdown","metadata":{"id":"tMtrejzioBMJ"},"source":["### <font color = 'pickle'>**5) Permute**\n","\n","Permute function rearranges the original tensor according to the desired ordering and returns a new multidimensional rotated tensor.\n","\n","Let us consider an example:\n","\n","If the size of a tensor is (2, 3, 4),\n","\n","- First size is 2\n","- Second size is 3\n","- Third size is 4\n","\n","Now, in case of permute we will just change the ordering of the sizes. Thus if we write permute(0, 2, 1) the new tensor will have:\n","\n","- First size is 2 (1st size of previous)\n","- Second size is 4 (3rd size of previous)\n","- Third size is 3 (2nd size of previous)\n","\n","Pytorch's function permute() only permutes or in other words shuffles the order of the axes of a tensor whereas view() reshapes the tensor by reducing/expanding the size of each dimension.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8f692b12"},"outputs":[],"source":["# Initilaize a tensor and print it's size and elements"]},{"cell_type":"markdown","metadata":{"id":"c78cb88c"},"source":["\n","<details>\n","<summary>Click to expand code</summary>\n","\n","```python\n","# Initilaize a tensor and print it's size and elements\n","torch.manual_seed(0)\n","t1 = torch.randint(0, 10, size=(2, 4))\n","print(t1.size())\n","print(t1)\n"]},{"cell_type":"markdown","metadata":{"id":"90cc9ed4"},"source":["\n","<details>\n","<summary>Click to expand code</summary>\n","\n","```python\n","t1.storage()\n"]},{"cell_type":"markdown","metadata":{"id":"0b3117a5"},"source":["\n","<details>\n","<summary>Click to expand code</summary>\n","\n","```python\n","t1.is_contiguous()\n"]},{"cell_type":"markdown","metadata":{"id":"8a6bc1cf"},"source":["\n","<details>\n","<summary>Click to expand code</summary>\n","\n","```python\n","t1.stride()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"69ca96b2"},"outputs":[],"source":["# Permute the tensor and print it's size and elements"]},{"cell_type":"markdown","metadata":{"id":"ceca726d"},"source":["\n","<details>\n","<summary>Click to expand code</summary>\n","\n","```python\n","# Permute the tensor and print it's size and elements\n","t1_p = t1.permute(1, 0)\n","print(t1_p.size())\n","print()\n","print(t1_p)\n"]},{"cell_type":"markdown","metadata":{"id":"8d68fd3e"},"source":["\n","<details>\n","<summary>Click to expand code</summary>\n","\n","```python\n","t1_p.storage()\n"]},{"cell_type":"markdown","metadata":{"id":"1d21a095"},"source":["\n","<details>\n","<summary>Click to expand code</summary>\n","\n","```python\n","t1_p.stride()\n"]},{"cell_type":"markdown","metadata":{"id":"b5fbf0e7"},"source":["\n","<details>\n","<summary>Click to expand code</summary>\n","\n","```python\n","t1_p.is_contiguous()\n"]},{"cell_type":"markdown","metadata":{"id":"62aee583"},"source":["\n","<details>\n","<summary>Click to expand code</summary>\n","\n","```python\n","t1_p.view(2, 4)\n"]},{"cell_type":"markdown","metadata":{"id":"a3f882c9"},"source":["\n","<details>\n","<summary>Click to expand code</summary>\n","\n","```python\n","t1_p.reshape(2, 4)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"becfea5a"},"outputs":[],"source":["# Initilaize a tensor and print it's size and elements"]},{"cell_type":"markdown","metadata":{"id":"8bfa25e9"},"source":["\n","<details>\n","<summary>Click to expand code</summary>\n","\n","```python\n","# Initilaize a tensor and print it's size and elements\n","torch.manual_seed(0)\n","t2 = torch.rand(2, 3, 4)\n","print(t2.size())\n","print(f\"\\n{t2}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"866e87f2"},"outputs":[],"source":["# Permute the tensor and print it's size and elements - use permute (0, 2, 1)"]},{"cell_type":"markdown","metadata":{"id":"b9d68a98"},"source":["\n","<details>\n","<summary>Click to expand code</summary>\n","\n","```python\n","# Permute the tensor and print it's size and elements - use permute (0, 2, 1)\n","t2_p = t2.permute(0, 2, 1)\n","print(t2_p.size())\n","print(f\"\\n{t2_p}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8c69975a"},"outputs":[],"source":["# difference between permute and view"]},{"cell_type":"markdown","metadata":{"id":"d314cf31"},"source":["\n","<details>\n","<summary>Click to expand code</summary>\n","\n","```python\n","# difference between permute and view\n","x = torch.arange(3 * 2).view(2, 3)\n","print(x)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"b1603fef"},"outputs":[],"source":["# create a view (3, 2)"]},{"cell_type":"markdown","metadata":{"id":"7af6b500"},"source":["\n","<details>\n","<summary>Click to expand code</summary>\n","\n","```python\n","# create a view (3, 2)\n","print(x.view(3, 2))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f1562d03"},"outputs":[],"source":["# permute axis(1, 0)"]},{"cell_type":"markdown","metadata":{"id":"09c20c49"},"source":["\n","<details>\n","<summary>Click to expand code</summary>\n","\n","```python\n","# permute axis(1, 0)\n","print(x.permute(1, 0))\n"]},{"cell_type":"markdown","metadata":{"id":"f-HexBDq4PIi"},"source":["Question and answer taken from following reference: <br>\n","https://discuss.pytorch.org/t/different-between-permute-transpose-view-which-should-i-use/32916\n","\n","- (1) If I have a feature size of BxCxHxW, I want to reshape it to BxCxHW. Which one is a good option?\n","- (2) If I have a feature size of BxCxHxW, I want to change it to BxCxWxH . Which one is a good option?\n","- (3) If I have a feature size of BxCxH, I want to change it to BxCxHx1 . Which one is a good option?\n","\n","Solution:\n","- permute changes the order of dimensions aka axes, so 2 would be a use case. Transpose is a special case of permute, use it with 2d tensors.\n","- view can combine and split axes, so 1 and 3 can use view,\n","- note that view can fail for noncontiguous layouts (e.g. crop a picture using indexing), in these cases reshape will do the right thing,\n","- for adding dimensions of size 1 (case 3), there also are unsqueeze and indexing with None.\n"]},{"cell_type":"markdown","metadata":{"id":"12538771"},"source":["\n","<details>\n","<summary>Click to expand code</summary>\n","\n","```python\n","\n"]},{"cell_type":"markdown","metadata":{"id":"wMkz0iL56Va7"},"source":["## <font color = 'pickle'>**Concatenating Tensors**\n","\n","We can use `torch.cat((tensors_to_concatenate), dim)` to concatenate tensors.\n","\n","The tensors must have the same shape (except in the concatenating dimension)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0d38667e"},"outputs":[],"source":["# we can use torch\n","# The tensors must have the same shape (except in the concatenating dimension)\n","# x1 and x2 have the same shape except for dim = 0, hence we can conactenate these along dim = 0\n","# x1 and x3 have the same shape except for dim = 1, hence we can conactenate these along dim = 1\n","# we cannot concatenate x2 and x3 along any dimension"]},{"cell_type":"markdown","metadata":{"id":"58b653ff"},"source":["\n","<details>\n","<summary>Click to expand code</summary>\n","\n","```python\n","# we can use torch\n","x1 = torch.randint(low=0, high=10, size=(2, 5))\n","x2 = torch.ones(4, 5)\n","x3 = torch.zeros(2, 3)\n","\n","# The tensors must have the same shape (except in the concatenating dimension)\n","# x1 and x2 have the same shape except for dim = 0, hence we can conactenate these along dim = 0\n","# x1 and x3 have the same shape except for dim = 1, hence we can conactenate these along dim = 1\n","# we cannot concatenate x2 and x3 along any dimension\n","\n","print(f\"shape of x1 is {x1.shape}\")\n","print(f\"shape of x2 is {x2.shape}\")\n","print(f\"shape of x3 is {x3.shape}\")\n","print(f\"\\nx1\\n:{x1}\")\n","print(f\"\\nx2\\n:{x2}\")\n","print(f\"\\nx3\\n:{x3}\\n\")\n","\n","x1_x2 = torch.cat((x1, x2), dim=0)\n","x1_x3 = torch.cat((x1, x3), dim=1)\n","print(f\"shape of x1_x2 is {x1_x2.shape}\")\n","print(f\"shape of x2_x3 is {x1_x3.shape}\")\n","print(f\"\\nx1_x2\\n:{x1_x2}\")\n","print(f\"\\nx1_x3\\n:{x1_x3}\")\n"]},{"cell_type":"markdown","metadata":{"id":"xX3tyHVrpN4s"},"source":["## <font color = 'pickle'>**Some commonly used Tensors**"]},{"cell_type":"markdown","metadata":{"id":"wpm1RohNpzKv"},"source":["###<font color = 'pickle'>**1) Tensor containing all zeros/ all ones/ or any value**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"39f501a7"},"outputs":[],"source":["# Tensor containing all zeros, size = 10\n","# Tensor containing all zeros, size = 2 X 2 X 3"]},{"cell_type":"markdown","metadata":{"id":"a8a927c3"},"source":["\n","<details>\n","<summary>Click to expand code</summary>\n","\n","```python\n","# Tensor containing all zeros, size = 10\n","z1 = torch.zeros(5)\n","\n","# Tensor containing all zeros, size = 2 X 2 X 3\n","z2 = torch.zeros(2, 2, 3)\n","\n","print(z1)\n","print(z2)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"b1e30810"},"outputs":[],"source":["# Tensor containing all ones, size = 7\n","# Tensor containing all ones, size = 1 X 2 X 3"]},{"cell_type":"markdown","metadata":{"id":"d039cb7e"},"source":["\n","<details>\n","<summary>Click to expand code</summary>\n","\n","```python\n","# Tensor containing all ones, size = 7\n","z1 = torch.ones(7)\n","\n","# Tensor containing all ones, size = 1 X 2 X 3\n","z2 = torch.ones(1, 2, 3)\n","\n","print(z1)\n","print(z2)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9cc22d2a"},"outputs":[],"source":["# We can also use torch.full(size, fill_value) to create a tensor filled with any value\n","# Tensor containing all fives, size = 1 X 2 X 3"]},{"cell_type":"markdown","metadata":{"id":"52e18cb3"},"source":["\n","<details>\n","<summary>Click to expand code</summary>\n","\n","```python\n","# We can also use torch.full(size, fill_value) to create a tensor filled with any value\n","# Tensor containing all fives, size = 1 X 2 X 3\n","\n","z3 = torch.full(size=(2, 2, 3), fill_value=5)\n","print(z3)\n"]},{"cell_type":"markdown","metadata":{"id":"HxOWMc03qszf"},"source":["### <font color = 'pickle'>**2) Tensor with elements in a particular range**\n","Suppose we need a tensor with values `1, 2, 3, 4.....n. `\n","\n","We can simply specify the range and tensor will automatically get filled with these values."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cd96b764"},"outputs":[],"source":["# Creating a tensor with integers from 1 to 5 with space 1: [1, 2, 3, 4, 5]\n","# syntax arange(start, end, step) - create tensor with values in the interval [start, end).\n","# start is inclusive , end is not i.e. start <= values < end"]},{"cell_type":"markdown","metadata":{"id":"8f032a9f"},"source":["\n","<details>\n","<summary>Click to expand code</summary>\n","\n","```python\n","# Creating a tensor with integers from 1 to 5 with space 1: [1, 2, 3, 4, 5]\n","# syntax arange(start, end, step) - create tensor with values in the interval [start, end).\n","# start is inclusive , end is not i.e. start <= values < end\n","tr1 = torch.arange(1, 6)\n","print(tr1)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"23507cc4"},"outputs":[],"source":["# Creating a tensor with integers from 0 to 10 with space 2 using \"step\" parameter: [0, 2, 4, 6, 8, 10]"]},{"cell_type":"markdown","metadata":{"id":"952acc29"},"source":["\n","<details>\n","<summary>Click to expand code</summary>\n","\n","```python\n","# Creating a tensor with integers from 0 to 10 with space 2 using \"step\" parameter: [0, 2, 4, 6, 8, 10]\n","tr2 = torch.arange(0, 11, step=2)\n","print(tr2)\n"]},{"cell_type":"markdown","metadata":{"id":"T4TOw7gUZ-He"},"source":["We can also use `torch.linspace()` to generate evenly spaced values between two numbers"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"37fec6e9"},"outputs":[],"source":["# Generate 10 evenly spaced values between 0 and 1 (both inclusive)"]},{"cell_type":"markdown","metadata":{"id":"ed66cd04"},"source":["\n","<details>\n","<summary>Click to expand code</summary>\n","\n","```python\n","# Generate 10 evenly spaced values between 0 and 1 (both inclusive)\n","t1 = torch.linspace(0, 1, 10)\n","print(t1)\n"]},{"cell_type":"markdown","metadata":{"id":"sVqmTY7IzBmT"},"source":["###<font color = 'pickle'>**3) Tensor with elements from probability distribution**\n","\n","We can use the randn function to get elements from standard normal probabilty distribution i.e. normal dustribution with mean = 0 and variance = 1. If we want to select elements from normal ditsribution with different mean and variance then we should use torch.normal"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"41c5ca02"},"outputs":[],"source":["# for reproducabilty so that we get same results everytime we run this cell\n","# Sample 500,000 values from standard normal distribution (mean = 0 , variance = 1)\n","# Sample 500,000 values from normal distribution (mean = 5 , std = 2)"]},{"cell_type":"markdown","metadata":{"id":"ca49534c"},"source":["\n","<details>\n","<summary>Click to expand code</summary>\n","\n","```python\n","# for reproducabilty so that we get same results everytime we run this cell\n","torch.manual_seed(42)\n","\n","# Sample 500,000 values from standard normal distribution (mean = 0 , variance = 1)\n","t1 = torch.randn(500000)\n","\n","# Sample 500,000 values from normal distribution (mean = 5 , std = 2)\n","t2 = torch.normal(mean=5, std=2, size=(500000,))\n"]},{"cell_type":"markdown","metadata":{"id":"7f961c8c"},"source":["\n","<details>\n","<summary>Click to expand code</summary>\n","\n","```python\n","print(\"Mean and std of tensor using torch.randn\")\n","print(torch.mean(t1))\n","print(torch.std(t1))\n","\n","print(\"\\nMean and std of tensor using torch.normal\")\n","print(torch.mean(t2))\n","print(torch.std(t2))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0076e54c"},"outputs":[],"source":["# for reproducabilty so that we get same results everytime we run this cell\n","# we sampled 10 values from standard noemal distribution. (5, 2) is the shape."]},{"cell_type":"markdown","metadata":{"id":"84d1f91d"},"source":["\n","<details>\n","<summary>Click to expand code</summary>\n","\n","```python\n","# for reproducabilty so that we get same results everytime we run this cell\n","torch.manual_seed(0)\n","\n","# we sampled 10 values from standard noemal distribution. (5, 2) is the shape.\n","t1 = torch.randn(5, 2)\n","t1\n"]},{"cell_type":"markdown","metadata":{"id":"SxiimV9UY7zu"},"source":["We can also sample from other distributions like torch.rand, torch.randint etc."]},{"cell_type":"markdown","metadata":{"id":"OIeaFS6SV5lS"},"source":["### <font color = 'pickle'>**4) Empty Tensor**\n","We can create uninitialized  tensors using torch.empty.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6bbb8339"},"outputs":[],"source":["# create empty tensor of shape (2, 4)"]},{"cell_type":"markdown","metadata":{"id":"8180ff4c"},"source":["\n","<details>\n","<summary>Click to expand code</summary>\n","\n","```python\n","# create empty tensor of shape (2, 4)\n","empty_tensor = torch.empty(2, 4)\n","empty_tensor\n"]},{"cell_type":"markdown","metadata":{"id":"UiNZ0HioJdXF"},"source":["### <font color = 'pickle'>**5) Commonly used tensors based on shape of other tensors**"]},{"cell_type":"markdown","metadata":{"id":"iJEmlIsUIuYu"},"source":["We can also use `torch.zeros_like(input)`, `torch.ones_like(input)`, `torch.full_like(input)` and `torch.empty_like(input)` to create tensors based on the shape of other tensors\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"ce559625"},"source":["\n","<details>\n","<summary>Click to expand code</summary>\n","\n","```python\n","input_tensor = torch.arange(6).view(2, 3)\n","input_tensor.shape\n"]},{"cell_type":"markdown","metadata":{"id":"ccd216bd"},"source":["\n","<details>\n","<summary>Click to expand code</summary>\n","\n","```python\n","print(torch.ones_like(input_tensor))\n","print(torch.zeros_like(input_tensor))\n","print(torch.full_like(input_tensor, 5))\n","print(torch.empty_like(input_tensor))\n"]},{"cell_type":"markdown","metadata":{"id":"tsMg-Jujsm2W"},"source":["###<font color = 'pickle'> **6) Identity Matrix**\n","\n","Identity matrix is a matrix which has 1's along the diagnal and zeros everywhere else."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3c1e933b"},"outputs":[],"source":["# Identity matrix of size 3"]},{"cell_type":"markdown","metadata":{"id":"525281cb"},"source":["\n","<details>\n","<summary>Click to expand code</summary>\n","\n","```python\n","# Identity matrix of size 3\n","id_matrix = torch.eye(3)\n","print(id_matrix)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8b5dcd98"},"outputs":[],"source":["# Identity matrix of size 5"]},{"cell_type":"markdown","metadata":{"id":"7794dc33"},"source":["\n","<details>\n","<summary>Click to expand code</summary>\n","\n","```python\n","# Identity matrix of size 5\n","id_matrix = torch.eye(5)\n","print(id_matrix)\n"]},{"cell_type":"markdown","metadata":{"id":"f23a8dd7"},"source":["\n","<details>\n","<summary>Click to expand code</summary>\n","\n","```python\n","\n"]},{"cell_type":"markdown","metadata":{"id":"24OjlU8RnSie"},"source":["## <font color = 'pickle'>**Masks using binary tensors**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"92bf2b8d"},"outputs":[],"source":["# create a tensor which has probailities of events\n","# Binary tensors"]},{"cell_type":"markdown","metadata":{"id":"fa5572b4"},"source":["\n","<details>\n","<summary>Click to expand code</summary>\n","\n","```python\n","# create a tensor which has probailities of events\n","prob = torch.tensor([0.7, 0.4, 0.6, 0.2, 0.8, 0.1])\n","\n","# Binary tensors\n","print(prob > 0.5)\n","print(prob <= 0.5)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"057db4b2"},"outputs":[],"source":["# create output tensor where output = 1 if prob >0.5 and 0 otherwise\n","# craete an empty output tensor of same shape as prob\n","# update output tensor using the binary mask"]},{"cell_type":"markdown","metadata":{"id":"d39aa9bf"},"source":["\n","<details>\n","<summary>Click to expand code</summary>\n","\n","```python\n","# create output tensor where output = 1 if prob >0.5 and 0 otherwise\n","# craete an empty output tensor of same shape as prob\n","output = torch.empty_like(prob)\n","\n","# update output tensor using the binary mask\n","output[prob > 0.5] = 1\n","output[prob <= 0.5] = 0\n","print(output)\n"]},{"cell_type":"markdown","metadata":{"id":"c8ae8660"},"source":["\n","<details>\n","<summary>Click to expand code</summary>\n","\n","```python\n","\n"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}